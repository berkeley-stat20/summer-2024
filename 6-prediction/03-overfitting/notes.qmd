---
title: "Overfitting"
subtitle: "Training sets and testing sets"
image: images/poly.png
self-contained: true
format:
  html:
    code-fold: true
    code-link: true
    code-summary: "."
  pdf:
    echo: false
    code-fold: false
execute: 
  warning: false
  message: false
---

<!---
Before reading these notes, take a listen to Act 3 of an episode of the podcast "This American Life" called "Kids Look Back" (10 minutes).

[This American Life: Kids Look Back](https://www.thisamericanlife.org/584/for-your-reconsideration)
---
-->

[B]{.dropcap}elow is data we collected about the association between number of hours studied and students' test scores in a math class. Our goal is to predict the exam score from number of hours studied. Both plots below show the same data, but show the predictions from two different predictive models.

```{r}
#| fig-width: 7
#| fig-height: 3.6
#| fig-align: center
#| code-fold: false
#| echo: false
library(tidyverse)
library(patchwork)
library(broom)

set.seed(1)
n_samples <- 10

min_n_hours <- 5
max_n_hours <- 10

exam_avg <- 75
point_per_hour <- 5

intercept <- exam_avg - point_per_hour * (min_n_hours + max_n_hours) / 2
noise <-  rnorm(n=n_samples, mean=0, sd=5)

# Setup data
math <- tibble(hours=runif(n=n_samples, min=min_n_hours, max=max_n_hours),
             score=point_per_hour * hours + intercept + noise)

# same x data, but much finer grid. useful for plotting lines
math_filled_in_range <- tibble(hours=seq(from=min_n_hours, to=max_n_hours, by=.01))

# fit models
lm_lin <- lm(score ~ hours, data=math)
lm_poly <- lm(score ~ poly(hours, degree=20, raw=T), data=math)

# add predictions into model
math <- math %>% 
    mutate(y_pred_linear = predict(lm_lin, newdata=math),
           y_pred_poly = predict(lm_poly, newdata=math))

math_filled_in_range <- math_filled_in_range %>% 
    mutate(y_pred_linear = predict(lm_lin, newdata=math_filled_in_range),
           y_pred_poly = predict(lm_poly, newdata=math_filled_in_range))

plot_linear <- math %>% 
    ggplot(aes(x=hours, y=score)) +
    geom_point() +
    geom_line(aes(x=hours, y=y_pred_linear), color='blue') +
    lims(x=c(5, 10), y=c(65, 90)) +
    ggtitle("Math class data") +
    theme_bw()

plot_poly <- math %>% 
    ggplot(aes(x=hours, y=score)) +
    geom_point() +
    geom_line(data=math, aes(x=hours, y=y_pred_poly), color='red')+
    lims(x=c(5, 10), y=c(65, 90)) +
    theme_bw()

plot_linear + plot_poly
```

Which model looks more appropriate: the blue, or the red? More specifically,

-   Does it make sense that there should be a big difference between studying 6.7 hours vs studying 6.8 hours?

-   Should studying a little more make your score go down?

The blue model seems more reasonable: studying more should steadily increase your score. The predictive model on the right seems like it took the particular data points "too seriously!" This will be an issue if a new set of students from the same class comes along and we want to predict what their exam scores will be based on the amount of hours studied. Let's use the blue and red models to predict scores from more students from this same class.

```{r}
#| fig-width: 7
#| fig-height: 3.6
#| fig-align: center
#| code-fold: false
#| echo: false
set.seed(6)

noise2 <-  rnorm(n=n_samples, mean=0, sd=5)
math2 <- tibble(hours=runif(n=n_samples, min=min_n_hours, max=max_n_hours),
             score=point_per_hour * hours + intercept + noise2)


# add predictions into model
math2 <- math2 %>% 
    mutate(y_pred_linear = predict(lm_lin, newdata=math2),
           y_pred_poly = predict(lm_poly, newdata=math2))

math_filled_in_range <- math_filled_in_range %>% 
    mutate(y_pred_linear = predict(lm_lin, newdata=math_filled_in_range),
           y_pred_poly = predict(lm_poly, newdata=math_filled_in_range))

plot_linear <- math2 %>% 
    ggplot(aes(x=hours, y=score)) +
    geom_point() +
    geom_line(aes(x=hours, y=y_pred_linear), color='blue') +
    lims(x=c(5, 10), y=c(65, 90)) +
    ggtitle("Math class data") +
    theme_bw()

plot_poly <- math2 %>% 
    ggplot(aes(x=hours, y=score)) +
    geom_point() +
    geom_line(data=math, aes(x=hours, y=y_pred_poly), color='red')+
    lims(x=c(5, 10), y=c(65, 90)) +
    theme_bw()

plot_linear + plot_poly
```

We see that the blue line is prepared to predict the exam scores well enough for these students--even though the model was not fit using them! The red model, however, does poorly. It is so beholden to the first group of students that it doesn't know how to manage when the students are even slightly different. In statistics, we say that the red model was **overfit**.

**Overfitting**
:    The practice of using a predictive model which is very effective at explaining the data used to fit it, but is poor at making predictions on *new* data. 

<!---
## Defining overfitting

Learning through examples is one of the most important ways we acquire knowledge.
Almost all predictive modeling -- from linear regression to artificial intelligence based on deep learning -- is accomplished though learning by example. 

In simple linear regression we show our model a bunch of pairs of examples $(x_1, y_1), \dots, (x_n, y_n)$ and ask the linear model to best approximate $y$ through the linear function $\widehat{y} = b_0 + b_1 \cdot x$. The "lesson" that the model learns are the values of $b_0$ and $b_1$ that define the relationship between the variables $x$ and $y$ (and those coefficient values are composed of nothing more than the original observations[^slr]).

[^trees]: Definition (Merriam-Webster): to not understand or appreciate a larger situation, problem, etc., because one is considering only a few parts of it.


https://www.merriam-webster.com/dictionary/miss%20the%20forest%20for%20the%20trees#:~:text=Definition%20of%20miss%20the%20forest,a%20few%20parts%20of%20it 


[^slr]: Recall $b_1 = r \frac{s_y}{s_x}$ and each of the statistics involved are functions of $x_i, y_i$ and $n$).

Have you ever given someone an example to illustrate a concept, but that person gets too caught up in the details of the example and learns the wrong lesson? There is an expression for this: "missing the forest for the trees"[^trees].
Missing the forest for the trees turns out to be one of the most important concepts predictive modeling. In statistics, we call it *overfitting*.

**Overfitting**
:    The practice of using a predictive model which is very effective at explaining the data used to fit it, but is poor at making predictions on *new* data. 
-->

## Overfitting with polynomials

Usually, overfitting occurs as a result of applying a model that is too complex, like the red one we saw for the math class data above.  We created that overfitted predictive model on the right by fitting a polynomial with a high degree. Polynomials are quite powerful models and are capable of creating very complex predictive functions. The higher the polynomial degree, the more complex function it can create.


Let's illustrate by fitting polynomial models with progressively higher degrees to the data set above.

```{r}
#| fig-height: 3
#| fig-width: 8
#| fig-align: center
#| code-fold: false
#| echo: false
plot_deg_1 <- math_filled_in_range %>% 
    mutate(y_pred = predict(lm(score ~ poly(hours, degree=5, raw=T), data=math), newdata=math_filled_in_range)) %>% 
    ggplot( aes(x=hours, y=score)) +
    geom_line(aes(x=hours, y=y_pred), color='blue')+
    geom_point(data=math, aes(x=hours, y=score)) +
    lims(x=c(5, 10), y=c(65, 90)) +
    ggtitle("Degree 5 polynomial") +
    theme_bw()

plot_deg_3 <- math_filled_in_range %>%
    mutate(y_pred = predict(lm(score ~ poly(hours, degree=6, raw=T), data=math), newdata=math_filled_in_range)) %>%
    ggplot(aes(x=hours, y=score)) +
    geom_line(aes(x=hours, y=y_pred), color='purple')+
    geom_point(data=math, aes(x=hours, y=score)) +
    lims(x=c(5, 10), y=c(65, 90)) +
    ggtitle("Degree 6 polynomial") +
    theme_bw()

plot_deg_5 <- math_filled_in_range %>%
    mutate(y_pred = predict(lm(score ~ poly(hours, degree=7, raw=T), data=math), newdata=math_filled_in_range)) %>%
    ggplot(aes(x=hours, y=score)) +
    geom_line(aes(x=hours, y=y_pred), color='hotpink') +
    geom_point(data=math, aes(x=hours, y=score)) +
    lims(x=c(5, 10), y=c(65, 90)) +
    ggtitle("Degree 7 polynomial") +
    theme_bw()

plot_deg_1 + plot_deg_3 + plot_deg_5
```

The higher the polynomial degree, the closer the prediction function comes to perfectly fitting the data[^interpolation]. Therefore, when it comes to evaluating which model is the best for prediction, we would say the degree seven polynomial is best.  Indeed, based on our knowledge so far, it would have the highest $R^2$. The true test is yet to come, though. Let's measure these three models on how well they predict to the second group of students that weren't used to fit the model. 

```{r}
#| fig-height: 3
#| fig-width: 8
#| fig-align: center
#| code-fold: false
#| echo: false

plot_deg_1 <- math_filled_in_range %>% 
    mutate(y_pred = predict(lm(score ~ poly(hours, degree=5, raw=T), data=math), newdata=math_filled_in_range)) %>% 
    ggplot( aes(x=hours, y=score)) +
    geom_line(aes(x=hours, y=y_pred), color='blue')+
    geom_point(data=math2, aes(x=hours, y=score)) +
    lims(x=c(5, 10), y=c(65, 90)) +
    ggtitle("Degree 5 polynomial") +
    theme_bw()

plot_deg_3 <- math_filled_in_range %>%
    mutate(y_pred = predict(lm(score ~ poly(hours, degree=6, raw=T), data=math), newdata=math_filled_in_range)) %>%
    ggplot(aes(x=hours, y=score)) +
    geom_line(aes(x=hours, y=y_pred), color='purple')+
    geom_point(data=math2, aes(x=hours, y=score)) +
    lims(x=c(5, 10), y=c(65, 90)) +
    ggtitle("Degree 6 polynomial") +
    theme_bw()

plot_deg_5 <- math_filled_in_range %>%
    mutate(y_pred = predict(lm(score ~ poly(hours, degree=7, raw=T), data=math), newdata=math_filled_in_range)) %>%
    ggplot(aes(x=hours, y=score)) +
    geom_line(aes(x=hours, y=y_pred), color='hotpink') +
    geom_point(data=math2, aes(x=hours, y=score)) +
    lims(x=c(5, 10), y=c(65, 90)) +
    ggtitle("Degree 7 polynomial") +
    theme_bw()

plot_deg_1 + plot_deg_3 + plot_deg_5
```

As we increase the degree, the polynomial begins to perform worse on this new data as it bends to conform to the original data. For example, we see that for the student having studied around five and a half hours, the fifth degree polynomial does well, but the seven degree polynomial does horribly! To put a cherry on top, the red model we showed you in the beginnng of these notes was a *twenty* degree polynomial!

What we see is that the higher the degree, the more risk we run of using a model that overfits. 

## Training and testing sets: a workflow to curb overfitting

What you should have taken away so far is the following: *we should not fit the model (set the $b_0, b_1$ coefficients) and evaluate the model (judge its predictive power) with the same data set!* 

We can further back up this idea quantitatively. The plot below shows the $R^2$ value for math class models fit with different polynomial degrees.

[^interpolation]: We say a function that perfectly predicts each data point *interpolates* the data. See the first red curve for the math class exams.

```{r}
#| fig-align: center
#| fig-width: 5
#| fig-height: 2.5
#| code-fold: false
#| echo: false
#########################
# Digression: for loops #
#########################

# We have not learned for loops. Here is a simple example where we calculate the squares of the
# first 10 integers
# squares <- c()
# for (i in 1:10){
#     squares <- c(squares, i^2)
# }
# squares

#####################
# Evaluate Rsquared #
#####################
degrees2evaluate <- 1:10

Rsq_values <- c()
# each iteration of this for loop evaluates the model for a different polynomial degree 
for (degree in degrees2evaluate) {
    
    # fit the model on the training data with the specified degree
    model <- lm(score~poly(hours, degree=degree, raw=T), data=math)
    rsq <- glance(model)$r.squared
    
    # track the Rsquared values
    # vector <- c(vector, value) will add the value to the end of the vector
    # i.e. c does concatenation
    Rsq_values <- c(Rsq_values, rsq)
}

data.frame(degree=degrees2evaluate,
           Rsq=Rsq_values) %>% 
  ggplot(aes(x=degree, y=Rsq)) + 
  geom_line() + 
  scale_x_continuous(breaks=degrees2evaluate)+
  theme_bw()
```

The $R^2$ value goes steadily upwards as the polynomial degree goes up. In fact this is mathematically guaranteed to happen: *for a fixed data set the $R^2$ value for a polynomial model with higher degree will always be higher than a polynomial model with lower degree.* 

This should be disconcerting, especially since we earlier saw that the model with the highest $R^2$ did the worst on our unseen data. What you might also notice is that the $R^2$ isn't increasing by that much between degrees as the degree gets higher. This suggests that adding that additional degree isn't improving our *general* predictive power much; it's just helping the model tailor itself to the *specific* data we have. 

Does that mean $R^2$ is not a good metric to evaluate our model? Not necessarily. We can just change our workflow slightly. Instead of thinking in terms of a single data set, we can *partition*, or split the observations of the data set into two separate sets. We can use one of these data sets to *fit* the model, and the other to *evaluate* it. 

**Training Set**
:    The set of observations used to fit a predictive model; i.e. estimate the model coefficients.

**Testing Set**
:    The set of observations used to assess the accuracy of a predictive model. This set is disjoint from the training set.

The partition of a data frame into training and testing sets is illustrated by the diagram below.

```{r}
#| code-fold: false
#| echo: false

library(gt)

df <- tibble(y  = rep(" ", 10),
             x1 = rep(" ", 10),
             x2 = rep(" ", 10),
             x3 = rep(" ", 10))

df %>%
  gt() %>%
  cols_width(
    everything() ~ px(60)
  ) %>%
  tab_options(data_row.padding = px(7)) %>%
  tab_style(
    style = list(
      cell_fill(color = "steelblue")
    ),
    locations = cells_body(
      columns = everything(),
      rows = c(1, 3, 4, 5, 7, 8, 9, 10)
    )
  )  %>%
  tab_style(
    style = list(
      cell_fill(color = "goldenrod")
    ),
    locations = cells_body(
      columns = everything(),
      rows = c(2, 6)
    )
  ) %>%
    tab_style(
    style = cell_borders(
      sides = c("left", "right")
    ),
    locations = cells_body(
      columns = everything(),
      rows = everything()
    )
  )
```

```{r}
#| code-fold: false
#| echo: false
#| eval: false

# NOTE: this currently isn't working

df_training <- slice(df, -c(2, 6))

tab_training <- df_training %>%
  gt() %>%
  cols_width(
    everything() ~ px(40)
  ) %>%
  tab_options(data_row.padding = px(4)) %>%
  tab_style(
    style = list(
      cell_fill(color = "steelblue")
    ),
    locations = cells_body(
      columns = everything(),
      rows = everything()
    )
  ) %>%
    tab_style(
    style = cell_borders(
      sides = c("left", "right")
    ),
    locations = cells_body(
      columns = everything(),
      rows = everything()
    )
  ) %>%
  gtsave("images/tab_training.png")

df_testing <- slice(df, c(2, 6))

tab_testing <- df_testing %>%
  gt() %>%
  cols_width(
    everything() ~ px(40)
  ) %>%
  tab_options(data_row.padding = px(4)) %>%
  tab_style(
    style = list(
      cell_fill(color = "goldenrod")
    ),
    locations = cells_body(
      columns = everything(),
      rows = everything()
    )
  ) %>%
    tab_style(
    style = cell_borders(
      sides = c("left", "right")
    ),
    locations = cells_body(
      columns = everything(),
      rows = everything()
    )
  ) %>%
  gtsave("images/tab_testing.png")
```

```{r}
#| echo: false
#| eval: false
#| code-fold: false

# NOTE: this currently isn't working

# This uses a hack to structure the three data frame into this diagram 
# like this. It should be doable with quarto layouts in time, but for now:
# https://www.fizzics.ie/posts/2022-02-22-patchework-and-gt-tables/

layout <- c(
  area(t=0, l=0, b=0, r=1),
  area(t=2, l=1, b=3, r=1),
  area(t=3, l=1, b=3, r=1)
)
tab_training <- png::readPNG("images/tab_training.png", native = TRUE)
tab_testing <- png::readPNG("images/tab_testing.png", native = TRUE)
library(patchwork)
(ggplot() + theme_void()) + tab_training + tab_testing + plot_layout(design = layout)
```

The original data frame consists of 10 observations. For each observation we have recorded a response variable, $y$, and three predictors, $x_1, x_2$, and $x_3$. If we do an 80-20 split, then 8 of the rows will randomly be assigned to the training set (in blue). The 2 remaining rows (rows 2 and 6) are assigned to the testing set (in gold).

So to recap, our new workflow for predictive modeling involves: 

1. Splitting the data into a training and a testing set
2. Fitting the model to the training set
3. Evaluating the model using the testing set

### More on splitting the data

As in the diagram above, a standard partition is to dedicate 80% of the observations to the training set and the remainder to the testing set (a 80-20 split), though this is not a rule which is set in stone. The other question is how best to assign the observations to the two sets. In general, it is best to do this *randomly* to avoid one set that is categorically different than the other.

### Mean square error: another metric for evaluation

While $R^2$ is the most immediate metric to evaluate the predictive quality of a linear regression, it is quite specific to linear modeling. Therefore, data scientists have come up with another, more general metric called **mean square error (MSE).**  Let $y_i$ be observations of the response variable in the testing set, and $\hat{y}_i$ be your model's predictions for those observations. Then $\text{MSE}$ is given by 

$$ \text{MSE} = \frac{1}{n}\sum_{i=1}^n(y_i-\hat{y_i})^2$$
You may notice that for a linear regression model, $\text{MSE} = \frac{1}{n}\text{RSS}$.

A common offshoot is **root mean square error** ($\text{RMSE}$), which you can obtain by taking the square root of $\text{MSE}$. Much like what standard deviation does for variance, $\text{RMSE}$ allows you to think above the average error on a regular scale rather than on a squared scale. 


## The Ideas in Code

Let's shift the subject to mathematics to biology, and illustrate the training and testing approaching to evaluating predictions for the exam scores from a biology class with 200 students using as a predictor the number of hours that they have studied. Let's visualize these data first. 

```{r}
#| code-fold: false
#| echo: false

# Simulate a data set

n_samples_big <- 200
set.seed(20)
noise <-  rnorm(n = n_samples_big, mean = 0, sd = 5)

biology <- tibble(hours = runif(n = n_samples_big,
                                min = min_n_hours,
                                max=max_n_hours),
                  score = point_per_hour * hours + intercept + noise)
```

```{r}
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| code-fold: false
#| echo: false
biology %>% 
    ggplot(aes(x = hours, y = score)) +
    geom_point() + 
    ggtitle("Biology class") +
    theme_bw()
```

Here we are going to compare two models: a simple linear model versus a 5^th^ degree polynomial, both fit using the method of least squares.

- $\textbf{Model 1:} \quad \widehat{score} = b_0 + b_1 \times hours$
- $\textbf{Model 2:} \quad \widehat{score} = b_0 + b_1 \times hours + b_2 \times hours^2 + b_3 \times hours^3 + b_4 \times hours^4 + b_5 \times hours^5$

### Step 1: Split data 

We'll use an 80-20 split, with each observation assigned to its set randomly. There are many ways to do this via code: here is one using functions we've seen. 

-   Generate a vector of $n$ observations (in this case, our data has 200 observations) in which *approximately* 80 percent of the observations are `"train"` and 20 percent of the observations are `"test"`. To do this, we can make use of the `sample()` function. 

```{r}
#| code-fold: false
#| echo: true
set.seed(20)

train_or_test <- sample(x = c("train", "test"), 
                   size = 200, 
                   replace = TRUE, 
                   prob = c(0.8, 0.2))
```

-   mutate this vector onto our data frame (our data frame here is called `biology`). Below, you can see which rows in the data frame have been assigned to `"train"` and which have been assigned to `"test"`.

```{r}
#| code-fold: false
#| echo: true
biology <- biology |>
    mutate(set_type = train_or_test)
```


```{r}
#| code-fold: false
#| echo: false
biology |>
  slice_sample(n = 6)
```

-   split the data based on whether the observations are in the `"train"` or `"test"` set.

```{r}
#| code-fold: false
#| echo: true
biology_train <- biology |>
    filter(set_type == "train")

biology_test <- biology |>
    filter(set_type == "test")
```

### Step 2: Fit the model to the training set

Now fit two models on the training data. We will be using `lm()`, and for both models, the `data` argument is given by `biology_train`. 

```{r}
#| code-fold: false
#| echo: true

lm_slr <- lm(score ~ hours, data = biology_train)
lm_poly <- lm(score ~ poly(hours, degree = 20, raw = T),
              data = biology_train)
```

<!--
Next we visually inspect the predictions.

```{r}
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
# same x data, but much finer grid. useful for plotting lines
biology_filled_in_range <- tibble(hours = seq(from = min_n_hours, to = max_n_hours, by = .01))

# this is useful for plotting
biology_filled_in_range <- biology_filled_in_range %>% 
    mutate(y_pred_linear = predict(lm_slr,
                                   newdata = biology_filled_in_range),
           y_pred_poly = predict(lm_poly, 
                                 newdata = biology_filled_in_range)) 

biology_train %>% 
    ggplot(aes(x=hours, y=score)) +
    geom_point() +
    geom_line(data=biology_filled_in_range, aes(x=hours, y=y_pred_linear), color='red') +
    geom_line(data=biology_filled_in_range, aes(x=hours, y=y_pred_poly), color='blue') +
    lims(x=c(5, 10), y=c(65, 90)) +
    theme_bw()
```
-->

We can evaluate the $R^2$'s for both models' performance on the training data just like before with `glance()`. Which model do you expect to have a better *training* set $R^2$ value?

```{r}
#| code-fold: false
#| echo: true
library(broom)

glance(lm_slr) %>%
    select(r.squared)

glance(lm_poly) %>%
    select(r.squared)
```

Just as we might have guessed from looking at the model fits, the polynomial model has a better $R^2$ value when evaluated on the training set.

### Step 3: Evaluate the model on the testing set.

The real test of predictive power between the two models comes now, when we will make exam score predictions using the *testing* set: data which the model was not used to fit and hasn't seen. 

We will still be using the `predict()` function for this purpose. Now, we can just plug `biology_test` into the `newdata` argument!

```{r}
#| code-fold: false
#| echo: true
score_pred_linear <- predict(lm_slr, newdata = biology_test)
score_pred_poly <- predict(lm_poly, newdata = biology_test)
```

Once these predictions $\hat{y}_i$ are made, we then can use `dplyr` code to:

-   mutate on the predictions to our testing data
-   set up the $R^2$ formula and calculate[^rsquare]. In the code below, we are using the formula

$$R^2 = 1-\frac{\text{RSS}}{\text{TSS}} = 1-\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{\sum_{i=1}^n(y_i-\bar{y})^2}$$

-   We can also calculate $\text{MSE}$ and $\text{RMSE}$ as $\frac{1}{n}\text{RSS}$ and $\frac{1}{n}\sqrt{\text{RSS}}$, respectively. 


```{r}
#| code-fold: false
#| echo: true
biology_test %>%
    mutate(score_pred_linear = score_pred_linear,
           score_pred_poly = score_pred_poly,
           resid_sq_linear = (score - score_pred_linear)^2,
           resid_sq_poly = (score - score_pred_poly)^2) %>%
    summarize(TSS = sum((score - mean(score))^2),
              RSS_linear = sum(resid_sq_linear),
              RSS_poly = sum(resid_sq_poly),
              n = n()) %>%
    mutate(Rsq_linear = 1 - RSS_linear/TSS,
           Rsq_poly = 1 - RSS_poly/TSS,
           MSE_linear = RSS_linear/n,
           MSE_poly = RSS_poly/n,
           RMSE_linear = sqrt(MSE_linear),
           RMSE_poly = sqrt(MSE_poly)) |>
  select(Rsq_linear, Rsq_poly, MSE_linear, MSE_poly)
```

Voila the linear model's test set $R^2$ is better than the polynomial model's test $R^2$! We also see the $\text{MSE}$ for the linear model is lower than that for the polynomial model.


[^rsquare]: Because $\hat{y}_i$ involve information from the training data and $y_i$ and $\bar{y}$ come from the testing data, the decomposition of the sum of squares does not work. So, we cannot interpret testing $R^2$ as we would training $R^2$, and you may have a testing $R^2$ less than 0. However, higher $R^2$ values still signal that the model has good predictive power. 

So which is the better predictive model: Model 1 or Model 2? In terms of training, Model 2 came out of top, but Model 1 won out in testing. 

Again, while training $R^2$ can tell us how well a predictive model explains the structure in the data set upon which it was trained, it is deceptive to use as a metric of true predictive accuracy. The task of prediction is fundamentally one applied to unseen data, so testing $R^2$ is the appropriate metric. Model 1, the simpler model, is the better predictive model. After all, the data we are using looks much better modeled by a line than a five degree polynomial. 



<!-- ## Hyperparameter tuning -->

<!-- Suppose we want to fit a polynomial model for a dataset; how do we pick the degree? -->
<!-- Selecting the degree of a polynomial model is an example of *hyperparameter tuning*. -->
<!-- It's tempted to fit polynomials for a bunch of degrees -- say every degree between 1 and 10 -- then pick the polynomial with the best $r^2$. -->
<!-- The above sections about overfitting should make us suspicious of this idea; larger degree polynomials will always give better $r^2$ values! -->

<!-- We can use what we learned in the previous train/test set section to select the degree. -->
<!-- In detail, we -->

<!-- 1. split our dataset into a training and test set  -->
<!-- 2. fit each polynomial model on the training set and then  -->
<!-- 3. evaluate each polynomial model on the test set. -->

<!-- This procedure is called *tuning with a validation set*; we conventionally call this test set that gets queried a number of times a *validation set*. -->

<!-- #### Example: Physics Exam Scores -->

<!-- Consider the following physics exam scores dataset where the relationship is no longer linear. -->

<!-- ```{r} -->
<!-- #| fig-align: center -->
<!-- #| fig-width: 5 -->
<!-- #| fig-height: 3 -->
<!-- # Setup data -->
<!-- n_samples_physics <- 20 -->

<!-- set.seed(23) -->

<!-- noise <-  rnorm(n=n_samples_physics, mean=0, sd=2) -->

<!-- physics <- tibble(hours=runif(n=n_samples_physics,  -->
<!--                               min=min_n_hours, -->
<!--                               max=max_n_hours), -->
<!--                   score= (hours - 7.5)^3 + 75 + noise) -->

<!-- physics %>%  -->
<!--     ggplot(aes(x=hours, y=score)) + -->
<!--     geom_point() + -->
<!--     ggtitle("Physics class") + -->
<!--     theme_bw() -->
<!-- ``` -->

<!-- We can select the degree using a train/validation set by fitting 10 different models - each using a different degree - and calculating their $r^2$ for the train and test sets separately. -->

<!-- ```{r} -->
<!-- # In this class the focus is not on the code used in this section, but rather -->
<!-- # how the procedure works. If you are curious about the code, read on... -->
<!-- # Two new programming concepts are used: functions and for loops -->

<!-- ######################### -->
<!-- # Digression: functions # -->
<!-- ######################### -->

<!-- # we have not covered functions yet, but they are pretty straightforward -->
<!-- # the name of the function is the first thing, the word function is special, -->
<!-- # all of the arguments go in the parentheses, and the last line of the function is returned -->
<!-- # Here is a simple example of a function that adds two numbers -->
<!-- add_two_nums <- function(a, b){ -->
<!--     a + b -->
<!-- } -->


<!-- # create a function to return the Rsquared value -->
<!-- get_R_sq <- function(model, test_data, test_y){ -->
<!--     y_pred <- predict(model, newdata=test_data) -->

<!--     TSS = sum((test_y - mean(test_y, na.rm=T))^2, na.rm=T) -->
<!--     RSS = sum((test_y - y_pred)^2, na.rm=T) -->

<!--     1 - RSS / TSS -->
<!-- } -->


<!-- ######################## -->
<!-- # train test set split # -->
<!-- ######################## -->

<!-- # Randomly assign observations to either train or test set -->
<!-- physics <- physics %>%  -->
<!--     mutate(set_type=sample(x=c('train', 'test'), -->
<!--                            size=n_samples_physics, replace=TRUE, prob=c(0.8, 0.2))) -->

<!-- physics_train <- physics %>%  -->
<!--     filter(set_type=='train') -->

<!-- physics_test <- physics %>%  -->
<!--     filter(set_type=='test') -->

<!-- ######################## -->
<!-- # Hyperparemter tuning # -->
<!-- ######################## -->

<!-- degrees2evaluate <- 1:10 -->


<!-- models <- c() -->
<!-- Rsq_values_train <- c() -->
<!-- Rsq_values_test <- c() -->
<!-- # each iteration of this for loop evaluates the model for a different polynomial degree  -->
<!-- for (degree in degrees2evaluate){ -->

<!--     # fit the model on the training data with the specified degree -->
<!--     model <- lm(score~poly(hours, degree=degree, raw=T), data=physics_train) -->

<!--     # compute the R squared value with the train and test data -->
<!--     train_rsq <- get_R_sq(model=model, test_data=physics_train, test_y=physics_train$score) -->
<!--     test_rsq <- get_R_sq(model=model, test_data=physics_test, test_y=physics_test$score) -->

<!--     # save everything -->
<!--     # vector <- c(vector, value) will add the value to the end of the vector -->
<!--     # i.e. c does concatenation -->
<!--     models <- c(models, model) -->
<!--     Rsq_values_train <- c(Rsq_values_train, train_rsq) -->
<!--     Rsq_values_test <- c(Rsq_values_test, test_rsq) -->

<!-- } -->

<!-- # we have not yet learned the gather function -->
<!-- # but you can probably figure out what it does by starting at this code -->
<!-- tuning_results = tibble(degree=degrees2evaluate, -->
<!--                         train = Rsq_values_train, -->
<!--                         test = Rsq_values_test) %>% -->
<!--     gather('type', 'Rsq', -degree) -->

<!-- tuning_results -->
<!-- ``` -->

<!-- Finally let's visualize the results. -->

<!-- ```{r} -->
<!-- #| fig-width: 5 -->
<!-- #| fig-height: 4 -->
<!-- #| fig-align: center -->
<!-- tuning_results %>%  -->
<!--     ggplot(aes(x=degree, y=Rsq, color=type)) +  -->
<!--     geom_point() + -->
<!--     geom_line() +  -->
<!--     scale_x_continuous(breaks=degrees2evaluate) + -->
<!--     theme_bw() -->
<!-- ``` -->

<!-- Taking a look at the numerical results we see degree 3 is the best model, though degrees 4 and 5 seem to work pretty well too. -->

<!-- ```{r} -->
<!-- tuning_results %>%  -->
<!--     filter(type=='test') %>%  -->
<!--     arrange(desc(Rsq)) -->
<!-- ``` -->

## Summary

This lecture is about overfitting: what happens when your model takes the particular data set it was built on too seriously. The more complex a model is, the more prone to overfitting it is. Polynomial models are able to create very complex functions thus high-degree polynomial models can easily overfit. Fitting a model and evaluating it on the same data set can be problematic; if the model is overfitted the evaluation metric (e.g. $R^2$) might be very good, but the model might be lousy on predictions on new data.

A better way to approach predictive modeling is to fit the model to a training set then evaluate it with a separate testing set.

[ ]{.column-margin}
