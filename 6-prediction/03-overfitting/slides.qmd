---
title: "Overfitting"
format:
  revealjs:
    author: "STAT 20: Introduction to Probability and Statistics"
    height: 900
    width: 1600
    theme: ../../assets/slides.scss
    multiplex: false
    transition: fade
    slide-number: c
    incremental: false
    center: false
    menu: false
    highlight-style: github
    progress: false
    code-overflow: wrap
    title-slide-attributes:
      data-background-image: ../../assets/stat20-hex-bg.png
      data-background-size: contain
---

## Agenda

- Announcements
- Concept Questions
- Problem Set: Overfitting
- Lab: Cancer Diagnosis

## Announcements

-   Problem Sets:
    -   PS 18: *Overfitting* releases Tuesday and due next Tuesday at 9am
    -   Extra Practice: *Logistic Regression* releases Thursday (non-turn in)
    
. . . 
    
-   Lab 6:
    -   Lab 6.1 releases Tuesday and due next Tuesday at 9am
    -   Lab 6.2 releases Thursday and due next Tuesday at 9am
    
. . . 
    
-   Quiz 4:
    -   next Monday in-class.
    -   covers *Wrong By Design* through *Logistic Regression* (Thu/Fri)
    

# Concept Questions

## RQ 1

```{r}
countdown::countdown(1, top = 0)
```

:::{.poll}
Which one of these (open `pollev.com`) is not an example of overfitting (either in real life or in statistics)?
:::


<!---
## Are RSS and R square related?

```{r}
countdown::countdown(1, top = 0)
```

:::{.poll}
If I decrease the RSS (e.g. by fitting a more accurate model) does the $R^2$ value necessarily increase?
:::


## Overfit models
```{r}
countdown::countdown(.5, top = 0)
```

Fill in the blanks: overfit models tend to

-   fit the training data (well/poor)

-   fit the testing data  (well/poor)
-->


## Where is overfitting worse?

```{r}
countdown::countdown(1, top = 0)
```

:::{.poll}
Suppose I overfit my model to the training data. In which scenario (for which training data) would I expect the test set performance to be significantly worse? Assume that the testing sets A and B look like their corresponding training sets.
:::

```{r}
#| fig-align: center

library(tidyverse)
library(patchwork)
set.seed(1)
n=200
plot_A <- tibble(x=runif(n=n, min=0, max=1)) |>
    mutate(y = x + rnorm(n=n, sd=.25))  |>
    ggplot(aes(x=x, y=y)) +
    geom_point() + 
    ggtitle("Training data A")

plot_B <- tibble(x=runif(n=n, min=0, max=1)) |>
    mutate(y = x + rnorm(n=n, sd=.01))  |>
    ggplot(aes(x=x, y=y)) +
    geom_point() + 
    ggtitle("Training data B")

plot_A + plot_B
```


:::{.notes}
If you overfit the data to training set A, you will likely have a complex curve (maybe a polynomial). This curve will fit poorly a testing set that looks similar to A. If you overfit to B, since the data has low variance you will have a positively sloping line. The testing set performance will be very similar to the training set performance (it will not be markedly worse).

Answer: Training set A
:::

# Problem Set

```{r}
countdown::countdown(25)
```

# Break

```{r}
countdown::countdown(5)
```

# Lab

```{r}
countdown::countdown(25)
```

# End of Lecture
