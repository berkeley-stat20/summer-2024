[
  {
    "objectID": "2-summarizing-data/04-conditioning/ps.html",
    "href": "2-summarizing-data/04-conditioning/ps.html",
    "title": "Data Pipelines",
    "section": "",
    "text": "The following practice problems all deal with a data set that’s near and dear to our hearts, the Palmer penguins dataset. Like always, make sure you start your analysis by loading in the necessary packages and data sets.\nCodeggplot(data = penguins,\n       mapping = aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = species)) +\n    geom_point() +\n  lims(x = c(30, 60),\n       y = c(12, 23))"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/ps.html#questions-1-3",
    "href": "2-summarizing-data/04-conditioning/ps.html#questions-1-3",
    "title": "Data Pipelines",
    "section": "Questions 1-3",
    "text": "Questions 1-3\nQuestion 1*\nExtract a data frame that excludes the Adelie penguins in two different ways:\n\nOne without using a data pipeline;\nOne with using a data pipeline.\n\n\nQuestion 2*\nExtract a data frame that excludes the Adelie penguins and retains penguins with bill lengths between 40 and 50 mm.\n\nQuestion 3\nSort the data frame from Question 2 in decreasing order by bill length."
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/ps.html#questions-4-6",
    "href": "2-summarizing-data/04-conditioning/ps.html#questions-4-6",
    "title": "Data Pipelines",
    "section": "Questions 4-6",
    "text": "Questions 4-6\nQuestion 4\nCalculate the mean bill length and bill depth for each of the three species of penguins. Do those statistics line up with what you see in the penguins plot provided by the (original) code given?\n\nQuestion 5\nConsider a new metric called bill_size that’s the sum of the length and depth. What is the average bill size and it’s standard deviation among each species, broken out among each of the island? You may end up with potentially nine pairs of statistics. Sort your resulting data structure in decreasing order by average bill size.\n\nQuestion 6\nWhat are the total number of penguins in the data set belonging to each species-island combination? Why may have you not gotten nine pairs of statistics in the last question?"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/ps.html#question-7",
    "href": "2-summarizing-data/04-conditioning/ps.html#question-7",
    "title": "Data Pipelines",
    "section": "Question 7",
    "text": "Question 7\nWhat is the proportion of penguins for each species having a body mass greater than 4000 grams? Hint: use a logical variable!"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/notes.html",
    "href": "2-summarizing-data/04-conditioning/notes.html",
    "title": "Data Pipelines",
    "section": "",
    "text": "At this stage in the course, the number of functions that you are familiar with has grown dramatically. To do truly powerful things with data, you need to not just call one of these functions, but string together many of them in a thoughtful and organized manner.\nAn an example, let’s return to our work with the penguins dataset from last lecture. In order to calculate the mean and standard deviation of bill length across each species of penguin, we need to take the original data frame and\n\nUse group_by() to inform R we would like to calculate summaries across levels of a categorical variable (namely, the species variable).\nUse summarise() to calculate these summaries.\n\nA conventional approach breaks this process into two distinct lines of code and saves the output mid-way through.\n\ngrouped_penguins &lt;- group_by(penguins, species)\nsummarise(grouped_penguins, \n          bill_length_mean = mean(bill_length_mm),\n          bill_length_sd = sd(bill_length_mm))\n\n# A tibble: 3 × 3\n  species   bill_length_mean bill_length_sd\n  &lt;fct&gt;                &lt;dbl&gt;          &lt;dbl&gt;\n1 Adelie                38.8           2.66\n2 Chinstrap             48.8           3.34\n3 Gentoo                47.6           3.11\n\n\nAn approach that is more concise, easier to read, and generally faster to run is to compose these functions together with “the pipe”. The pipe, written |&gt;, is an operator that you have access to when you load the tidyverse package. If you have two functions, f1 and f2, both of which take a data frame as the first argument, you can pipe the output of f1 directly into f2 using.\n\nf1(DF) |&gt; f2()\n\nLet’s use the pipe to rewrite the code shown above.\n\ngroup_by(penguins, species) |&gt; \n  summarise(bill_length_mean = mean(bill_length_mm),\n            bill_length_sd = sd(bill_length_mm))\n\n# A tibble: 3 × 3\n  species   bill_length_mean bill_length_sd\n  &lt;fct&gt;                &lt;dbl&gt;          &lt;dbl&gt;\n1 Adelie                38.8           2.66\n2 Chinstrap             48.8           3.34\n3 Gentoo                47.6           3.11\n\n\nThe first function, group_by(), is unchanged. However the second function, summarise(), is now missing its first argument, the data frame. That is because it is being piped directly in from the output of the first function.\nWhile this is a fine way to use the pipe, your code is made much more readable if you format it like this:\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarise(bill_length_mean = mean(bill_length_mm),\n            bill_length_sd = sd(bill_length_mm))\n\n# A tibble: 3 × 3\n  species   bill_length_mean bill_length_sd\n  &lt;fct&gt;                &lt;dbl&gt;          &lt;dbl&gt;\n1 Adelie                38.8           2.66\n2 Chinstrap             48.8           3.34\n3 Gentoo                47.6           3.11\n\n\nFor the rest of the examples given in the notes, we will stick with this formatting when using the pipe operator. This code results in the same output as the first version, but it now reads a bit like a poem: “Take the penguins data frame, prepare it for calculations across each species, then calculate the mean and standard deviation of bill length (across each species of penguin)”.\n\n\nThis poem is admittedly not particularly poetic.\n\n\n\n\n\nHere’s another, less poetic way to think about the pipe, as described by the above image! Most claims about data start with a raw data set, undergo many subsetting, aggregating, and cleaning operations, then return a data product. Each one of these operations can be represented by one of the turns through the pipe. It’s good practice to understand the output of each line of code by breaking the pipe. With regards to our previous example, we can remove the summarise() and just look at the output of the group_by() step.\n\npenguins |&gt;\n  group_by(species)\n\n# A tibble: 333 × 8\n# Groups:   species [3]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ 323 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThis looks . . . exactly like the original data frame. Well, not exactly like it: there is now a note at the top that the data frame now has the notion of groups based on species. In effect, group_by() has taken the generic data frame and turned it into the one in the middle below: the same data frame but with rows now flagged as belonging to one group or another. When we pipe this grouped data frame into summarise(), summarise() collapses that data frame down into a single row for each group and creates a new column for each new summary statistic.\n\n\n\n\nIn addition to providing greater insight on each step in your data pipeline, breaking the pipe is a fantastic way to troubleshoot any errors in your pipeline. If you run just the first piece of your pipeline and it works, add the second piece and try again. Continue this process with the rest of the pieces until the error occurs. When you run into the error, the piece you just added is the guilty party."
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/notes.html#introducing-the-pipe",
    "href": "2-summarizing-data/04-conditioning/notes.html#introducing-the-pipe",
    "title": "Data Pipelines",
    "section": "",
    "text": "At this stage in the course, the number of functions that you are familiar with has grown dramatically. To do truly powerful things with data, you need to not just call one of these functions, but string together many of them in a thoughtful and organized manner.\nAn an example, let’s return to our work with the penguins dataset from last lecture. In order to calculate the mean and standard deviation of bill length across each species of penguin, we need to take the original data frame and\n\nUse group_by() to inform R we would like to calculate summaries across levels of a categorical variable (namely, the species variable).\nUse summarise() to calculate these summaries.\n\nA conventional approach breaks this process into two distinct lines of code and saves the output mid-way through.\n\ngrouped_penguins &lt;- group_by(penguins, species)\nsummarise(grouped_penguins, \n          bill_length_mean = mean(bill_length_mm),\n          bill_length_sd = sd(bill_length_mm))\n\n# A tibble: 3 × 3\n  species   bill_length_mean bill_length_sd\n  &lt;fct&gt;                &lt;dbl&gt;          &lt;dbl&gt;\n1 Adelie                38.8           2.66\n2 Chinstrap             48.8           3.34\n3 Gentoo                47.6           3.11\n\n\nAn approach that is more concise, easier to read, and generally faster to run is to compose these functions together with “the pipe”. The pipe, written |&gt;, is an operator that you have access to when you load the tidyverse package. If you have two functions, f1 and f2, both of which take a data frame as the first argument, you can pipe the output of f1 directly into f2 using.\n\nf1(DF) |&gt; f2()\n\nLet’s use the pipe to rewrite the code shown above.\n\ngroup_by(penguins, species) |&gt; \n  summarise(bill_length_mean = mean(bill_length_mm),\n            bill_length_sd = sd(bill_length_mm))\n\n# A tibble: 3 × 3\n  species   bill_length_mean bill_length_sd\n  &lt;fct&gt;                &lt;dbl&gt;          &lt;dbl&gt;\n1 Adelie                38.8           2.66\n2 Chinstrap             48.8           3.34\n3 Gentoo                47.6           3.11\n\n\nThe first function, group_by(), is unchanged. However the second function, summarise(), is now missing its first argument, the data frame. That is because it is being piped directly in from the output of the first function.\nWhile this is a fine way to use the pipe, your code is made much more readable if you format it like this:\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarise(bill_length_mean = mean(bill_length_mm),\n            bill_length_sd = sd(bill_length_mm))\n\n# A tibble: 3 × 3\n  species   bill_length_mean bill_length_sd\n  &lt;fct&gt;                &lt;dbl&gt;          &lt;dbl&gt;\n1 Adelie                38.8           2.66\n2 Chinstrap             48.8           3.34\n3 Gentoo                47.6           3.11\n\n\nFor the rest of the examples given in the notes, we will stick with this formatting when using the pipe operator. This code results in the same output as the first version, but it now reads a bit like a poem: “Take the penguins data frame, prepare it for calculations across each species, then calculate the mean and standard deviation of bill length (across each species of penguin)”.\n\n\nThis poem is admittedly not particularly poetic.\n\n\n\n\n\nHere’s another, less poetic way to think about the pipe, as described by the above image! Most claims about data start with a raw data set, undergo many subsetting, aggregating, and cleaning operations, then return a data product. Each one of these operations can be represented by one of the turns through the pipe. It’s good practice to understand the output of each line of code by breaking the pipe. With regards to our previous example, we can remove the summarise() and just look at the output of the group_by() step.\n\npenguins |&gt;\n  group_by(species)\n\n# A tibble: 333 × 8\n# Groups:   species [3]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ 323 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThis looks . . . exactly like the original data frame. Well, not exactly like it: there is now a note at the top that the data frame now has the notion of groups based on species. In effect, group_by() has taken the generic data frame and turned it into the one in the middle below: the same data frame but with rows now flagged as belonging to one group or another. When we pipe this grouped data frame into summarise(), summarise() collapses that data frame down into a single row for each group and creates a new column for each new summary statistic.\n\n\n\n\nIn addition to providing greater insight on each step in your data pipeline, breaking the pipe is a fantastic way to troubleshoot any errors in your pipeline. If you run just the first piece of your pipeline and it works, add the second piece and try again. Continue this process with the rest of the pieces until the error occurs. When you run into the error, the piece you just added is the guilty party."
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/notes.html#the-pipe-in-action",
    "href": "2-summarizing-data/04-conditioning/notes.html#the-pipe-in-action",
    "title": "Data Pipelines",
    "section": "The pipe in action",
    "text": "The pipe in action\nWe will now look at a few examples to understand the power of such a simple piece of syntax. In doing so, we will introduce a few more dplyr functions that will expand your ability to perform more specific pieces of data analysis.\nOur data set of choice begins with a very general focus. In 2007, Savage and West published A qualitative, theoretical framework for understanding mammalian sleep1, wherein they “develop a general, quantitative theory for mammalian sleep that relates many of its fundamental parameters to metabolic rate and body size”. Characterizing the sleep patterns of all mammals is a broad task and their data set is corresponding diverse. Take a look at the first ten rows of their data below.\n\n\n# A tibble: 83 × 5\n   name                       sleep_total log_bodywt vore  conservation\n   &lt;chr&gt;                            &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n 1 Cheetah                           12.1      10.8  carni lc          \n 2 Owl monkey                        17         6.17 omni  &lt;NA&gt;        \n 3 Mountain beaver                   14.4       7.21 herbi nt          \n 4 Greater short-tailed shrew        14.9       2.94 omni  lc          \n 5 Cow                                4        13.3  herbi domesticated\n 6 Three-toed sloth                  14.4       8.26 herbi &lt;NA&gt;        \n 7 Northern fur seal                  8.7       9.93 carni vu          \n 8 Vesper mouse                       7         3.81 &lt;NA&gt;  &lt;NA&gt;        \n 9 Dog                               10.1       9.55 carni domesticated\n10 Roe deer                           3         9.60 herbi lc          \n# ℹ 73 more rows\n\n\nIn this data set, the unit of observation is a single species and the variables observed on each are its name, the average length of sleep each day, the natural log of the average weight, its dietary pattern, and its conservation status. We can visualize the relationship between sleep and body size in all 83 species using a scatter plot.\n\n\n\n\n\n\n\n\nThe mammals vary from the wee brown bat, slumbering for nearly 20 hours a day, to the massive African elephant, nodding off for less than five. That is quite a range! Lets drill down to smaller subsets of this data frame to gain a more nuanced sense of what is going on.\nExample 1: Mutation\nOftentimes the variables we are most interested in analyzing in a data set don’t exist in the form most conducive for doing so. Other times, they don’t exist at all. For example, note how in our first output of the msleep data and the scatter plot that body weight is being presented in log grams (log_bodywt). In reality, the body weight as originally present in the msleep dataset is called bodywt and is recorded in kilograms! To get to log_bodywt, we will perform what is called a mutation.\nMutation\nThe act of creating a new column in a dataset based on information in existing column(s).\nThere are a variety of different mutations that you can apply to a column(s) in a dataset. In the last lecture, we overwrote an existing column with an updated version of itself. Today, we will perform a mathematical mutation on the bodywt column.\n\nmsleep |&gt;\n    mutate(log_bodywt = log(bodywt * 1000)) |&gt;\n    select(log_bodywt, bodywt)\n\n# A tibble: 83 × 2\n   log_bodywt  bodywt\n        &lt;dbl&gt;   &lt;dbl&gt;\n 1      10.8   50    \n 2       6.17   0.48 \n 3       7.21   1.35 \n 4       2.94   0.019\n 5      13.3  600    \n 6       8.26   3.85 \n 7       9.93  20.5  \n 8       3.81   0.045\n 9       9.55  14    \n10       9.60  14.8  \n# ℹ 73 more rows\n\n\nHere, the first argument, msleep, is piped to the dplyr function mutate() and the second argument, log_bodywt, creates a new column named log_bodywt by multiplying the bodywt column by 100 (converting kilograms to grams) and then taking the natural log. We then pipe this output into select() to compare the two columns.\nYou can use mutate() to create multiple columns at the same time:\n\nmsleep |&gt;\n    mutate(log_bodywt = log(bodywt * 1000),\n           sleep_total_min = sleep_total*60) |&gt;\n    select(log_bodywt, bodywt,\n           sleep_total, sleep_total_min)\n\n# A tibble: 83 × 4\n   log_bodywt  bodywt sleep_total sleep_total_min\n        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n 1      10.8   50            12.1             726\n 2       6.17   0.48         17              1020\n 3       7.21   1.35         14.4             864\n 4       2.94   0.019        14.9             894\n 5      13.3  600             4               240\n 6       8.26   3.85         14.4             864\n 7       9.93  20.5           8.7             522\n 8       3.81   0.045         7               420\n 9       9.55  14            10.1             606\n10       9.60  14.8           3               180\n# ℹ 73 more rows\n\n\nIf you plan to use these new columns after creating them more than once, it’s best to save them back into the original dataset!\n\nmsleep &lt;- msleep |&gt;\n  mutate(log_bodywt = log(bodywt * 1000),\n           sleep_total_min = sleep_total*60) \n\nExample 2: Filtering\nIf you think about the shape of a data frame, there are two basic ways you might go about slicing and dicing it into smaller subsets.\nOne way is to go at it is column-by-column. The act of selecting a subset of the columns of a data frame is called, well, selecting. This is what we touched on briefly in the last lecture and in the previous example. When you select a column, you can do so either by its name or by its column number (or index). Selecting columns by name is more useful because their order tends to be arbitrary and might change over the course of an analysis.\nThe other way to go at it is row-by-row. The act of subsetting the rows of the data frame based on their row number is called slicing. As with columns, the order of the rows is also often arbitrary, so this is of limited use. Much more useful is filtering.\n\n\nIn the tidyverse, these functions are named select(), slice(), and filter().\n\nFiltering\n\nThe act of subsetting the rows of a data frame based on the values of one or more variables to extract the observations of interest.\n\n\nFilters are powerful because they comb through the values of the data frame, which is where most of the information is. The key part of any filter is the condition that you assert for the rows that are retained in your data frame. Let’s set up a filter to return only the little brown bat.\n\nmsleep |&gt;\n  filter(name == \"Little brown bat\")\n\n# A tibble: 1 × 13\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Little… Myot… inse… Chir… &lt;NA&gt;                19.9         2         0.2   4.1\n# ℹ 4 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, log_bodywt &lt;dbl&gt;,\n#   sleep_total_min &lt;dbl&gt;\n\n\nHere name == \"Little brown bat\" is the condition that must be met by any row in the data set to be retained. The syntax used to set up the condition is a comparison between a column in the data frame on the left and a possible value of that column on the right.\nExample 2 Detour: Comparison Operators\nThe filter above uses the most direct condition: it retains the rows that have a value in the name variable that is precisely \"Little brown bat\". In this case, there is only one such row. There are a range of different comparisons that can be made, though, and each has its own operator.\n\n\n\n\n\n\nOperator\nTranslation\n\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n&lt;\nless than\n\n\n&gt;\ngreater than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;=\ngreater than or equal to\n\n\n\n\n\n\n\nAt first, the == operator looks like a typo. Why doesn’t we use =? The reason is that a single equals sign is already busy at work in R: it sets the values of arguments inside a function. Instead of assignment, we want to determine whether the thing on the left holds the same value as the thing on the right, so we use ==. It might help you keep things straight if you read it in your head as “is exactly equal to”.\nLet’s return only the rows with large animals, defined as those with a log body weight greater than 12.\n\nmsleep |&gt;\n  filter(log_bodywt &gt; 12)\n\n# A tibble: 9 × 13\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n2 Asian … Elep… herbi Prob… en                   3.9      NA        NA      20.1\n3 Horse   Equus herbi Peri… domesticated         2.9       0.6       1      21.1\n4 Donkey  Equus herbi Peri… domesticated         3.1       0.4      NA      20.9\n5 Giraffe Gira… herbi Arti… cd                   1.9       0.4      NA      22.1\n6 Pilot … Glob… carni Ceta… cd                   2.7       0.1      NA      21.4\n7 Africa… Loxo… herbi Prob… vu                   3.3      NA        NA      20.7\n8 Brazil… Tapi… herbi Peri… vu                   4.4       1         0.9    19.6\n9 Bottle… Turs… carni Ceta… &lt;NA&gt;                 5.2      NA        NA      18.8\n# ℹ 4 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, log_bodywt &lt;dbl&gt;,\n#   sleep_total_min &lt;dbl&gt;\n\n\nThere were 9 such animals and you can see all of them are large.\nExample 2 Detour: Logical Operators\nWhat if you want both the little brown bat and the African elephant? What if you want both the large creatures as well as those that sleep only briefly? These are tasks that call for multiple comparisons composed together with the logical operators &, |, and %in%.\nThis filter returns the creatures who are large and who sleep little.\n\nmsleep |&gt;\n  filter(log_bodywt &gt; 12 & sleep_total &lt; 5)\n\n# A tibble: 8 × 13\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n2 Asian … Elep… herbi Prob… en                   3.9      NA        NA      20.1\n3 Horse   Equus herbi Peri… domesticated         2.9       0.6       1      21.1\n4 Donkey  Equus herbi Peri… domesticated         3.1       0.4      NA      20.9\n5 Giraffe Gira… herbi Arti… cd                   1.9       0.4      NA      22.1\n6 Pilot … Glob… carni Ceta… cd                   2.7       0.1      NA      21.4\n7 Africa… Loxo… herbi Prob… vu                   3.3      NA        NA      20.7\n8 Brazil… Tapi… herbi Peri… vu                   4.4       1         0.9    19.6\n# ℹ 4 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, log_bodywt &lt;dbl&gt;,\n#   sleep_total_min &lt;dbl&gt;\n\n\nThis can be read as “filter the msleep data frame to return the rows where both the log body weight is greater than 12 and the sleep total is less than 5”. We see that there are 8 such creatures, one fewer than the data frame with only the body weight filter (bottle-nosed dolphins sleep, on average, 5.2 hrs).\nUsing & to represent “and” is common across most computer languages but you can alternatively use the somewhat more compact syntax of simply adding the second filter after a comma.\n\nmsleep |&gt;\n  filter(log_bodywt &gt; 12,\n         sleep_total &lt; 5)\n\n# A tibble: 8 × 13\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n2 Asian … Elep… herbi Prob… en                   3.9      NA        NA      20.1\n3 Horse   Equus herbi Peri… domesticated         2.9       0.6       1      21.1\n4 Donkey  Equus herbi Peri… domesticated         3.1       0.4      NA      20.9\n5 Giraffe Gira… herbi Arti… cd                   1.9       0.4      NA      22.1\n6 Pilot … Glob… carni Ceta… cd                   2.7       0.1      NA      21.4\n7 Africa… Loxo… herbi Prob… vu                   3.3      NA        NA      20.7\n8 Brazil… Tapi… herbi Peri… vu                   4.4       1         0.9    19.6\n# ℹ 4 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, log_bodywt &lt;dbl&gt;,\n#   sleep_total_min &lt;dbl&gt;\n\n\nThese two methods are equivalent.\nTo return all rows that either have a high body weight or low sleep time or both, use the | operator (sometimes called “vertical bar”).\n\nmsleep |&gt;\n  filter(log_bodywt &gt; 12 | sleep_total &lt; 5)\n\n# A tibble: 12 × 13\n   name   genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 Cow    Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n 2 Roe d… Capr… herbi Arti… lc                   3        NA        NA      21  \n 3 Asian… Elep… herbi Prob… en                   3.9      NA        NA      20.1\n 4 Horse  Equus herbi Peri… domesticated         2.9       0.6       1      21.1\n 5 Donkey Equus herbi Peri… domesticated         3.1       0.4      NA      20.9\n 6 Giraf… Gira… herbi Arti… cd                   1.9       0.4      NA      22.1\n 7 Pilot… Glob… carni Ceta… cd                   2.7       0.1      NA      21.4\n 8 Afric… Loxo… herbi Prob… vu                   3.3      NA        NA      20.7\n 9 Sheep  Ovis  herbi Arti… domesticated         3.8       0.6      NA      20.2\n10 Caspi… Phoca carni Carn… vu                   3.5       0.4      NA      20.5\n11 Brazi… Tapi… herbi Peri… vu                   4.4       1         0.9    19.6\n12 Bottl… Turs… carni Ceta… &lt;NA&gt;                 5.2      NA        NA      18.8\n# ℹ 4 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, log_bodywt &lt;dbl&gt;,\n#   sleep_total_min &lt;dbl&gt;\n\n\nBe cautious in deciding whether you want to use & or |. While | is generally read as “or”, we could also describe the above filter as one that returns the rows that have a high body weight and the rows that have low sleep times.\nOne way to keep them straight is to keep an eye on the number of observations that are returned. The intersection of multiple conditions (using &) should result in the same or fewer rows (the orange area) than the union of multiple conditions (using |) (the blue area).\n\n\n\n\n\n\n\n\nWhen working with nominal categorical variables, the only operator that you’ll be using is ==. You can return a union like normal using |,\n\nmsleep |&gt;\n  filter(name == \"Little brown bat\" | name == \"African elephant\")\n\n# A tibble: 2 × 13\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Africa… Loxo… herbi Prob… vu                   3.3        NA        NA    20.7\n2 Little… Myot… inse… Chir… &lt;NA&gt;                19.9         2         0.2   4.1\n# ℹ 4 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, log_bodywt &lt;dbl&gt;,\n#   sleep_total_min &lt;dbl&gt;\n\n\nOr you can save some typing (and craft more readable code) by using %in% instead:\n\nmsleep |&gt;\n  filter(name %in% c(\"Little brown bat\", \"African elephant\"))\n\n# A tibble: 2 × 13\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Africa… Loxo… herbi Prob… vu                   3.3        NA        NA    20.7\n2 Little… Myot… inse… Chir… &lt;NA&gt;                19.9         2         0.2   4.1\n# ℹ 4 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, log_bodywt &lt;dbl&gt;,\n#   sleep_total_min &lt;dbl&gt;\n\n\nTaxonomy of Data: Logicals\nIt is useful to pause here to look under the hood of this code. Once you get accustomed to the comparison operators and the syntax, the R code reads very similarly to the equivalent English command. But how are those comparisons being represented in terms of data?\nTo answer this question, consider a simple numeric vector of four integers.\n\na &lt;- c(2, 4, 6, 8)\n\nWe can apply a comparison operator to this vector using the same syntax as above. Let’s compare each value in this vector to see if its less than 5.\n\na &lt; 5\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\nThe result is a vector of the same length as a where each value indicates whether the comparison to each element was true or false. While it looks like a factor or a character vector TRUE and FALSE, this is actually our newest entry into the Taxonomy of Data: the logical vector.\n\nclass(a &lt; 5)\n\n[1] \"logical\"\n\n\nA logical vector can only take two values, TRUE and FALSE (R also recognizes T and F but not True or true). While it might seem like a categorical variable with only two levels, a logical vector has an important property that makes it behave like a numerical variable.\n\nsum(a &lt; 5)\n\n[1] 2\n\n\nIn a logical vector, a value of true is represented both by TRUE and by the number 1 and false by FALSE and the number 0. This integer representation is why TRUE + TRUE will work (it’s 2!) but \"TRUE\" + \"TRUE\" will not.\nThis dual representation is very useful because it allows us to compute a proportion using, paradoxically, the mean() function.\n\nmean(a &lt; 5)\n\n[1] 0.5\n\n\na &lt; 5 results in a vector with two 1s and two 0s. When you take the mean like this, you’re really finding the proportion of the elements that meet the condition that you laid out in your comparison. This is a very handy trick. Let’s apply it to the msleep dataset as part of our next example.\nExample 3\nIn the following data pipeline, we will\n\nSplit our mammals up by diet (carnivore, herbivore, omnivore, etc.)\nCalculate the proportion of mammals in each diet group that sleep over eight hours in the day\nCalculate the number of mammals in each group\n\nThis will require both group_by() and summarise(), as well as our handy trick discussed above and a new summary function.\n\nmsleep |&gt;\n  group_by(vore) |&gt;\n  summarise(p_gt_8hrs = mean(sleep_total &gt; 8),\n            n = n())\n\n# A tibble: 5 × 3\n  vore    p_gt_8hrs     n\n  &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;\n1 carni       0.684    19\n2 herbi       0.594    32\n3 insecti     1         5\n4 omni        0.95     20\n5 &lt;NA&gt;        0.714     7\n\n\n\nIn the above code, sleep_total &gt; 8 creates a vector of TRUEs and FALSEs depending on whether the mammal slept for over eight hours a day. mean() then treats these TRUEs and FALSEs like 1s and 0s, and produces a proportion of mammals who sleep over eight hours a day in each diet group.\nThe n() summary statistic counts the number of observations within each group. For example, there are nineteen carnivores in the msleep dataset. Based on this, we can deduce that thirteen carnviores satisfied our sleeping condition. This means that when calculating the proportion for carnivores, the mean() function is taking the average of thirteen 1s/TRUEs and six 0s/FALSEs!\n\nThe code written above could be equivalently written as follows:\n\nmsleep |&gt;\n  mutate(long_sleep = sleep_total &gt; 8) |&gt;\n  group_by(vore) |&gt;\n  summarise(p_gt_8hrs = mean(long_sleep),\n            n = n())\n\nIn this version, the sleep_total &gt; 8 vector is made into a new column using mutate() called long_sleep. This is a column of TRUEs and FALSEs, and we can then take its mean directly in the summarise() step.\n\n\n\n\n\n\n\n\n\n\nExample 4: Arranging\nSometimes, we are interested in arranging the rows of a data frame according to some logical ordering of a column. This ordering is straightforward for numeric columns; the smallest numbers should be placed first and ascend to the larger ones (or vice versa). We might also think about what happens when passing in a column of characters. Luckily, the dplyr package has a catch-all solution to this in the form of the arrange() function.\nLet’s take a look:\n\nmsleep |&gt;\n  arrange(sleep_total) |&gt;\n  select(name, sleep_total)\n\n# A tibble: 83 × 2\n   name             sleep_total\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 Giraffe                  1.9\n 2 Pilot whale              2.7\n 3 Horse                    2.9\n 4 Roe deer                 3  \n 5 Donkey                   3.1\n 6 African elephant         3.3\n 7 Caspian seal             3.5\n 8 Sheep                    3.8\n 9 Asian elephant           3.9\n10 Cow                      4  \n# ℹ 73 more rows\n\n\nFrom this output, we can see that the giraffe sleeps for the smallest amount of time per day (not even two hours)! But maybe we are interested in which mammal sleeps the longest. If this is the case, we can modify the arrange() function slightly by wrapping our column of interest within desc().\n\nmsleep |&gt;\n  arrange(desc(sleep_total)) |&gt;\n  select(name, sleep_total)\n\n# A tibble: 83 × 2\n   name                           sleep_total\n   &lt;chr&gt;                                &lt;dbl&gt;\n 1 Little brown bat                      19.9\n 2 Big brown bat                         19.7\n 3 Thick-tailed opposum                  19.4\n 4 Giant armadillo                       18.1\n 5 North American Opossum                18  \n 6 Long-nosed armadillo                  17.4\n 7 Owl monkey                            17  \n 8 Arctic ground squirrel                16.6\n 9 Golden-mantled ground squirrel        15.9\n10 Tiger                                 15.8\n# ℹ 73 more rows\n\n\nWhat if you pass a column of characters to arrange()?\n\nmsleep |&gt;\n  arrange(vore) |&gt;\n  select(name, vore, sleep_total)\n\n# A tibble: 83 × 3\n   name                       vore  sleep_total\n   &lt;chr&gt;                      &lt;chr&gt;       &lt;dbl&gt;\n 1 Cheetah                    carni        12.1\n 2 Northern fur seal          carni         8.7\n 3 Dog                        carni        10.1\n 4 Long-nosed armadillo       carni        17.4\n 5 Domestic cat               carni        12.5\n 6 Pilot whale                carni         2.7\n 7 Gray seal                  carni         6.2\n 8 Thick-tailed opposum       carni        19.4\n 9 Slow loris                 carni        11  \n10 Northern grasshopper mouse carni        14.5\n# ℹ 73 more rows\n\n\nWhen arranged by vore, carni comes first, and the rest of the diet groups will follow in alphabetical order. The mammals aren’t arranged in any specific order within a diet group, but we can change that by passing another column to arrange(). Passing additional columns to arrange() will systematically break ties. The below code arranges the data frame first by diet group (from A to Z) and then breaks ties by (ascending) sleep time:\n\nmsleep |&gt;\n  arrange(vore, sleep_total) |&gt;\n  select(name, vore, sleep_total)\n\n# A tibble: 83 × 3\n   name                 vore  sleep_total\n   &lt;chr&gt;                &lt;chr&gt;       &lt;dbl&gt;\n 1 Pilot whale          carni         2.7\n 2 Caspian seal         carni         3.5\n 3 Bottle-nosed dolphin carni         5.2\n 4 Common porpoise      carni         5.6\n 5 Gray seal            carni         6.2\n 6 Genet                carni         6.3\n 7 Northern fur seal    carni         8.7\n 8 Red fox              carni         9.8\n 9 Dog                  carni        10.1\n10 Jaguar               carni        10.4\n# ℹ 73 more rows"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/notes.html#summary",
    "href": "2-summarizing-data/04-conditioning/notes.html#summary",
    "title": "Data Pipelines",
    "section": "Summary",
    "text": "Summary\nIf you’re thinking, 😬 , yikes there was a lot of coding in these notes, you’re right. Don’t worry. We’ll have plenty of time to practice in class! Here’s a recap of what we went through.\nAs we begin to do analyses that require multiple operations, the pipe operator, |&gt;, can be used to stitch the functions together into a single pipeline. With the pipe operator in tow, we then introduced some new types of data operations.\nWe can express existing variables in new ways or create new variables altogether by performing a mutation. There are several ways to subset a data frame but the most important for data analysis is filtering: subsetting the rows according to a condition. In R, that condition is framed in terms of a comparison between a variable and a value (or set of values). Comparisons take many forms and can be combined using logical operators. The result is a logical vector that can be used for filtering or computing summary statistics. Finally, we learned to arrange a data frame by the ordering of a column."
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/notes.html#section",
    "href": "2-summarizing-data/04-conditioning/notes.html#section",
    "title": "Data Pipelines",
    "section": "—————————",
    "text": "—————————"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/notes.html#the-ideas-in-code",
    "href": "2-summarizing-data/04-conditioning/notes.html#the-ideas-in-code",
    "title": "Data Pipelines",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nSome notes rely heavily on code to augment your learning and understanding of the main concepts. This “Ideas in Code” section is meant to expand more on concepts and functions that the notes utilize but may not fully explain.\nThis specific set of notes contains references to many functions from the tidyverse library such as mutate(), select() filter(), arrange(), ggplot(), group_by(), summarize(). We delve more into some of these functions here.\nmutate()\nThis function allows you to create a new column in a dataframe. In typical tidyverse fashion, the first argument is a dataframe. The second argument names and defines how that new column is created. Above, we saw:\n\narbuthnot %&gt;%\n    mutate(total = boys + girls) %&gt;%\n    arrange(desc(total)) %&gt;%\n    select(year, total)\n\n# A tibble: 82 × 2\n    year total\n   &lt;int&gt; &lt;int&gt;\n 1  1705 16145\n 2  1707 16066\n 3  1698 16052\n 4  1708 15862\n 5  1697 15829\n 6  1702 15687\n 7  1701 15616\n 8  1703 15448\n 9  1706 15369\n10  1699 15363\n# ℹ 72 more rows\n\n\nHere, the first argument, arbuthnot, is piped to mutate() and the second argument, total = boys + girls, creates a new column named total by adding together the columns boys and girls. You can use mutate() to create multiple columns at the same time:\n\narbuthnot %&gt;%\n    mutate(total = boys + girls,\n           girl_proportion = girls / total) %&gt;%\n    arrange(desc(total)) %&gt;%\n    select(year, total, girl_proportion)\n\n# A tibble: 82 × 3\n    year total girl_proportion\n   &lt;int&gt; &lt;int&gt;           &lt;dbl&gt;\n 1  1705 16145           0.482\n 2  1707 16066           0.478\n 3  1698 16052           0.475\n 4  1708 15862           0.481\n 5  1697 15829           0.491\n 6  1702 15687           0.488\n 7  1701 15616           0.481\n 8  1703 15448           0.497\n 9  1706 15369           0.483\n10  1699 15363           0.485\n# ℹ 72 more rows\n\n\nNote that switching the order of the two new columns created above such that girl_proportion = girls / total comes before total = boys + girls will produce an error because total is used before it is created.\nselect()\nThis function is defined above as “selecting a subset of the columns of a data frame.” You’ve seen how to use select() to select or “grab” certain columns, but you can also use select() to omit certain columns. The last block of code can be rewritten to produce the same output by placing a minus sign, -, in front of the columns to omit:\n\narbuthnot %&gt;%\n    mutate(total = boys + girls,\n           girl_proportion = girls / total) %&gt;%\n    arrange(desc(total)) %&gt;%\n    select(-c(boys, girls))\n\n# A tibble: 82 × 3\n    year total girl_proportion\n   &lt;int&gt; &lt;int&gt;           &lt;dbl&gt;\n 1  1705 16145           0.482\n 2  1707 16066           0.478\n 3  1698 16052           0.475\n 4  1708 15862           0.481\n 5  1697 15829           0.491\n 6  1702 15687           0.488\n 7  1701 15616           0.481\n 8  1703 15448           0.497\n 9  1706 15369           0.483\n10  1699 15363           0.485\n# ℹ 72 more rows\n\n\narrange()\nThis function arranges the rows of a data frame according to some logical ordering of a column. This ordering is straightforward for numeric columns; the smallest numbers are placed first and ascend to the larger ones. That is, unless you use desc() (which stands for descending).\nBut what if you pass a column of characters to arrange()? Let’s take a look:\n\npenguins %&gt;%\n  arrange(species) %&gt;%\n  select(species, island, bill_length_mm)\n\n# A tibble: 333 × 3\n   species island    bill_length_mm\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;\n 1 Adelie  Torgersen           39.1\n 2 Adelie  Torgersen           39.5\n 3 Adelie  Torgersen           40.3\n 4 Adelie  Torgersen           36.7\n 5 Adelie  Torgersen           39.3\n 6 Adelie  Torgersen           38.9\n 7 Adelie  Torgersen           39.2\n 8 Adelie  Torgersen           41.1\n 9 Adelie  Torgersen           38.6\n10 Adelie  Torgersen           34.6\n# ℹ 323 more rows\n\n\nWhen arranged by species, Adelie penguins come first, followed by Chinstrap, then Gentoo. The penguins aren’t arranged in any specific order within a species, but we can change that by passing another column to arrange(). Passing additional columns to arrange() will systematically break ties. The below code arranges the data frame first by species (alphabetically) and then breaks ties by (ascending) bill length:\n\npenguins %&gt;%\n  arrange(species, bill_length_mm) %&gt;%\n  select(species, island, bill_length_mm)\n\n# A tibble: 333 × 3\n   species island    bill_length_mm\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;\n 1 Adelie  Dream               32.1\n 2 Adelie  Dream               33.1\n 3 Adelie  Torgersen           33.5\n 4 Adelie  Dream               34  \n 5 Adelie  Torgersen           34.4\n 6 Adelie  Biscoe              34.5\n 7 Adelie  Torgersen           34.6\n 8 Adelie  Torgersen           34.6\n 9 Adelie  Biscoe              35  \n10 Adelie  Biscoe              35  \n# ℹ 323 more rows\n\n\nsummarize()\nThis function summarizes a data frame into a single row. We can summarize a data frame by taking means or calculating the number of rows as above. We can also do other calculations like taking a median or calculating the variance of a column:\n\nmsleep %&gt;%\n    summarize(median_sleep = median(sleep_total),\n              variance_sleep = var(sleep_total),\n              n = n())\n\n# A tibble: 1 × 3\n  median_sleep variance_sleep     n\n         &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;\n1         10.1           19.8    83\n\n\nHowever, if summarize() is preceded by group_by(), then it will output multiple rows according to groups specified by group_by():\n\nmsleep %&gt;%\n    group_by(vore) %&gt;%\n    summarize(median_sleep = median(sleep_total),\n              variance_sleep = var(sleep_total),\n              n = n())\n\n# A tibble: 5 × 4\n  vore    median_sleep variance_sleep     n\n  &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;\n1 carni           10.4          21.8     19\n2 herbi           10.3          23.8     32\n3 insecti         18.1          35.1      5\n4 omni             9.9           8.70    20\n5 &lt;NA&gt;            10.6           9.02     7\n\n\nThis syntax looks a lot like the syntax used for mutate()! Like in mutate(), we name and define new columns: new_column = formula. The difference is that summarize() returns a brand new data frame that does not contain the columns of the original data frame where mutate() returns a data frame with all columns of the original data frame in addition to the newly defined ones."
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/notes.html#footnotes",
    "href": "2-summarizing-data/04-conditioning/notes.html#footnotes",
    "title": "Data Pipelines",
    "section": "Footnotes",
    "text": "Footnotes\n\nV. M. Savage and G. B. West. A quantitative, theoretical framework for understanding mammalian sleep. Proceedings of the National Academy of Sciences, 104 (3):1051-1056, 2007.↩︎"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html",
    "title": "A Grammar of Graphics",
    "section": "",
    "text": "Announcements\nConcept Question\nConcept Activity\nQuiz Review\nBreak\nLab 2.2: Computing on the Data"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#agenda",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#agenda",
    "title": "A Grammar of Graphics",
    "section": "",
    "text": "Announcements\nConcept Question\nConcept Activity\nQuiz Review\nBreak\nLab 2.2: Computing on the Data"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#announcements",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#announcements",
    "title": "A Grammar of Graphics",
    "section": "Announcements",
    "text": "Announcements\n\nQuiz 1 is not this upcoming Monday, but next Monday(2/12) in-class. This Monday will be a workshop to complete Lab 2 and PS 4.\n\n. . .\n\nLab 2 (both parts) due Tuesday Feb 13 at 9am.\n\n. . .\n\nProblem Set 4 due Tuesday Feb 13 at 9am.\n\n. . .\n\nTwo problem sets without numbers relating to the Understanding the World with Data and A Grammar of Graphics will be posted to the resources tab on the home page of the course website. These are not for a grade and are for studying purposes.\n\n. . .\n\nQuiz 1 on Monday Feb 12, in class.\n\nIndividual portion [25 minutes]\nGroup (max. 3) portion [15 minutes]\nPlease consult the Quiz Megathread and Syllabus for content and logistical questions."
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-code-was-used-to-make-the-following-plot",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-code-was-used-to-make-the-following-plot",
    "title": "A Grammar of Graphics",
    "section": "What code was used to make the following plot?",
    "text": "What code was used to make the following plot?\n\nCodepenguins |&gt;\n  filter(species == \"Gentoo\") |&gt;\n  mutate(is_male = (sex == \"male\")) |&gt;\n  ggplot(mapping = \n           aes(x = body_mass_g,\n               y = is_male)) +\n  geom_boxplot() +\n  labs(x = \"Body Mass (in grams)\",\n      y =  \"Male Penguin?\",\n      title = \"Male Gentoo penguins have larger body masses\")\n\n\n\n\n\n\n\n. . .\n\nChoose an answer at pollev.com."
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#concept-activity-1",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#concept-activity-1",
    "title": "A Grammar of Graphics",
    "section": "Concept Activity",
    "text": "Concept Activity\nYou will be watching a 2.5 minute video of a presentation by a scientist, Hans Rosling, who studied global public health. He presents data visualizations depicting the change in life expectancy and family size over several decades in the 20th century.\n\nOn a piece of note paper:\n\nSketch out the data frame used to create the graphic and add the names of the variables.\nList the aesthetic attributes used to encode the data in the graphic.\nIdentify the geometry used in the plot."
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#section-1",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#section-1",
    "title": "A Grammar of Graphics",
    "section": "",
    "text": "Please turn to your neighbors and…\n\nDiscuss what you came up with in terms of . . .\n\nthe variables present in the data frame\nthe aesthetic attributes used to encode that data in the plot\nthe geometry\n\n\n\nCodecountdown::countdown(4, top = 0)\n\n\n\n−&plus;\n\n04:00"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-were-the-variables-and-aesthetic-attributes",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-were-the-variables-and-aesthetic-attributes",
    "title": "A Grammar of Graphics",
    "section": "What were the variables and aesthetic attributes?",
    "text": "What were the variables and aesthetic attributes?\n. . .\n\n\nVisual Cues / Aesthetics\n\nLocation along the x-axis\nLocation along the y-axis\nSize of point\nColor of point\nAnimation\n\n\nVariables\n\nFertility rate\nLife expectancy\nPopulation\nRegion\nYear"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-did-the-data-frame-look-like",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-did-the-data-frame-look-like",
    "title": "A Grammar of Graphics",
    "section": "What did the data frame look like?",
    "text": "What did the data frame look like?\nWhat was the unit of observation? What were the variables? What were their type?\n. . .\n\n\nUnit of observation\n\nA country in a given year\n\n\nVariables\n\nFertility rate (continuous)\nLife expectancy (continuous)\nPopulation (continuous)\nRegion (nominal)\nYear (discrete)"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-geometry-is-used-to-represent-the-observations",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-geometry-is-used-to-represent-the-observations",
    "title": "A Grammar of Graphics",
    "section": "What geometry is used to represent the observations?",
    "text": "What geometry is used to represent the observations?\n. . .\n\nPoints"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-type-of-claim-was-made",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-type-of-claim-was-made",
    "title": "A Grammar of Graphics",
    "section": "What type of claim was made?",
    "text": "What type of claim was made?\n\nCodeknitr::include_graphics(\"images/poll.png\")\n\n\n\n\n\n\n\n. . .\n\nCodecountdown::countdown(minutes = 1, top = 0)\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-type-of-variable-is-listeners",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-type-of-variable-is-listeners",
    "title": "A Grammar of Graphics",
    "section": "What type of variable is listeners?",
    "text": "What type of variable is listeners?\n\nCodeknitr::include_graphics(\"images/spotify.png\")\n\n\n\n\n\n\n\n. . .\n\nCodecountdown::countdown(minutes = 1, top = 0)\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-type-of-proportion-is-used",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-type-of-proportion-is-used",
    "title": "A Grammar of Graphics",
    "section": "What type of proportion is used?",
    "text": "What type of proportion is used?\n\nCodeknitr::include_graphics(\"images/q1.png\")\n\n\n\n\n\n\n\n. . .\n\nRoughly 68 percent of those passengers who were in the first class survived the wreckage of the Titanic.\n\n. . .\n\nCodecountdown::countdown(minutes = 1, top = 0)\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#which-measure-of-centerspread-is-least-appropriate",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#which-measure-of-centerspread-is-least-appropriate",
    "title": "A Grammar of Graphics",
    "section": "Which measure of center/spread is least appropriate?",
    "text": "Which measure of center/spread is least appropriate?\n\nCodelibrary(patchwork)\n\np1 &lt;- penguins |&gt;\n  filter(species == \"Gentoo\",\n         sex == \"male\") |&gt;\n  ggplot(mapping = aes(x = body_mass_g)) +\n  geom_histogram(color = \"white\",\n                 bins = 10) +\n  labs(title = \"Image 1\")\n\n\np2 &lt;- class_survey |&gt;\n  filter(new_COVID_variant &gt;0,\n         new_COVID_variant &lt; 1) |&gt;\n  ggplot(mapping = aes(x = new_COVID_variant)) +\n  geom_histogram(color = \"white\", bins = 10) +\n  labs(title = \"Image 2\")\n\np1 / p2\n\n\n\n\n\n\n\n\nCodecountdown::countdown(minutes = 1, top = 0)\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-are-the-aesthetics-and-geometry-of-this-plot",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-are-the-aesthetics-and-geometry-of-this-plot",
    "title": "A Grammar of Graphics",
    "section": "What are the aesthetics and geometry of this plot?",
    "text": "What are the aesthetics and geometry of this plot?\n\nCodelibrary(tidyverse)\nlibrary(palmerpenguins)\n\nggplot(penguins, aes(x = bill_length_mm,\n                     fill = species)) +\n    geom_density() +\n    theme_gray(base_size = 20)\n\n\n\n\n\n\n\n. . .\n\nCodecountdown::countdown(1, top = 1)\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-has-not-changed-when-moving-from-left-to-right",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/slides.html#what-has-not-changed-when-moving-from-left-to-right",
    "title": "A Grammar of Graphics",
    "section": "What has not changed when moving from left to right?",
    "text": "What has not changed when moving from left to right?\n\nCodelibrary(patchwork)\np1 &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                           y = bill_depth_mm,\n                           color = species)) +\n    geom_point(size=2) +\n    theme_gray(base_size = 20) +\n    theme(legend.position = \"none\") \np2 &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                           y = bill_depth_mm,\n                           shape = species)) +\n    geom_point(size=5, alpha = .5) +\n    theme_bw(base_size = 20) +\n    lims(y = c(10,24)) +\n    theme(legend.position = \"none\") +\n    xlab(\"Bill Length (in millimeters)\") +\n    annotate(geom = \"text\", x = 45, y = 11,\n             label = \"Gentoo\", size = 5) +\n    annotate(geom = \"segment\", x = 45, y = 11.2,\n             xend = 45, yend = 13, \n             size = 1)\np1 + p2"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/ps.html",
    "href": "2-summarizing-data/01-summarizing-categorical-data/ps.html",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "Here is a contingency table of college students with their Favorite Color (Red or Blue) down the columns and their School (Berkeley or Stanford) across the rows.\n\n\n\nRed\nBlue\n\n\n\nBerkeley\n10\n90\n\n\nStanford\n60\n40\n\n\n\n\nFind the proportion of all students who attend Berkeley. What type of proportion is this?\n \n \n\nFind the proportion of all students who attend Berkeley and like red best. What type of proportion is this?\n \n \n\nOf the students who attend Berkeley, find the proportion that like red best. What type of proportion is this?"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/ps.html#questions-1-3",
    "href": "2-summarizing-data/01-summarizing-categorical-data/ps.html#questions-1-3",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "Here is a contingency table of college students with their Favorite Color (Red or Blue) down the columns and their School (Berkeley or Stanford) across the rows.\n\n\n\nRed\nBlue\n\n\n\nBerkeley\n10\n90\n\n\nStanford\n60\n40\n\n\n\n\nFind the proportion of all students who attend Berkeley. What type of proportion is this?\n \n \n\nFind the proportion of all students who attend Berkeley and like red best. What type of proportion is this?\n \n \n\nOf the students who attend Berkeley, find the proportion that like red best. What type of proportion is this?"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/ps.html#questions-4-5",
    "href": "2-summarizing-data/01-summarizing-categorical-data/ps.html#questions-4-5",
    "title": "Summarizing Categorical Data",
    "section": "Questions 4-5",
    "text": "Questions 4-5\nRMS Titanic was a British passenger liner with 2,224 people aboard (including both passengers and crew). The Titanic sank in the North Atlantic Ocean on 15 April 1912 after striking an iceberg on her way to New York City. It was the deadliest sinking of a single ship at the time with almost 70% of the 2,224 passengers and crew dying.\nBelow are the first five rows of 2,224 for a data frame called titanic.\n\nQuestion 4\nWhat is the unit of observation for titanic?\n \nQuestion 5\nThe three following graphs were generated using titanic.\nWhich graph is best used to answer the following questions? Write the letter associated with the graph to the right of each question\nHow many first class passengers survived the Titanic’s sinking? _______\nIs there an association between a passenger’s class and their survival? _______\nWhat is the mode of the Class variable? _______"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/ps.html#questions-6-7",
    "href": "2-summarizing-data/01-summarizing-categorical-data/ps.html#questions-6-7",
    "title": "Summarizing Categorical Data",
    "section": "Questions 6-7",
    "text": "Questions 6-7\nOpen up RStudio and write down the code you used to complete each of the following questions in the space below. Make sure you load in any libraries where necessary!\nQuestion 6\nConsider the vector q6 which was made as follows:\n\nCodeq6 &lt;- c(1,2,3,4,5,6,NA)\n\n\nLoad q6 as is into your session. Then, write R code to calculate the mean of q6 so that the result is 3.5 (this is the mean of the numbers 1 through 6). Hint: how can you learn more about the function mean()?\n \n \nQuestion 7\nThe promote dataset can be found in the stat20data package. Using this data, write ggplot2 code to make a stacked, normalized bar chart having identified gender on the x axis, with the bars being filled in by promotion decision. Write the code you used below. Then, make a claim about the association between the two variables (we will revisit this study in more detail later in the course)!"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set 2: Summarizing Categorical Data\n\nBreak\nIntroducing Lab 1\nWork time for Lab 1: Understanding Context"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#agenda",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#agenda",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set 2: Summarizing Categorical Data\n\nBreak\nIntroducing Lab 1\nWork time for Lab 1: Understanding Context"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#announcements",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#announcements",
    "title": "Summarizing Categorical Data",
    "section": "Announcements",
    "text": "Announcements\n\nRQ: Summarizing Numerical Data due Wednesday at 11:59pm on Gradescope.\n\n. . .\n\nQuiz 1 next Monday, January 29th in-class. Topics covered:\n\nUnderstanding the World with Data\nTaxonomy of Data\nSummarizing Categorical Data (today)\nSummarizing Numerical Data (next class)\n\n\n\n. . .\n\nLab 1 (Class Survey) due Tuesday, January 29th at 9am on Gradescope. Make sure you read the Lab Submission Guidelines posted to Ed!\n\n. . .\n\nProblem Set 2 due Tuesday, January 29th at 9am on Gradescope"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "The table below displays data from a survey on a class of students.\n\n\nWhat proportion of the class was in the marching band?\n\n\nCodecountdown::countdown(.5, top = 0)\n\n\n\n−&plus;\n\n00:30\n\n\n\n\nAn example of a marginal proportion."
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-1",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-1",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "What proportion of those in the marching band where juniors?\n\n\nCodecountdown::countdown(.5, top = 0)\n\n\n\n−&plus;\n\n00:30\n\n\n\n\nAn example of a conditional proportion."
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-2",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-2",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "What proportion were sophomores not in the marching band?\n\n\nCodecountdown::countdown(.5, top = 0)\n\n\n\n−&plus;\n\n00:30\n\n\n\n\nAn example of a joint proportion."
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-3",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-3",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "What were the dimensions of the raw data from which this table was constructed?\n\n\nCodecountdown::countdown(.5, top = 0)\n\n\n\n−&plus;\n\n00:30"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-4",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-4",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "How would you characterize the association between these two variables?\n\n\nCodecountdown::countdown(.5, top = 0)\n\n\n\n−&plus;\n\n00:30"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#concept-question-2",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#concept-question-2",
    "title": "Summarizing Categorical Data",
    "section": "Concept Question 2",
    "text": "Concept Question 2\nPolitical affiliation and college degree status of 500 survey participants.\n\nCodelibrary(tidyverse)\nlibrary(stat20data)\nlibrary(infer)\nggplot(gss, aes(x = partyid, fill = college)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Party\", fill = \"College\", y = \"proportion\") +\n  theme_gray(base_size = 18)\n\n\n\n\n\n\n\n\nWhich group is the largest?\n\n\nThe General Social Survey is a widely used sources of data on the attitudes, behaviors, and attributes of Americans. This plot shows the relationship between the political affiliation and college degree status of 500 participants.\nCannot tell which group is the largest since this plot as been normalized so that the proportions within each party sum to 1. The unnormalized plot on the following slide is one that allows us to answer this questions.\nThe unnormalized bar chart of counts preserves original counts and thus is good at comparing joint proportions. The normalized bar count shows condition proportions and thus is good for showing associations between variables.\n\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-5",
    "href": "2-summarizing-data/01-summarizing-categorical-data/slides.html#section-5",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "What does this plot show?\n\n\nImportant note: it looks like a leading “1” was cropped from the numbers along the y axis.\nThis is a confusing stacked bar chart! (unfortunately I wasn’t able to track down the source.)\nThe height of each total bar appears to be the energy supply from renewables worldwide in each of these years. The proportion on top, however, appears to be that number divided by the global total energy supply, which changes every year. The fact that those proportions are decreasing left to right as the bar heights are increasing suggests that renewables are increasing every year (the numerator) but the total energy supply (the denominator) is increasing at a faster rate.\n\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/ps.html",
    "href": "2-summarizing-data/06-summarizing-associations/ps.html",
    "title": "Summarizing Numerical Associations",
    "section": "",
    "text": "There is a data set built into R called mtcars that includes several measures on different types of cars. Learn more about the data set using ?mtcars. Where applicable, complete the following questions in RStudio and write the code you used below."
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/ps.html#question-1",
    "href": "2-summarizing-data/06-summarizing-associations/ps.html#question-1",
    "title": "Summarizing Numerical Associations",
    "section": "Question 1",
    "text": "Question 1\nWe seek to explain the fuel efficiency of cars using their weight per gallon. Summarize the association between the fuel efficiency (measured in miles per gallon) and the weight of the car using:\n\nQuestion 1a\na scatter plot,\n\n\n\n\n\n\nQuestion 1b\nthe correlation coefficient\n\n\n\n\n\n\n\nQuestion 1c\nand a linear model."
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/ps.html#question-2",
    "href": "2-summarizing-data/06-summarizing-associations/ps.html#question-2",
    "title": "Summarizing Numerical Associations",
    "section": "Question 2",
    "text": "Question 2\nRepeat Question 1 but use the horsepower of the car instead of the weight as the explanatory variable.\n\nQuestion 2a\na scatter plot,\n\n\n\n\n\n\nQuestion 2b\nthe correlation coefficient\n\n\n\n\n\n\nQuestion c\nand a linear model.\n\n\n\n\nQuestion 2d\nCompare the scatter plots of Question 1a and Question 2a. Why does one of them have a higher correlation coefficient than the other? Answer in a few sentences."
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/ps.html#question-3",
    "href": "2-summarizing-data/06-summarizing-associations/ps.html#question-3",
    "title": "Summarizing Numerical Associations",
    "section": "Question 3",
    "text": "Question 3\nWhat is the better way to compare the strength of the linear relationship between these two pairs of variables (mpg and wt; mpg and hp): the correlation coefficients or the slopes of the linear models? Why? Explain in one-two sentences."
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/ps.html#question-4",
    "href": "2-summarizing-data/06-summarizing-associations/ps.html#question-4",
    "title": "Summarizing Numerical Associations",
    "section": "Question 4",
    "text": "Question 4\nWhich car has the lowest fuel efficiency given its weight? State the car and provide supporting code for your answer."
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/ps.html#question-5",
    "href": "2-summarizing-data/06-summarizing-associations/ps.html#question-5",
    "title": "Summarizing Numerical Associations",
    "section": "Question 5",
    "text": "Question 5\nVisualize the relationship between number of forward gears and the number of cylinders. Address any overplotting that might occur, and title the plot with a claim about the strength of the association between the two variables.\nOR\nGo back to the class_survey dataset and find two numerical discrete variables. Visualize their relationship, addressing any overplotting which may occur, and title the plot with a claim about the strength of the association between the two variables."
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/notes.html",
    "href": "2-summarizing-data/06-summarizing-associations/notes.html",
    "title": "Summarizing Numerical Associations",
    "section": "",
    "text": "When we first discussed constructing summaries for numerical data, you may have noticed that we left out the case when we are working with two numerical variables. This is a very common scenario in statistics and data science– so much so that it deserves its own set of notes! In this lecture, we will discuss how we can make visualizations and calculate summary statistics involving two numerical variables. Then, we will introduce a third method of describing data: building a model."
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/notes.html#overplotting",
    "href": "2-summarizing-data/06-summarizing-associations/notes.html#overplotting",
    "title": "Summarizing Numerical Associations",
    "section": "Overplotting",
    "text": "Overplotting\nFirst, we should spotlight an issue that can arise when visualizing numerical associations. This issue may have the potential to hide an association between them if it is not treated.\nLet’s examine the class survey dataset from earlier in this course. Stat 20 students filled out a survey that asked them their opinion on several topics including:\n\n\n\n\n\n\n\n\nThe result was a data frame with 619 rows (one for every respondent) and 2 columns of discrete numerical data. A natural way to visualize this data is by creating a scatter plot.\n\n\n\n\n\n\n\n\nThe eye is immediately drawn to the eerie geometric regularity of this data. Isn’t real data messier than this? What’s going on?\nA hint is in the sample size. The number of observations in the data set was over 600 and yet the number of points shown here is just a bit under 100. Where did those other observations go?\nIt turns out they are in this plot, they’re just piled on top of one another! Since there are only 10 possible values for each question, many students ended up selecting the same values for both, leading their points to be drawn on top of one another.\nThis phenomenon is called overplotting and it is very common in large data sets. There are several strategies for dealing with it, but here we cover two of them.\nOne approach to fixing the problem of points piled on top of one another is to unpile them by adding just a little bit of random noise to their x- and y-coordinate. This technique is called jittering and can be done in ggplot2 by replacing geom_point() with geom_jitter().\n\nggplot(class_survey, aes(x = tech,\n                         y = crypto)) +\n    geom_jitter() +\n    labs(x = \"Technology is destructive to relationships\",\n         y = \"cryptocurrency will play a\\n dominant role in finance\",\n         title = \"No association between opinions on technology and \\n cryptocurrency\")\n\n\n\n\n\n\n\nAhh . . . there are those previously hidden students. Interestingly, the title on the first plot still holds true: even when we’re looking at all of the students, there doesn’t appear to be much of a pattern. That is certainly not the case in all overplotted data sets! Often overplotting will obscure a pattern that jumps out after the overplotting has been attended to.\nThe second technique is to make the points transparent by changing an aesthetic attribute (setting) called the alpha value. Let’s combine transparency with jittering to understand the effect.\n\nggplot(class_survey, aes(x = tech,\n                         y = crypto)) +\n    geom_jitter(alpha = .3) +\n    labs(x = \"Technology is destructive to relationships\",\n         y = \"cryptocurrency will play a\\n dominant role in finance\",\n         title = \"No association between opinions on technology and \\n cryptocurrency\")\n\n\n\n\n\n\n\nThe alpha argument runs between 0 and 1, where 1 is fully opaque and 0 is fully see-through. Here, alpha = .3, which changes all observations from black to gray. Where the points overlap, their alpha values add to create a dark blob.\nThere’s still no sign of a strong association between these variables, but at least, by taking overplotting into consideration, we’ve made that determination after incorporating all of the data."
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/notes.html#associations-and-the-correlation-coefficient",
    "href": "2-summarizing-data/06-summarizing-associations/notes.html#associations-and-the-correlation-coefficient",
    "title": "Summarizing Numerical Associations",
    "section": "Associations and the correlation coefficient",
    "text": "Associations and the correlation coefficient\nWhich of the following plots do you think depicts the relationship between the high school graduation rate and the poverty rate among the 50 US states?\n\n\n\n\n\n\n\n\nIf you guessed the plot on the left, you are correct 🎉.\nStates with higher poverty rates tend to have lower graduation rates. This is a prime example of two variables that are associated. In a previous set of notes we defined association between two categorical variables, but lets replace that with a more general definition that can apply here.\n\nAssociation\n\nThere is an association between two variables if the conditional distribution of one varies as you move across values of the other.\n\n\nYou can detect associations in scatter plots by scanning from left to right along the x-axis and determining whether or not the conditional distribution of the y-variable is changing or not. In the figure to the left below, when you look first to the states with low poverty rates (in the blue box), you find that the conditional distribution of the graduation rate (represented by the blue density curve along the right side of the scatter plot) is high: most of those states have graduation rates between 85% and 90%. When you scan to the right in that scatter plot, and condition on having a high poverty rate (the states in the red box), the conditional distribution shifts downwards. Those states have graduations rates in the low 80%s.\n\n\nThese density curves are conditional distributions because we’ve set a condition on the data we’re visualizing. When focusing on the data that’s in the blue box, for example, we’ve in effect set up a filter where Poverty &lt; 9.\n\n\n\n\n\n\n\n\nThe plot on the right, by contrast, exhibits no association between poverty rate and graduation rate. When we compare the low poverty states with the high poverty states, their conditional distributions of Graduation rate are essentially the same.\nSo we can use the simple scatter plot to determine whether or not two numerical variables are associated, but sometimes a graphic isn’t enough. In these notes we’ll move from graphical summaries to numerical summaries and construct two different approaches to capturing these associations in numbers: the correlation coefficient and the simple linear model.\nThe Correlation Coefficient\nLet’s set out to engineer our first numerical summary in the same manner that we have previously, by laying out the properties that we’d like our summary to have.\nPlease watch the following 12 minute video.\n\n\nCorrelation coefficient, \\(r\\)\n\nThe correlation coefficient, \\(r\\), between two variables \\(x\\) and \\(y\\) is \\[r = \\frac{1}{n-1}\\sum_{i=1}^n \\left( \\frac{x_i - \\bar{x}}{s_x} \\right) \\left( \\frac{y_i - \\bar{y}}{s_y} \\right)\\]\n\n\n\n\nSeveral different statistics have been proposed for measuring association. This is the most common and is more specifically called the Pearson correlation.\nExample: Poverty and Graduation rate\nThe data frame used to create the scatter plot above on the left looks like this.\n\n\n# A tibble: 51 × 2\n   Graduates Poverty\n       &lt;dbl&gt;   &lt;dbl&gt;\n 1      79.9    14.6\n 2      90.6     8.3\n 3      83.8    13.3\n 4      80.9    18  \n 5      81.1    12.8\n 6      88.7     9.4\n 7      87.5     7.8\n 8      88.7     8.1\n 9      86      16.8\n10      84.7    12.1\n# ℹ 41 more rows\n\n\nSince it is a data frame, we can use the summarize() function to calculate our summary statistic.\n\npoverty |&gt;\n  summarize(r = cor(Poverty, Graduates))\n\n# A tibble: 1 × 1\n       r\n   &lt;dbl&gt;\n1 -0.747\n\n\nThe value of -0.747 tells us that the linear association between these variables is negative and reasonably strong. This is our first example of a bivariate summary statistic: there are two variables that we put inside the cor() function to compute our statistic.\nLet’s repeat this calculation for the data frame that created the shapeless scatter plot with no association, poverty_shuffled.\n\npoverty_shuffled |&gt;\n  summarize(r = cor(Poverty, Graduates))\n\n# A tibble: 1 × 1\n        r\n    &lt;dbl&gt;\n1 -0.0546\n\n\nAs expected, that scatter plot yields a correlation coefficient very close to zero because the points are scattered across all four quadrants of the plot."
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/notes.html#the-simple-linear-model",
    "href": "2-summarizing-data/06-summarizing-associations/notes.html#the-simple-linear-model",
    "title": "Summarizing Numerical Associations",
    "section": "The Simple Linear Model",
    "text": "The Simple Linear Model\nAnother approach to summarizing the linear association is to just … draw a line.\n\n\n\n\nThis line serves both as a graphical summary and also as a numerical summary. After all, every line that you draw on a scatter plot is defined by two numbers: the slope and the y-intercept. This line is called a simple linear model.\n\nSimple Linear Model\n\nAn expression for a possible value of the \\(y\\) variable, \\(\\hat{y}\\), as a linear function of the \\(x\\) variable with slope \\(b_1\\) and y-intercept \\(b_0\\). \\[\\hat{y} = b_0 + b_1x\\]\n\n\nTherefore, a simple linear model captures the linear relationship of two variables in not one but two summary statistics, \\(b_0\\) and \\(b_1\\).\nFor the line above, we can do our best to eye-ball these. The line appears to rise -2 percentage points for every 2.5 that it runs, so I’d estimate the slope to be about \\(-2/2.5 = -0.8\\). If I were to draw the line all the way to the left until it crossed the y-axis at a poverty rate of 0, its y-intercept would be around 95. So I could express the line that is drawn above as:\n\\[\\hat{y} = 95 - 0.8 x\\]\nThe Least Squares Line\nIf that felt a little shifty to you - drawing a line by hand and then eyeballing its slope and intercept - we can be more precise by using a more precisely-defined type of linear model: the least squares line. This is a method that we’ll study in depth when we get to the unit on prediction, but for now, we’ll use it because it makes calculation very easy. You can find the slope and intercept of the least squares line using statistics that we’re already familiar with: \\(\\bar{x}, \\bar{y}, s_x, x_y\\), and \\(r\\).\n\nLeast Squares Slope\n\n\\[ b_1 = r \\frac{s_y}{s_x} \\]\n\nLeast Squares Intercept\n\n\\[ b_0 = \\bar{y} - b_1 \\bar{x}\\]\n\n\nSo how does this line look compared to the hand-drawn line? Let’s calculate the slope and intercept. In R, we can do this with a function called lm(). To see the slope and intercept for our model, we can print out our model object.\n\nm1 &lt;- lm(formula = Graduates ~ Poverty, data = poverty)\nm1\n\n\nCall:\nlm(formula = Graduates ~ Poverty, data = poverty)\n\nCoefficients:\n(Intercept)      Poverty  \n    96.2022      -0.8979  \n\n\nThe syntax for lm() uses what’s called “formula notation” in R. The first argument is a formula of the form y ~ x and can be read as, “Explain the y as a function of the x”. In the second argument, you specify which data frame contains the variables used in the formula. If we want to use to save this slope and intercept for later use, we can save it into an object, just like a data frame or a vector can be saved.\nWe can then add our model to our scatter plot. This can be done with the geom_smooth() layer in ggplot2 and the method = \"lm\" argument (you do not need to worry about the purpose of the se argument).\n\nggplot(poverty, aes(x = Poverty, \n                    y = Graduates)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Poverty Rate\",\n       y = \"Graduation Rate\") +\n  theme_bw()\n\n\n\n\n\n\n\nThat works remarkably well!\nInterpreting the slope and intercept\nSo if the correlation coefficient measures the strength of the linear relationship between two variables, what exactly are the slope and intercept of a linear model between involving these two variables measuring?\nThe slope captures the expected change in the \\(y\\) variable associated with the \\(x\\) variable changing by 1 unit. In this example, states that are separated by 1 percentage point in their poverty rate tend to be separated by about -.89 in their graduation rate. This is distinct from what the correlation tells us because while \\(r\\) will stay the same regardless of the units in which the data is measured, \\(b_1\\) is expressly designed to tell us how those units of measurement relate to one another.\nWhat about the intercept? It tells us the value that we’d expect the \\(y\\) to take when the \\(x\\) takes a value of zero. Sometimes that’s an informative statistic, sometime it is not. In this setting, do you really expect the graduation rate to be around 96% when their poverty rate is zero? What would it even look like for a state to have a poverty rate of zero? The abstraction of the linear model allows us to ponder such a world, but the reality of economics in the US is that we would never actually observe poverty rates of zero.\nSo what good is the intercept? Well, it’s useful in helping us calculate a residual.\nResiduals\nOne of the benefits of explaining the association between two variables with a line instead of just the correlation coefficient is that it allows us to calculate what we would expect an observation’s y-value to be based on its x value, so that we can see how far our expectation is from reality. That gap between expectation and reality is called a residual.\n\n\nResidual (\\(\\hat{e}_i\\))\n\nThe difference between the observed value of a data point, \\(y_i\\), and the value that we would expect according to a linear model, \\(\\hat{y}_i\\). \\[ \\hat{e}_i = y_i - \\hat{y}_i \\]\n\n\n\n\n\\(\\hat{y}_i\\) is said “y hat sub i” and is also called the “fitted value”.\nLet’s calculate the residual for California. Here is that row in the data set.\n\npoverty |&gt;\n  filter(State == \"California\") |&gt;\n  select(State, Graduates, Poverty)\n\n# A tibble: 1 × 3\n  State      Graduates Poverty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 California      81.1    12.8\n\n\nThis shows us that for California, \\(y = 81.1\\), so the next step is to find where the line passes through California’s x-value, \\(x = 12.8\\). There are several ways to do that calculation, including using R like a calculator and simply plugging that value into the equation for the line show above.\n\ny_hat &lt;- 96.2022 - 0.8979 * 12.8\ny_hat\n\n[1] 84.70908\n\n\nWith that in hand, we can calculate California’s residual.\n\n81.1 - y_hat\n\n[1] -3.60908\n\n\nThis residual tells us that California is actually a bit of an underachiever. Among states with a poverty rate around 12.8, we would expect their graduate rate to be around 84.7. California’s rate, however, is 81.1, a decrease of 3.6.\nThe calculation of the residual can be seen in the plot below.\n\n\n\n\n\n\n\n\nThe horizontal dashed line represents \\(\\hat{y} = 84.7\\), the y-value of the least squares line when it passes through \\(x = 12.8\\). The vertical red dashed line is the residual: the distance between the line and the observation in the y direction.\nResiduals open up a new avenue for numerical statistics. While the slope and intercept are two statistics that tell us about the overall linear relationship between the two variables, each residual is a statistic that tells us whether an individual observation’s y-value is higher or lower than we’d expect based on its x-value.\n\n\nIf you have \\(n\\) data points, you can calculate \\(n\\) residuals. This is described below.\nWhile using R as a calculator directly to obtain one residual is somewhat efficient, this changes when you would like to calculate a residual for each point in your data set. Imagine having to write \\(n\\) lines of code, one for each observation: \\(y_1\\), \\(y_2\\), and all the way to \\(y_n\\)! Luckily, when you save a linear model into an object, you store lots of useful information, including \\(\\hat{y}_i\\) and \\(\\hat{e}_i\\) for every observation \\(y_i\\). These can be accessed via the fitted() and residuals() functions, respectively.\nfitted() and residuals() return vectors. To match the vectors up with the observations, we can mutate them as columns onto the original data frame. From here, we can isolate the residual for the state of California as before.\n\npoverty |&gt;\n  mutate(y_hat = fitted(m1),\n         e_hat = residuals(m1)) |&gt;\n  select(State, Graduates, Poverty, y_hat, e_hat) |&gt;\n  filter(State == \"California\") \n\n# A tibble: 1 × 5\n  State      Graduates Poverty y_hat e_hat\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 California      81.1    12.8  84.7 -3.61"
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/notes.html#summary",
    "href": "2-summarizing-data/06-summarizing-associations/notes.html#summary",
    "title": "Summarizing Numerical Associations",
    "section": "Summary",
    "text": "Summary\nIn these notes we considered the question of how to capture the association between two variables with both visualizations and numerical summary statistics. The correlation coefficient is one of the most common statistics to use in this case: it captures the strength and direction of the linear trend. This statistic can be used, along with other simple summary statistics, to calculate the slope and intercept of the least squares line. The least squares line is an alternative approach to summarizing the linear relationship between two numerical variables. It has the advantage of providing an expectation for the y-value of every observation, which allows us to calculate residuals which are expressions of whether each observation is higher or lower than we’d expect.\nWe’ll spend time practicing calculating these statistics - and looking at lots of scatter plots - in class."
  },
  {
    "objectID": "2-summarizing-data/05-communicating-with-graphics/slides.html",
    "href": "2-summarizing-data/05-communicating-with-graphics/slides.html",
    "title": "Communicating with Graphics",
    "section": "",
    "text": "Concept Question\nPractice: Communicating with Graphics"
  },
  {
    "objectID": "2-summarizing-data/05-communicating-with-graphics/slides.html#agenda",
    "href": "2-summarizing-data/05-communicating-with-graphics/slides.html#agenda",
    "title": "Communicating with Graphics",
    "section": "",
    "text": "Concept Question\nPractice: Communicating with Graphics"
  },
  {
    "objectID": "2-summarizing-data/05-communicating-with-graphics/slides.html#question-1",
    "href": "2-summarizing-data/05-communicating-with-graphics/slides.html#question-1",
    "title": "Communicating with Graphics",
    "section": "Question 1",
    "text": "Question 1\n\nWhich elements have been applied to the following plot?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/learning-objectives.html",
    "href": "2-summarizing-data/02-summarizing-numerical-data/learning-objectives.html",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "Summarizing Numerical Data\n\nConcept Acquisition\n\n\n\n\n\nTool Acquisition\n\n\n\n\n\nConcept Application\n\nDescribe a distribution in terms of shape, center, and spread"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/reading-questions.html",
    "href": "2-summarizing-data/02-summarizing-numerical-data/reading-questions.html",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "Question 1\nWhich of the following plot types maintain all of the information found in the original data set?\n\ndot plot ( ) histogram ( ) violin plot ( ) box plot\n\n\n\nQuestion 2\nIf you wish to see less detail in your histogram and perform more aggregation, which of the following is the best course of action?\n( ) switch to a dot plot ( ) switch to a bar chart ( ) instead of presenting the histogram, display the original data frame with the raw data (X) increase the bin width of the histogram ( ) decrease the bin width of the histogram\n\n\nQuestion 3\nWhich word best describes a distribution with a long tail stretching out to the left?\n\nleft skewed ( ) right skewed ( ) unimodal ( ) heteroskedastic\n\n\n\nQuestion 4\nIn which situation does the median do a better job than the mean at capturing the center of a distribution? Select the best answer.\n( ) when the median is less than the mean ( ) when the median is greater than the mean (X) when the variable under study is known to have a strongly skewed distribution ( ) when the mode doesn’t exist"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/notes.html",
    "href": "2-summarizing-data/02-summarizing-numerical-data/notes.html",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "Man feigns madness, contemplates life and death, and seeks revenge.\nSon avenges his father, and it only takes four hours.\nA tragedy written by the English playwright around 1600.\n29,551 words on a page.\nYou may recognize each of these as summaries of the play, “Hamlet”. None of these are wrong, per se, but they do focus on very different aspects of the work. Summarizing something as rich and complex as Hamlet invariably involves a large degree of omission; we’re reducing a document of 29,551 words down to a single sentence or phrase, after all. But summarization also involves important choices around what to include.\nThe same considerations of omission and inclusion come into play when developing a numerical or graphical summary of a data set. Some guidance to bear in mind:\nWhat should I include?\nWhat should I omit?\nIn these notes we’ll keep this guidance in mind as we discuss how to summarize numerical data with graphics, in words, and with statistics. Specifically, we will learn how to:\nSummarizing two numerical variables is a task which deserves its own set of notes; that set of notes will come later on!"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/notes.html#constructing-graphical-summaries",
    "href": "2-summarizing-data/02-summarizing-numerical-data/notes.html#constructing-graphical-summaries",
    "title": "Summarizing Numerical Data",
    "section": "Constructing Graphical Summaries",
    "text": "Constructing Graphical Summaries\nLet’s turn to an example admittedly less complex than Hamlet: the Palmer penguins. One of the numerical variables Dr. Gorman recorded was the length of the bill in millimeters. The values of the first 16 penguins are:\n\n\n\n\nbill_16 &lt;- penguins %&gt;%\n  select(bill_length_mm) %&gt;%\n  slice(1:16)\nbill_16  \n\n# A tibble: 16 × 1\n   bill_length_mm\n            &lt;dbl&gt;\n 1           39.1\n 2           39.5\n 3           40.3\n 4           36.7\n 5           39.3\n 6           38.9\n 7           39.2\n 8           41.1\n 9           38.6\n10           34.6\n11           36.6\n12           38.7\n13           42.5\n14           34.4\n15           46  \n16           37.8\n\n\nWe have many options for different plot types that we could use to summarize this data graphically. To understand the differences, it’s helpful to lay out the criterion that we hold for a summary to be a success. Let’s call those criteria the desiderata, a word meaning “that which is desired or needed”.\nFor our first graphic, let’s set a high bar.\n\n\n\n\n\n\nDesiderata\n\n\n\n\nAll information must be preserved.\n\n\n\nThe most commonly used graphic that fulfills this criterion is the dot plot.\n\nggplot(data = penguins, \n       mapping = aes(x= bill_length_mm)) +\n  geom_dotplot()\n\n\n\n\n\n\n\n\n\nThe dot plot is, in effect, a one-dimensional scatter plot. Each observation shows up as a dot and its value corresponds to its location along the x-axis. Importantly, it fulfills our desiderata: given this graphic, one can recreate the original data perfectly. There was no information loss.\nAs the number of observations grow, however, this sort of graphical summary becomes unwieldy. Instead of focusing on the value of each observation, it becomes more practical to focus on the general shape of the distribution. Let’s consider a broader goal for our graphic.\n\n\n\n\n\n\nDesiderata\n\n\n\n\nBalance depiction of the general characteristics of the distribution with a fidelity to the individual observations.\n\n\n\nThere are several types of graphics that meet this criterion: the histogram, the density plot, and the violin plot.\nHistograms\n\nggplot(data = penguins, \n       mapping = aes(x= bill_length_mm)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\nAt first glance, a histogram looks like deceptively like a bar chart. There are bars arranged along the x-axis according to their values with heights that correspond to the count of each value found in the data set. So how is this not a bar chart?\nA histogram involves aggregation. The first step in creating a histogram is to divide the range of the variable into bins of equal size. The second step is to count up the number of observations that occur in each bin. In this way, some observations will have their own bar (every bar with a count of one) but other observations will be aggregated into the same bar: the tallest bar, with a count of 3, corresponds to all observations from 39.09 to 39.30: 39.1, 39.2, and 39.3.\nThe degree of aggregation performed by the histogram is determined by the binwidth. Most software will automatically select the binwidth1, but it can be useful to tinker with different values to see the distribution at different levels of aggregation. The binwidth argument within geom_histogram() can be altered to implement this. Here are four histograms of the same data that use four different binwidths (try setting your histogram’s binwidth to one of the options in the below plots)!\n\n\n\n\n\n\n\n\nIf you are interested in only the coarsest structure in the distribution, best to use the larger binwidths. If you want to see more detailed structure, a smaller binwidth is better.\nThere is a saying that warns about times when you, “can’t see the forest for the trees”, being overwhelmed by small details (the trees) and unable to see the bigger picture (the forest). The histogram, as a graphical tool for summarizing the distribution of a numerical variable, offers a way out. Through your choice of binwidth, you can determine how much to focus on the forest (large bindwidth) or the trees (small binwidth).\nDensity plots\nImagine that you build a histogram and place a cooked piece of spaghetti over the top of it. The curve created by the pasta is a form of a density plot.\n\nggplot(data = penguins, \n       mapping = aes(x= bill_length_mm)) +\n  geom_density()\n\n\n\n\n\n\n\n\n\nBesides the shift from bars to a smooth line, the density plot also changes the y-axis to feature a quantity called “density”. We will return to define this term later in the course, but it’s sufficient to know that the values on the y-axis of a density plot are rarely useful. The important information is relative: an area of the curve with twice the density as another area has roughly twice the number of observations.\nThe density plot, like the histogram, offers the ability to balance fidelity to the individual observations against a more general shape of the distribution. You can tip the balance to feature what you find most interesting but adjusting the bandwidth of the density plot. This can be done by including the bw argument inside of density and setting it to a number (again, try setting it to one of the options in the below plots)!\n\n\n\n\n\n\n\n\nA density curve tends to convey the overall shape of a distribution more quickly than does a histogram, but be sure to experiment with different bandwidths. Strange but important features of a distribution can be hidden behind a density curve that is too smooth.\nViolin plots\nOften we’re interested not in the distribution of a single variable, but in the way the distribution of that variable changes from one group of observational units to another. Let’s add this item to our list of criteria for a statistical graphic.\n\n\n\n\n\n\nDesiderata\n\n\n\n\nBalance depiction of the general characteristics of the distribution with a fidelity to the individual observations.\nAllow for easy comparisons between groups.\n\n\n\nThere are several different ways to compare the distribution of a variable across two or more groups, but one of the most useful is the violin plot. Here is a violin plot of the distribution of bill length across the three species of penguins.\n\nggplot(data = penguins, \n       mapping = aes(x= bill_length_mm,\n                     y = species)) +\n  geom_violin()\n\n\n\n\n\n\n\n\n\nThe distribution of bill length in each species is represented by a shape that often looks like a violin but is in fact a simple density curve reflected about its x-axis. This means that you can tinker with a violin plot the same as a density plot, but changing the bandwidth.\nIf this plot type looks familiar, you may have seen its cousin, the box plot.\n\nggplot(data = penguins, \n       mapping = aes(x= bill_length_mm,\n                     y = species)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nThe box plot conveys a similar story to the violin plot: Adelies have shorter bills than Chinstraps and Gentoos. Box plots have the advantage of requiring very little computation to construct2, but in a world of powerful computers, that is no longer remarkable. What they lack is a “smootness-knob” that you can turn to perform more or less smoothing. For this reason, violin plots are a more flexible alternative to box plots."
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/notes.html#describing-distributions",
    "href": "2-summarizing-data/02-summarizing-numerical-data/notes.html#describing-distributions",
    "title": "Summarizing Numerical Data",
    "section": "Describing Distributions",
    "text": "Describing Distributions\nThe desideratum that we used to construct the histogram and the violin plot include the ability to “depict general characteristics of the distribution”. The most important characteristics of a distribution are its shape, center, and spread.\nWhen describing the shape of a distribution in words, pay attention to its modality and skew. The modality of a distribution captures the number of distinct peaks (or modes) that are present.\n\n\n\n\nA good example of a distribution that would be described as unimodal is the original density plot of bill lengths of 16 Adelie penguins (below left). There is one distinct peak around 39. Although there is another peak around 34, it is not prominent enough to be considered a distinct mode. The distribution of the bill lengths of all 344 penguins (below right), however, can be described as bimodal.\n\n\n\n\n\n\n\n\nMultiple modes are often a hint that there is something more going on. In the plot to the right above, Chinstraps and Gentoo penguins, which are larger, are clumped under the right mode while the smaller Adelie penguins are dominant in the left mode.\nThe other important characteristic of the shape of a distribution is its skew.\n\n\n\n\nThe skew of a distribution describes the behavior of its tails: whether the right tail stretches out (right skew), the left tail stretches out (left skew), or if both tails are of similar length (symmetric). An example of a persistently right skewed distribution is household income in the United States:\n\n\n\n\nIn the US, the richest households have much much higher incomes than most, while the poorest households have incomes that are only a bit lower than most.\nWhen translating a graphical summary of a distribution into words, some degree of judgement must be used. When is a second peak a mode and when is it just a bump in the distribution? When is one of the tails of a distribution long enough to tip the description from being symmetric to being right skewed? You’ll hone your judgement in part through repeated practice: looking at lots of distributions and readings lots of descriptions. You can also let the questions of inclusion and omission be your guide. Is the feature a characteristic relevant to the question you’re answering and the phenomenon you’re studying? Or is it just a distraction from the bigger picture?\nModality and skew capture the shape of the distribution, but how do we describe its center and spread? “Eyeballing it” by looking at a graphic is an option. A more precise option, though, is to calculate a statistic."
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/notes.html#constructing-numerical-summaries",
    "href": "2-summarizing-data/02-summarizing-numerical-data/notes.html#constructing-numerical-summaries",
    "title": "Summarizing Numerical Data",
    "section": "Constructing Numerical Summaries",
    "text": "Constructing Numerical Summaries\nStatistics is both an area of academic study and the object of that study. Any numerical summary of a data set - a mean or median, a count or proportion - is a statistic. A statistic is, fundamentally, a mathematical function where the data is the input and the output is the observed statistic.\n\n\n\nStatisticians don’t just study statistics, though, they construct them. A statistician gets to decide the form of \\(f\\) and, as with graphics, they construct it to fulfill particular needs: the desiderata.\nTo examine the properties of common statistics, let’s move to an even simpler data set: a vector called x that holds 11 integers.\n\\[8, 11, 7, 7, 8, 11,  9,  6,  10,  7,  9\\]\n\n\nMeasures of Center\nThe mean, the median, and the mode are the three standard statistics used to measure the center of a distribution. Despite their ubiquity, these three are not carved somewhere on a stone tablet. They’re common because they’re useful and they’re useful because they were constructed very thoughtfully.\nLet’s start by laying out some possible criteria for a measure of center.\n\n\n\n\n\n\nDesiderata\n\n\n\n\nSynthesizes the magnitudes of all \\(n\\) observations.\nAs close as possible to all of the observations.\n\n\n\nThe (arithmetic) mean fulfills all of these needs.\n\n\nThe function in R: mean()\n\\[\n\\frac{8 + 11 + 7 + 7 + 8 + 11 + 9 + 6 + 10 + 7 + 9}{11} = \\frac{93}{11} = 8.45\n\\]\nThe mean synthesizes the magnitudes by taking their sum, then keeps that sum from getting larger than any of the observations by dividing by \\(n\\). In order to express this function more generally, we use the following notation\n\\[\n\\bar{x} = \\frac{x_1 + x_2 + \\ldots + x_n}{n}\n\\]\nwhere \\(x_1\\) is the first observation, \\(x_2\\) is the second observation, and so on till the \\(n^{th}\\) observation, \\(x_n\\); and \\(\\bar{x}\\) is said “x bar”.\nThe mean is the most commonly used measure of center, but has one notable drawback. What if one of our observations is an outlier, that is, has a value far more extreme than the rest of the data? Let’s change the \\(6\\) to \\(-200\\) and see what happens.\n\\[\n\\frac{8 + 11 + 7 + 7 + 8 + 11 + 9 - 200 + 10 + 7 + 9}{11} = \\frac{-113}{11} = -10.27\n\\]\nThe mean has plummeted to -10.27, dragged down by this very low outlier. While it is doing it’s best to stay “as close as possible to all of the observations”, it isn’t doing a very good job of representing 10 of the 11 values.\nWith this in mind, let’s alter the first criterion to inspire a different statistic.\n\n\n\n\n\n\nDesiderata\n\n\n\n\nSynthesize the order of all \\(n\\) observations.\nAs close as possible to all of the observations.\n\n\n\nIf we put the numbers in order from smallest to largest, then the number that is as close as possible to all observations will be the middle number, the median.\n\n\nThe function in R: median()\n\\[ 6 \\quad 7 \\quad 7 \\quad  7 \\quad 8 \\quad {\\Large 8} \\quad 9 \\quad 9 \\quad 10 \\quad 11  \\quad 11\\]\nAs measured by the median, the center of this distribution is 8 (recall the mean measured 8.45). If there were an even number of observations, the convention is to take the mean of the middle two values.\nThe median has the desirable property of being resistant (or “robust”) to the presence of outliers. See how it responds to the inclusion of -200.\n\\[ -200 \\quad 7 \\quad 7 \\quad  7 \\quad 8 \\quad {\\Large 8} \\quad 9 \\quad 9 \\quad 10 \\quad 11  \\quad 11\\]\nWith this outlier, the median remains at 8 while the mean had dropped to -10.27. This property makes the median the favored statistic for capturing the center of a skewed distribution.\nWhat if we took a stricter notion of “closeness”?\n\n\n\n\n\n\nDesiderata\n\n\n\n\nIs identical to as many observations as possible.\n\n\n\nThat leads us to the measure of the mode, or the most common observation. For our example data set, the mode is \\(7\\).\n\\[ 6 \\quad {\\Large 7 \\quad 7 \\quad  7} \\quad 8 \\quad 8 \\quad 9 \\quad 9 \\quad 10 \\quad 11  \\quad 11\\]\nWhile using the mode for this data set is sensible, it is common in numerical data for each value to be unique3. In that case, there are no repeated values, and no identifiable mode. For that reason, it is unusual to calculate the mode to describe the center of a numerical variable.\nFor categorical data, however, the mode is very useful. The mode of the species variable among the Palmer penguins is “Adelie”. Trying to compute the mean or the median species will leave you empty-handed. This is one of the lingering lessons of the Taxonomy of Data: the type of variable guides how it is analyzed.\nMeasures of Spread\nThere are many different ways to capture spread or dispersion of data. Here are some basic desiderata we might hope to achieve with a measure of spread.\n\n\n\n\n\n\nDesiderata\n\n\n\n\nThe statistic should be low when the numbers are the same or very similar to one another.\nThe statistic should be high when the numbers are very different.\nThe statistic should not grow or shrink with the sample size ( n ).\n\n\n\nThese desiderata are not met by every measure of spread. Here is one such measure of spread which does not meet all three!\nRange\nThe range of a set of numbers is simply the maximum number in the dataset minus its minimum.\n\\[ range = max - min\\]\nFor our set of numbers, the range will be \\(5\\).\n\\[ {\\Large 6} \\quad {\\ 7 \\quad 7 \\quad  7} \\quad 8 \\quad 8 \\quad 9 \\quad 9 \\quad 10 \\quad 11  \\quad {\\Large 11}\\]\nAlthough the first two criterion above are met by the range, the third one is not. The reason is that if we add a number to our set which is greater than the maximum value, or smaller than the minimum value, the value of range will change. Therefore, increasing our sample size \\(n\\) could cause our statistic to grow or shrink.\nHere are some measures of spread which do meet all three criterion. They do this by using incorporating some of the measures of center that we have already talked about, such as the mean and the median.\nThe Sample Variance\nThe sample variance:\n\nTakes the differences from each observation, \\(x_i\\), to the sample mean \\(\\bar{x}\\);\nsquares them;\nadds them all up;\ndivides by \\(n - 1\\);\nthen finally, takes the square root.\n\n\n\nThe function in R: var()\n\\[ s^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( x_i - \\bar{x}\\right)^2 \\] This formula is rather dense; don’t worry! We won’t ask you to memorize it. The key is that is fits the three criterion we were hoping for.\nThe distance \\((x_i - \\bar{x})^2\\) will be small when \\(x_i\\) is close to the mean, and large when it’s not. This means the first two criterion are met. Additionally, because we are dividing by a number close to \\(n\\), (\\(n-1\\)), the statistic does not grow or shrink with \\(n\\). This means the last criterion is met!\nOne other question you might have: why the square?\nThe reason is that spread/distance is a positive quantity. Recall that the mean of our list was \\(8.45\\). Two numbers in our list, \\(7\\) and \\(9\\), are both \\(1.45\\) units away from the mean. However, \\(7-8.45 = -1.45\\) and \\(9-8.45 = 1.45\\). As part of the sum, we will have to add up \\(-1.45\\) and \\(1.45\\), which comes out to \\(0\\).\nThis means that the information from the two data points \\(7\\) and \\(9\\) have been canceled out! We don’t want this to happen, so we need to make all of the terms in the sum positive. The square takes care of that.\nFor our list:\n\\[ s^2 =  2.87 \\]\nThe Sample Standard Deviation\n\\[ s = \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( x_i - \\bar{x}\\right)^2} \\]\n\n\nThe function in R: sd()\nThe sample standard deviation \\(s\\) is the same as the sample variance \\(s^2\\): we’ve just taken the square root. One reason for using the sample standard deviation is that it at times is more interpret-able than the sample variance, since it’s measured in units rather than units squared.\nFor our list:\n\\[ s =  1.69 \\] We can say, therefore, that each data point is about 1.7 units apart from each other.\n\nWhile the sample variance and sample standard deviation are great for measuring symmetric data (which appear enough in statistics) and also show up a lot in the theory of some topics that we will later discover, they do have their faults.\nNamely, when data is not symmetric, the square around \\(x_i - \\bar{x}\\) can cause some issues. Asymmetrical data will have many values \\(x_i\\) (large or small) which are far from the mean \\(\\bar{x}\\).\nIf \\(x_i - \\bar{x}\\) is large, then \\((x_i - \\bar{x})^2\\) will be very large. Therefore, \\(s^2\\) and \\(s\\) can produce values that are overestimates of the spread of most of the data. Here are two measures of spread which counter-act this.\nIQR (Interquartile Range)\nThe IQR is the difference between the median of the larger half of the sorted data set, \\(Q3\\), and the median of the smaller half, \\(Q1\\).\n\n\nThe function in R: IQR()\n \n\\[ IQR = Q_3 - Q_1 \\]  \nLet’s calculate the IQR for the list of eleven numbers we’ve been working with. First, we find the median of our dataset. That’s \\(8\\). Then, we split the data into two halves of five. Then we find the medians of these halves. and take the difference. These steps are visualized below.\n \n\\[ 6 \\quad 7 \\quad {\\Large 7 \\quad  7} \\quad 8 \\quad  {\\large 8} \\quad 9 \\quad {\\Large 9 \\quad 10} \\quad 11  \\quad 11\\]  \nWe have that \\(Q_3 = 9.5\\) and \\(Q_1 = 7\\). Then:\n \n\\[IQR =  9.5−7 = 2.5\\]\nThe reason the IQR works well for asymmetric data is because the measure of center it’s based on is the median, not the mean. The median, being just the middle point of the data and not a value obtained by calculation of all the numbers in a list, is not impacted when extreme values are tacked onto the end of the list.\n\n\n\n\n\n\nAdditional Desiderata\n\n\n\n\nIs robust to extreme values (outliers).\n\n\n\nOur final measure of spread is another option which is resillent against outliers, but is based off of the mean instead of the median.\nMean Absolute Deviation (MAD)\nThe \\(MAD\\) is very similar to the sample variance \\(s^2\\), except that:\n\nwe divide by \\(n\\) rather than \\(n-1\\);\nwe take the absolute value of \\(x_i - \\bar{x}\\) instead of squaring it.\n\n\\[\nMAD: \\quad \\frac{1}{n}\\sum_{i = 1}^n |x_i - \\bar{x}|\n\\]\nThe key difference is the second one. The MAD is great for summarizing skewed distributions because it isn’t bothered too much by the presence of extreme values in a set of numbers. That’s because the absolute value bar just ensures a number is positive; it doesn’t further amplify that number by squaring it.\n\n\n\n\n\n\nAdditional Desiderata\n\n\n\n\nIs robust to extreme values (outliers)."
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/notes.html#the-ideas-in-code",
    "href": "2-summarizing-data/02-summarizing-numerical-data/notes.html#the-ideas-in-code",
    "title": "Summarizing Numerical Data",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nOnce you have your data in front of you, you’ve seen how we can form visual summaries with ggplot2. But how can we calculate numerical summaries? Furthermore, what if we are concerned about summarizing a portion of our data, like just one species of penguin at a time? We will answer these questions below, and introduce some new functions from the dplyr package (within the tidyverse library) along the way. We’ll also look at how factor() can come in handy while plotting.\nCalculating Numerical Summaries\nOne example of a numerical variable we could have examine is the body mass of a particular penguin (measured in grams). Let’s calculate both a measure of center and spread for this variable.\nTo get an idea of what summaries we should pick, let’s revisit the density plot from earlier.\n\n\n\n\n\n\n\n\nWhat we can glean from this figure is that the distribution of body masses across all species of penguin is skewed right. This means that, for instance, a more typical observation lies closer to 4000 grams than 5000 grams.\nIf we take an average, it is likely to be pulled to the right by the larger, but less typical, observations. The median observation, however, would be more resistant to this pull. Therefore, the median might be a nice choice for a measure of center. Similarly, since the IQR is initially constructed from the median, it will serve well here as a measure of spread.\nNow, let’s calculate these values. We should first isolate our variable of interest. We can do this in code by using the dplyr function select().\n\nbody_mass &lt;- select(penguins, body_mass_g)\nbody_mass\n\n# A tibble: 333 × 1\n   body_mass_g\n         &lt;int&gt;\n 1        3750\n 2        3800\n 3        3250\n 4        3450\n 5        3650\n 6        3625\n 7        4675\n 8        3200\n 9        3800\n10        4400\n# ℹ 323 more rows\n\n\nAs is custom with dplyr functions, the first argument goes to the data frame you are working with. The following arguments are more function specific. In select()’s case, we tell the computer which column/variable we are interested in.\nNow, we can calculate our summaries. When working with a vector, we could use functions like mean() and median() directly, e.g. median(body_mass_g). However, body_mass_g is not a standalone vector but is now a column in a data frame called body_mass! Therefore, we need to access it through a dplyr function called summarise().\n\nsummarise(body_mass, \n          body_mass_median = median(body_mass_g),\n          body_mass_IQR = IQR(body_mass_g))\n\n# A tibble: 1 × 2\n  body_mass_median body_mass_IQR\n             &lt;int&gt;         &lt;dbl&gt;\n1             4050          1225\n\n\nNote that while the first argument goes to the name of the data frame, the following arguments are given to the names of the new columns that summarise() puts in another new data frame (one row by two columns). You can name the columns whatever you would like.\nBased on what we’ve found, the median here supports the claim we made above: that a typical penguin has a body mass closer to 4000 grams than to 5000 grams. The middle 50 percent of the penguins have body masses within 1225/2 grams, or roughly 600 grams, of 4050.\nGroupwise Operations\nLet’s return to the bill length examine of a particular penguin, measured in millimeters. Here is the density plot for all of the data; for simplicity, earlier we showed you the plot for only the first 16 observations.\n\n\n\n\n\n\n\n\nThis plot is interesting. It appears we have a bimodal shape! While it’s tempting to state that the data is roughly symmetric and calculate an overall mean, we should first see if there are any other variables at play. It stands to reason that different species of penguin might have different anatomical features. Let’s add species to the mix by using the color aesthetic (see if you can code along)!\n\n\n\n\n\n\n\n\nAha! We now see that each penguin species has its own shape of distribution when it comes to bill length.\nThe example above demonstrates a very common scenario: you want to perform some calculations on one particular group of observations in your data set. But what if you want to do that same calculation for every group? For example, what if we’d like to find the average and standard deviation of bill length among each species of penguin separately?\nThis task - performing an operation on all groups of a data set one-by-one - is such a common data science task that nearly every software tool has a good solution. In the dplyr package, the solution is the group_by() function. Let’s see it in action.\n\ngrouped_penguins &lt;- group_by(penguins, species)\n\nLike most tidyverse functions, the first argument to group_by() is a data frame. The second argument is the name of the variable that you want to use to delineate groups. In this case, we want to group by species to calculate three separate mean/standard deviation pairs.\nNow, assuming we roll with our new grouped_penguins data frame, we can use summarise() like we did before!\n\nsummarise(grouped_penguins, \n          bill_length_mean = mean(bill_length_mm),\n          bill_length_sd = sd(bill_length_mm))\n\n# A tibble: 3 × 3\n  species   bill_length_mean bill_length_sd\n  &lt;fct&gt;                &lt;dbl&gt;          &lt;dbl&gt;\n1 Adelie                38.8           2.66\n2 Chinstrap             48.8           3.34\n3 Gentoo                47.6           3.11\n\n\nFrom both the visuals and the numbers, we can see that Adelie penguins have much smaller bill lengths on average when compared to Chinstrap and Gentoo penguins. We also see that the Adelie distribution of bill lengths is less variable than the distributions of the other two species.\nPlotting with Categorical Variables\nFinally, let’s return to the violin plot of bill lengths grouped by species of penguin.\n\n\n\n\n\n\n\n\nWhat if I wanted the Adelie violin to show up on the top of the graph? By default, the violin plot puts the level first in the alphabetical order on the bottom of the plot. Therefore, I need to reorder the levels of species to put Adelie at the top. This is where factor() will do the job!\nAs before, bill_length_mm is not a standalone vector but a column in a data frame! We cannot access it directly, e.g. by factor(species, levels = c(\"Gentoo\", \"Chinstrap\", \"Adelie\")).\nTherefore, we use the dplyr function mutate(). A mutation involves changing the properties of an existing column, or adding a new one altogether (which we will explore next week).\n\nreordered_penguins &lt;- \n  mutate(penguins, \n  species = \n    factor(species, levels = c(\"Gentoo\", \"Chinstrap\", \"Adelie\")))\n\nThe first argument of mutate() is dedicated to our data frame, penguins. The second argument can be the name of an existing column or the name of a new column (next week). We want to change species to be an altered version of itself, hence we name the second argument species. Make sure you understand where each set of parentheses closes and ends.\nNow, assuming we roll with our new reordered_penguins data frame, we can use ggplot() like we did before!"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/notes.html#summary",
    "href": "2-summarizing-data/02-summarizing-numerical-data/notes.html#summary",
    "title": "Summarizing Numerical Data",
    "section": "Summary",
    "text": "Summary\nA summary of a summaries…this better be brief! Summaries of numerical data - graphical and numerical - often involve choices of what information to include and what information to omit. These choices involve a degree of judgement and knowledge of the criteria that were used to construct the commonly used statistics and graphics."
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/notes.html#footnotes",
    "href": "2-summarizing-data/02-summarizing-numerical-data/notes.html#footnotes",
    "title": "Summarizing Numerical Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe ggplot2 package in R defaults to 30 bins across the range of the data. That’s a very rough rule-of-thumb, so tinkering is always a good idea.↩︎\nTo read more about one common way to construct a box plot, see the ggplot2 documentation.↩︎\nThat is, unless you aggregate! The aggregation performed by a histogram or a density plot is what allows us to describe numerical data as unimodel or bimodal.↩︎"
  },
  {
    "objectID": "2-summarizing-data/labs/02-class-survey/slides.html",
    "href": "2-summarizing-data/labs/02-class-survey/slides.html",
    "title": "Lab: Class Survey",
    "section": "",
    "text": "Lab 1.1: Understanding the Context of the Data\n\n\n\n\n−&plus;\n\n25:00\n\n\n\nLab 1.2: Computing on the Data\n\n\n\n\n−&plus;\n\n25:00"
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/lab-context.html",
    "href": "2-summarizing-data/labs/01-arbuthnot/lab-context.html",
    "title": "Lab 1.1: Arbuthnot",
    "section": "",
    "text": "What question did John Arbuthnot set out to answer in collecting this data?\n\n\n\nWhat do you think the probability is that a newborn child is recorded as a girl? What form of evidence or reasoning did you use to come to that determination?\n\n\n\nSketch below a simplified version of the christening records (see the lab slides) as a data frame. What is the unit of observation?\n\n\n\nWhen Arbuthnot published his study of the christening records, he presented his data like this (we show here just the first 10 rows).\n\n\n# A tibble: 82 × 3\n    year  boys girls\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  1629  5218  4683\n 2  1630  4858  4457\n 3  1631  4422  4102\n 4  1632  4994  4590\n 5  1633  5158  4839\n 6  1634  5035  4820\n 7  1635  5106  4928\n 8  1636  4917  4605\n 9  1637  4703  4457\n10  1638  5359  4952\n# ℹ 72 more rows\n\n\n\nWhat does every row in this data frame published by John Arbuthnot correspond to? For each variable, state its type and the sort of values you would expect it to take (and also values it should not take).\n\n\n\nWhat operations were necessary to get from the original christenings data frame to Arbuthnot’s data frame?"
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/lab.html",
    "href": "2-summarizing-data/labs/01-arbuthnot/lab.html",
    "title": "Lab 1: Arbuthnot",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/lab.html#part-1-understanding-the-context-of-the-data",
    "href": "2-summarizing-data/labs/01-arbuthnot/lab.html#part-1-understanding-the-context-of-the-data",
    "title": "Lab 1: Arbuthnot",
    "section": "Part 1: Understanding the Context of the Data",
    "text": "Part 1: Understanding the Context of the Data\nThe first part of each of our labs will be a worksheet where you’ll think more about the context of the data, help formulate your questions, and set expectations for what the data will look like. Please record your answers on the worksheet below and upload it to Gradescope as Lab 1.1.\n\nPart 1: Understanding the Context of the Data"
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/lab.html#part-ii-computing-on-the-data",
    "href": "2-summarizing-data/labs/01-arbuthnot/lab.html#part-ii-computing-on-the-data",
    "title": "Lab 1: Arbuthnot",
    "section": "Part II: Computing on the Data",
    "text": "Part II: Computing on the Data\nQuarto Lab Format\nThe second part of each lab is your chance to dive into a real data set to answer the questions that you posed in Part 1. To ensure your analysis is reproducible, please record your answers to these questions (both the text and code) in a .qmd file. Render your work to an HTML file regularly as a check that your code is running. When you are pleased with the result, print that HTML to a pdf (File &gt; Print &gt; Save as pdf in your browser), then upload to Gradescope as Lab 1.2. See Ed for more info about lab submission.\nLab Questions\nThe first several questions pertain to the arbuthnot data frame found in library(stat20data).\n\nWhat is the time frame covered by Arbuthnot’s data?\nWhich year saw the greatest number of children christened?\nWhat is the proportion of girls christened in 1700?\nWhat is the trend over time in the total number of children christened? Please answer with a plot and written interpretation.\nWhat is the trend over time in the proportion of girls christened? Please answer with a plot and written interpretation.\n\n\nThe remaining questions pertain to the present data frame found in library(stat20data).\n\nWhat is the time frame covered by the present-day data?\nIn terms of general magnitude (size), how do the counts in Arbuthnot’s data compare to the counts in the present-day data?\nWhat is the trend over time in the proportion of births that are girls? Please answer with a plot and written interpretation.\nBased on these two data sets, what claim are you prepared to make regarding John Arbuthnot’s original question? What reservations, if any, do you have about using this data to make the claim?"
  },
  {
    "objectID": "2-summarizing-data/labs/03-flights/lab-context.html",
    "href": "2-summarizing-data/labs/03-flights/lab-context.html",
    "title": "Stat 20",
    "section": "",
    "text": "What is the unit of observation in the data frame in the slides?\n\n\n\n\nCome up with four questions that can be posed using the variables in this dataset:\n\n\nOne that can be answered via a summary\nOne that can be answered via a generalization\nOne that can be answered via a prediction\nOne that can be answered via a causal claim.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName a variable which has an ambiguous (could be numerical or categorical) type to you. Then, sketch two visualizations of this plot: one where the variable is treated numerically, and one where the variable is being treated categorically. In each plot:\n\n\ndepict a shape which reflects your expectation of the phenomenon;\nlabel your axes and provide a title.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is your guess for the units/format used to record the departure time? Said another way, what would a value of 1517 represent?\n\n\n\n\n\n\nWhat filter would you use to extract the flights that left in the springtime destined for Portland, Oregon? Draw a sketch of this smaller data frame showing at least the columns used in the filter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat mutation would you use to create a column that records TRUE for flights leaving during or after March 2020 and FALSE otherwise?\n\n\n\n\n\n\n\nWhat mutation would you use to create a column that records the average speed of a plane during its flight, in miles per hour? Hint: think about the units of the variables needed to complete this mutation, and perform any unit conversion that is necessary.\n\n\n\n\n\n\n\nProvide one dplyr pipeline that calculates the mean departure delay for Oakland and San Francisco Airports. Provide a sketch of the resulting data structure with correct dimensions."
  },
  {
    "objectID": "2-summarizing-data/labs/03-flights/slides.html#section-2",
    "href": "2-summarizing-data/labs/03-flights/slides.html#section-2",
    "title": "Lab: Flights",
    "section": "",
    "text": "Codecountdown::countdown(25, top = 0)\n\n\n\n−&plus;\n\n25:00"
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/notes.html",
    "href": "2-summarizing-data/07-multiple-linear-regression/notes.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "In the last lecture we built our first linear model: an equation of a line drawn through the scatter plot.\n\\[ \\hat{y} = 96.2 + -0.89 x \\]\nWhile the idea is simple enough, there is a sea of terminology that floats around this method. A linear model is any model that explains the \\(y\\), often called the response variable or dependent variable, as a linear function of the \\(x\\), often called the explanatory variable or independent variable. There are many different methods that can be used to decide which line to draw through a scatter plot. The most commonly-used approach is called the method of least squares, a method we’ll look at closely when we turn to prediction. If we think more generally, a linear model fit by least squares is one example of a regression model, which refers to any model (linear or non-linear) used to explain a numerical response variable.\nThe reason for all of this jargon isn’t purely to infuriate students of statistics. Linear models are one of the most widely used statistical tools; you can find them in use in diverse fields like biology, business, and political science. Each field tends to adapt the tool and the language around them to their specific needs.\nA reality of practicing statistics in these field, however, is that most data sets are more complex than the example that we saw in the last notes, where there were only two variables. Most phenomena have many different variables that relate to one another in complex ways. We need a more more powerful tool to help guide us into these higher dimensions. A good starting point is to expand simple linear regression to include more than one explanatory variable!"
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/notes.html#multiple-linear-regression",
    "href": "2-summarizing-data/07-multiple-linear-regression/notes.html#multiple-linear-regression",
    "title": "Multiple Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nMultiple Linear Regression\n\nA method of explaining a continuous numerical \\(y\\) variable in terms of a linear function of \\(p\\) explanatory terms, \\(x_i\\). \\[ \\hat{y} = b_0 + b_1x_1 + b_2x_2 + \\ldots +b_px_p \\] Each of the \\(b_i\\) are called coefficients.\n\n\nTo fit a multiple linear regression model using least squares in R, you can use the lm() function, with each additional explanatory variable separated by a +.\n\nlm(formula = y ~ x_1 + x_2 + x_3 + ... + x_p, data = df)\n\nMultiple linear regression is powerful because it has no limit to the number of variables that we can include in the model. While Hans Rosling was able to fit 5 variables into a single graphic, what if we had 10 variables? Multiple linear regression allows us to understand high dimensional linear relationships beyond whats possible using our visual system.\nIn today’s notes, we’ll discuss two specific examples where a multiple linear regression model might be applicable\n\nA scenario involving two numerical variables and one categorical variable\nA scenario involving three numerical variables."
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/notes.html#two-numerical-one-categorical",
    "href": "2-summarizing-data/07-multiple-linear-regression/notes.html#two-numerical-one-categorical",
    "title": "Multiple Linear Regression",
    "section": "Two numerical, one categorical",
    "text": "Two numerical, one categorical\nThe Zagat Guide was for many years the authoritative source of restaurant reviews. Their approach was very different from Yelp!. Zagat’s review of a restaurant was compiled by a professional restaurant reviewer who would visit a restaurant and rate it on a 30 point scale across three categories: food, decor, and service. They would also note the average price of a meal and write up a narrative review.\nHere’s an example of a review from an Italian restaurant called Marea in New York City.\n\n\n\n\nIn addition to learning about the food scores (27), and getting some helpful tips (“bring your bank manager”), we see they’ve also recorded a few more variables on this restaurant: the phone number and website, their opening hours, and the neighborhood (Midtown).\nYou might ask:\n\nWhat is the relationship between the food quality and the price of a meal at Italian restaurant? Are these two variables positively correlated or is the best Italian meal in New York a simple and inexpensive slice of pizza?\n\nTo answer these questions, we need more data. The data frame below contains Zagat reviews from 168 Italian restaurants in Manhattan.\n\nCodelibrary(tidyverse)\nzagat &lt;- read_csv(\"https://www.dropbox.com/s/c797oanmvdzjegt/zagat.csv?dl=1\")\nzagat\n\n# A tibble: 168 × 6\n   restaurant          price  food decor service geo  \n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1 Daniella Ristorante    43    22    18      20 west \n 2 Tello's Ristorante     32    20    19      19 west \n 3 Biricchino             34    21    13      18 west \n 4 Bottino                41    20    20      17 west \n 5 Da Umberto             54    24    19      21 west \n 6 Le Madri               52    22    22      21 west \n 7 Le Zie                 34    22    16      21 west \n 8 Pasticcio              34    20    18      21 east \n 9 Belluno                39    22    19      22 east \n10 Cinque Terre           44    21    17      19 east \n# ℹ 158 more rows\n\n\nApplying the taxonomy of data, we see that for each restaurant we have recorded the price of an average meal, the food, decor, and service scores (all numerical variables) as well as a note regarding geography (a categorical nominal variable). geo captures whether the restaurant is located on the east side or the west side of Manhattan1.\nLet’s summarize the relationship between food quality, price, and one categorical variable - geography - using a colored scatter plot.\n\nCodezagat %&gt;%\n  ggplot(aes(x = food,\n             y = price,\n             color = geo)) +\n  geom_jitter() +\n  theme_bw() +\n  labs(x = \"Food Quality\",\n       y = \"Price of Average Meal\",\n       color = \"Geography\")\n\n\n\n\n\n\n\nIt looks like if you want a very tasty meal, you’ll have to pay for it. There is a moderately strong, positive, and linear relationship between food quality and price. This plot, however, has a third variable in it: geography. The restaurants from the east and west sides are fairly well mixed, but to my eye the points on the west side might be a tad bit lower on price than the points from the east side. I could numerically summarize the relationship between these three variables by hand-drawing two lines, one for each neighborhood.\n\n\n\n\nFor a more systematic approach for drawing lines through the center of scatter plots, we need to return to the method of least squares, which is done in R using lm(). In this linear model, we wish to explain the \\(y\\) variable as a function of two explanatory variables, food and geo, both found in the zagat data frame. We can express that relationship using the formula notation.\n\nm1 &lt;- lm(price ~ food + geo, zagat)\nm1\n\n\nCall:\nlm(formula = price ~ food + geo, data = zagat)\n\nCoefficients:\n(Intercept)         food      geowest  \n    -15.970        2.875       -1.459  \n\n\nIt worked . . . or did it? If extend our reasoning from the last notes, we should write this model as\n\\[\\widehat{price} = -15.97 + 2.87 \\times food - 1.45 \\times geo\\]\nWhat does it mean to put a categorical variable, geo, into a linear model? And how do three three numbers translate into the two lines shown above?\nIndiciator variables\nWhen working with linear models like the one above, the value of the explanatory variable, \\(geowest\\), is multiplied by a slope, 1.45. According to the Taxonomy of Data, arithmetic functions like multiplication are only defined for numerical variables. While that would seem to rule out categorical variables for use as explanatory variables, statisticians have come up with a clever work-around: the indicator variable.\n\nIndicator Variable\n\nA variable that is 1 if an observation takes a particular level of a categorical variable and 0 otherwise. A categorical variable with \\(k\\) levels can be encoded using \\(k-1\\) indicator variables.\n\n\nThe categorical variable geo can be converted into an indicator variable by shifting the question from “Which side of Manhattan are you on?” to “Are you on the west side of Manhattan?” This is a mutate step.\n\nzagat |&gt;\n  mutate(geowest = geo == \"west\") |&gt;\n  select(food, price, geo, geowest)\n\n# A tibble: 168 × 4\n    food price geo   geowest\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;  \n 1    22    43 west  TRUE   \n 2    20    32 west  TRUE   \n 3    21    34 west  TRUE   \n 4    20    41 west  TRUE   \n 5    24    54 west  TRUE   \n 6    22    52 west  TRUE   \n 7    22    34 west  TRUE   \n 8    20    34 east  FALSE  \n 9    22    39 east  FALSE  \n10    21    44 east  FALSE  \n# ℹ 158 more rows\n\n\nThe new indicator variable geowest is a logical variable, so it has a dual representation as TRUE/FALSE as well as 1/0. Previously, this allowed us to do Boolean algebra. Here, it allows us to include an indicator variable in a linear model.\nWhile you can create indicator variables by hand using mutate, in practice, you will not need to do this. That’s because they are created automatically whenever you put a categorical variable into lm(). Let’s revisit the linear model that we fit above with geowest in the place of geo.\n\\[\\widehat{price} = -15.97 + 2.87 \\times food - 1.45 \\times geowest\\]\nTo understand the geometry of this model, let’s focus on what the fitted values will be for any restaurant that is on the west side. For those restaurants, the geowest indicator variable will take a value of 1, so if we plug that in and rearrange,\n\\[\\begin{eqnarray}\n\\widehat{price} &= -15.97 + 2.87 \\times food - 1.45 \\times 1 \\\\\n&= (-15.97 - 1.45) + 2.87 \\times food \\\\\n&= -17.42 + 2.87 \\times food\n\\end{eqnarray}\\]\nThat is a familiar sight: that is an equation for a line.\nLet’s repeat this process for the restaurants on the east side, where the geowest indicator variable will now take a value of 0.\n\\[\\begin{eqnarray}\n\\widehat{price} &= -15.97 + 2.87 \\times food - 1.45 \\times 0 \\\\\n&= -15.97 + 2.87 \\times food\n\\end{eqnarray}\\]\nThat is also the equation for line.\nIf you look back and forth between these two equations, you’ll notice that they share the same slope and have different y-intercepts. Geometrically, this means that the output of lm() was describing the equation of two parallel lines:\n\none where geowest is 1 (for restaurants on the west side of town)\none where geowest is 0 (for restaurants on the east side of town).\n\nThat means we can use the output of lm() to replace my hand-drawn lines with ones that arise from the method of least squares.\n\n\n\n\n\n\n\n\nReference levels\nOne question you still might have: Why did R include the indicator variable for the west side of town as opposed to the one for the east side?. The answer lies in the type of variable that geo is recorded as in the zagat dataframe. If you look closely at the initial output, you will see that geo is currently designated chr, which is short for character. geo is indeed a categorical variable with two levels: east and west.\nLike in previous settings, R will determine the “order” of levels in a categorical variable registered as a character by way of the alphabet. This means that east will be tagged first and chosen as the reference level: the level of a categorical variable which does not have an indicator variable in the model. If you would like west to be the reference level, then you would need to reorder the levels using factor() inside of a mutate() so that west comes first. This would change the equation that results from then fitting a linear model with lm(), as you can see below!\n\nzagat |&gt;\n  mutate(geo = factor(geo, levels = c(\"west\", \"east\"))) |&gt;\n  lm(formula = price ~ food + geo)\n\n\nCall:\nlm(formula = price ~ food + geo, data = mutate(zagat, geo = factor(geo, \n    levels = c(\"west\", \"east\"))))\n\nCoefficients:\n(Intercept)         food      geoeast  \n    -17.430        2.875        1.459  \n\n\nNow our equation looks a little bit different!\n\\[\\widehat{price} = -17.43 + 2.87 \\times food + 1.46 \\times geoeast\\]\nIn general, if you include a categorical variable with \\(k\\) levels in a regression model, there will be \\(k-1\\) indicator variables (and thus, coefficients) associated with it in the model: one for each level of the variable except the reference level2. Knowing the reference level also helps us interpret indicator variables that are part of the regression equation; we will see this in a moment. For now, let’s move to our second scenario."
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/notes.html#three-numerical",
    "href": "2-summarizing-data/07-multiple-linear-regression/notes.html#three-numerical",
    "title": "Multiple Linear Regression",
    "section": "Three numerical",
    "text": "Three numerical\nWhile the standard scatter plot allows us to understand the association between two numerical variables like price and food, to understand the relationship between three numerical variables, we will need to build this scatterplot in 3D3.\n\n\n\n\n\n\n\n\n\n\n\nTake a moment to explore this scatter plot4. Can you find the name of the restaurant with very bad decor but pretty good food and a price to match? (It’s Gennaro.) What about the restaurant that equally bad decor but has rock bottom prices that’s surprising given it’s food quality isn’t actually somewhat respectable? (It’s Lamarca.)\nInstead of depicting the relationship between these three variables graphically, let’s do it numerically by fitting a linear model.\n\nm2 &lt;- lm(price ~ food + decor, data = zagat)\nm2\n\n\nCall:\nlm(formula = price ~ food + decor, data = zagat)\n\nCoefficients:\n(Intercept)         food        decor  \n    -24.500        1.646        1.882  \n\n\nWe can write the corresponding equation of the model as\n\\[ \\widehat{price} = -24.5 + 1.64 \\times food + 1.88 \\times decor \\]\nTo understand the geometry of this model, we can’t use the trick that we did with indicator variables. decor is a numerical variable just like food, so it takes more values than just 0 and 1.\nIndeed this linear model is describing a plane.\n\n\n\n\n\n\n\n\n\n\n\nIf you inspect this plane carefully you’re realize that the tilt of the plane is not quite the same in every dimension. The tilt in the decor dimension is just a little bit steeper than that in the food dimension, a geometric expression of the fact that the coefficient in front of decor, 1.88, is just a bit higher than the coefficient in front of food, 1.64."
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/notes.html#interpreting-coefficients",
    "href": "2-summarizing-data/07-multiple-linear-regression/notes.html#interpreting-coefficients",
    "title": "Multiple Linear Regression",
    "section": "Interpreting coefficients",
    "text": "Interpreting coefficients\nWhen moving from simple linear regression, with one explanatory variable, to the multiple linear regression, with many, the interpretation of the coefficients becomes trickier but also more insightful.\nThree numerical\nMathematically, the coefficient in front of \\(food\\), 1.64, can be interpreted a few different ways:\n\nIt is the difference that we would expect to see in the response variable, \\(price\\), when two Italian restaurants are separated by a food rating of one and they have the same decor rating.\nControlling for \\(decor\\), a one point increase in the food rating is associated with a $1.64 increase in the \\(price\\).\n\nSimilarly for interpreting \\(decor\\): controlling for the quality of the food, a one-point increase in \\(decor\\) is associated with a $1.88 increase in the \\(price\\).\nTwo numerical, one categorical\nThis conditional interpretation of the coefficients extends to the first setting we looked at, when one variable is numerical and the other is an indicator. Here is that model:\n\\[\\widehat{price} = -15.97 + 2.87 \\times food - 1.45 \\times geowest\\]\nOne might interpret \\(food\\) like this:\n\nFor two restaurants both on the same side of Manhattan, a one point increase in food score is associated with a $2.87 increase in the price of a meal.\n\nAs for \\(geowest\\):\n\nFor two restaurants with the exact same quality of food, the restaurant on the west side is expected to be $1.45 cheaper than the restaurant on the east side.\n\nWe make the comparison to the the east side since this level is the reference level according to the linear model shown. This is a useful bit of insight - it gives a sense of what the premium is of being on the eastside.\nIt is also visible in the geometry of the model. When we’re looking at restaurants with the same food quality, we’re looking at a vertical slice of the scatter plot. Here the vertical gray line is indicating restaurants where the food quality gets a score of 18. The difference in expected price of meals on the east side and west side is the vertical distance between the red line and the blue line, which is exactly 1.45. We could draw this vertical line anywhere on the graph and the distance between the red line and the blue will still be exactly 1.45."
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/notes.html#summary",
    "href": "2-summarizing-data/07-multiple-linear-regression/notes.html#summary",
    "title": "Multiple Linear Regression",
    "section": "Summary",
    "text": "Summary\nWe began this unit on Summarizing Data with graphical and numerical summaries of just a single variable: histograms and bar charts, means and standard deviations. In the last set of notes we introduced our first bivariate numerical summaries: the correlation coefficient, and the linear model. In these notes, we introduced multiple linear regression, a method that can numerically describe the linear relationships between an unlimited number of variables. The types of variables that can be included in these models is similarly vast. Numerical variables can be included directly, generalizing the geometry of a line into a plane in a higher dimension. Categorical variables can be included using the trick of creating indicator variables: logical variables that take a value of 1 where a particular condition is true. The interpretation of all of the coefficients that result from a multiple regression is challenging but rewarding: it allows us to answer questions about the relationship between two variables after controlling for the values of other variables.\nIf this felt like a deep dive into a multiple linear regression, don’t worry. Linear models are one of the most commonly used statistical tools, so we’ll be revisiting them throughout the course: investigating their use in making generalizations, causal claims, and predictions."
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/notes.html#footnotes",
    "href": "2-summarizing-data/07-multiple-linear-regression/notes.html#footnotes",
    "title": "Multiple Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\nFifth Avenue is the wide north-south street that divides Manhattan into an east side and a west side.↩︎\nThis is the case for a model including an intercept term; these models will be our focus this semester and are the most rcommonly used.↩︎\nWhile ggplot2 is the best package for static statistical graphics, it does not have any interactive functionality. This plot was made using a system called plotly, which can be used both in R and Python. Read more about how it works at https://plotly.com/r/.↩︎\nThis is a screenshot from an interactive 3D scatter plot. We’ll see the interactive plot in class tomorrow.↩︎"
  },
  {
    "objectID": "assets/quiz-slides.html",
    "href": "assets/quiz-slides.html",
    "title": "Quiz",
    "section": "",
    "text": "🧹 Clear your desk of everything except a pen/pencil and your cheat sheet.\n🧢 Remove hats, hoods, and sunglasses.\n📱 All electronic devices (laptop, phone, airbuds, smart watch) should be remain in your bag for the entire class period.\nSpread out as much as possible: one 1 student to a triangle desk, 2 to a tall desk.\nWhen you get a quiz, cover it with your cheat sheet and do not begin.\n\n\n\n\n👀 Keep your eyes on your own quiz.\nIf you need to use the bathroom, bring phone and quiz to the front of the class.\n⌛If you finish early, please hold on to your quiz and wait.\n❗You must stop writing when time is called or risk a 0.\n\nGood luck! 🍀\n\n\n\n\nCodecountdown::countdown(25, top = 0, font_size = \"1.5em\")\n\n\n\n−&plus;\n\n25:00"
  },
  {
    "objectID": "assets/quiz-slides.html#individual-quiz",
    "href": "assets/quiz-slides.html#individual-quiz",
    "title": "Quiz",
    "section": "",
    "text": "🧹 Clear your desk of everything except a pen/pencil and your cheat sheet.\n🧢 Remove hats, hoods, and sunglasses.\n📱 All electronic devices (laptop, phone, airbuds, smart watch) should be remain in your bag for the entire class period.\nSpread out as much as possible: one 1 student to a triangle desk, 2 to a tall desk.\nWhen you get a quiz, cover it with your cheat sheet and do not begin.\n\n\n\n\n👀 Keep your eyes on your own quiz.\nIf you need to use the bathroom, bring phone and quiz to the front of the class.\n⌛If you finish early, please hold on to your quiz and wait.\n❗You must stop writing when time is called or risk a 0.\n\nGood luck! 🍀\n\n\n\n\nCodecountdown::countdown(25, top = 0, font_size = \"1.5em\")\n\n\n\n−&plus;\n\n25:00"
  },
  {
    "objectID": "assets/quiz-slides.html#group-quiz",
    "href": "assets/quiz-slides.html#group-quiz",
    "title": "Quiz",
    "section": "Group Quiz",
    "text": "Group Quiz\n\nCodecountdown::countdown(15, top = 0, font_size = \"1.5em\")\n\n\n\n−&plus;\n\n15:00"
  },
  {
    "objectID": "3-probability/01-chance-intro/ps.html",
    "href": "3-probability/01-chance-intro/ps.html",
    "title": "Introducing Probability",
    "section": "",
    "text": "Consider the seven sided die (with sides 0, 1, 2, 3, 4, 5 and 6) from your reading questions. You roll this die once. Let:\n\nA be the event that a given roll yields an even number.\n\nB be the event that a given roll is greater than or equal to three.\n\nC be the event that the number appears in the phrase “Stat 20”.\n\n\nFind \\(\\text{P}(A)\\), \\(\\text{P}(B)\\) and \\(\\text{P}(C)\\)\n\n\n\n\n\nFind \\(\\text{P}(A \\cup B)\\).\n\n\n\n\nFind \\(\\text{P}(B \\cap C)\\)\n\n\n\n\n\nFind \\(\\text{P}(A \\cap B^C)\\).\n\n\n\nOne ticket will be drawn at random from each of the two boxes below:\n\nWhat is the probability the number drawn from \\(A\\) is greater than the one drawn from \\(B\\)?\n\n\n\n\nWhat is the probability that the number drawn from \\(A\\) is equal to the one drawn from \\(B\\)?\n\n\n\n\nWhat is the probability the number drawn from \\(A\\) is smaller than the one drawn from \\(B\\)?\n\n\nConsider an outcome space \\(\\Omega\\) with events \\(A, B\\) that are not mutually exclusive. Let \\(P(A) = 0.5, P(B) = 0.7\\) and \\(P(A \\cap B) = .4\\).\n\nDraw a Venn Diagram containing two circles, one representing \\(A\\) and the other \\(B\\).\n\n\n\n\n\nShade in the space in the diagram above corresponding to event \\(A\\).\nShade in the space in the diagram above corresponding to event \\(B\\) in a way that is different than the way you shaded in the space for event \\(A\\).\nBased on your shading, calculate the probability \\(P(A \\cup B)\\). Explain why you cannot add \\(P(A)\\) and \\(P(B)\\) directly to obtain this answer.\n\n\n\nReturn to the example of the seven-sided die from your reading questions and earlier in this problem set. Write code in R (and write the code below) to:\n\nSimulate rolling the seven-sided die once.\n\n\n\nSimulate rolling the seven-sided die seven times.\n\n\n\nRender a Quarto document with parts 1 and 2 such that you receive the same sample each time you run the code. You can just provide the line of code which is responsible for this behavior.\n\n\nExamine the following code.\n\nCode# First create a vector with the possible sums of a pair of dice\ndice &lt;- seq(from = 2, by = 1, to = 12)\n# Then sample once from these sums \nsample(x = dice, size = 1)\n\n\n\nIf we want to simulate rolling a pair of dice and summing the spots (for say, playing Monopoly), will the following code do this correctly? Explain.\n\n\n\nIf you said that the code is incorrect, modify the following code to simulate this process correctly. Hint: you may want to use sample() more than once."
  },
  {
    "objectID": "3-probability/01-chance-intro/notes.html",
    "href": "3-probability/01-chance-intro/notes.html",
    "title": "Introducing Probability",
    "section": "",
    "text": "In an enormously entertaining paper written about a decade ago, the economist Peter Backus estimated his chance of finding a girlfriend on any given night in London at about 1 in 285,000 or 0.0000034%. As he writes, this is either depressing or cheering news for a person, depending on what you had estimated your chance to be before reading the paper and doing a similar computation for yourself.1 The interesting point in the paper was using a probabilistic argument (originally developed by the astronomer and astrophysicist Frank Drake to estimate the probability of extra-terrestrial civilizations) to think about his dating problems. Anyone can follow the arguments put forward by Backus, including his statements that use probability.\nWe all have some notion of chance or probability, and can ask questions like: - What is the chance you will get an A in Stat 20? (About 32%, based on last fall.)2 - What is the chance the 49ers will win the Super Bowl this year? (They are the favorites, with an implied probability of about 54.5%, .)3 - What is the chance you will roll a double on your next turn to get out of jail while playing Monopoly? (One in six.) - What is the chance that Donald Trump will win the Presidential election? (About 47%.) 4\nThe second of our four types of claim we will investigate is a generalization. To do so, we first need to quantify uncertainty and randomness. This is the purpose of the Probability unit.\nSo far, we have examined data sets and summarized them, both numerically and visually. We have looked at data distributions, and associations between variables. Can we extend the conclusions that we make about the data sets to larger populations? If we notice that bill length and flipper length have a strong linear relationship for the penguins in our data, can we say this is true about all penguins? How do we draw valid conclusions about the population our data was drawn from? These are the kinds of questions we we will study using tools from probability theory.\nIn order to be taken seriously, we need to be careful about how we collect data, and then how we generalize our findings. For example, you may have observed that some polling companies are more successful than others in their estimates and predictions, and consequently people pay more attention to them. Below is a snapshot of rankings of polling organizations from the well-known website FiveThirtyEight5, and one can imagine that not many take heed of the polling done by the firms with C or worse grades. According to the website, the rankings are based on the polling organization’s ``historical accuracy and methodology”.\nIn order to make estimates as these polling organizations are doing, or understand the results of a clinical trial, or other such questions in which we generalize from our data sample to a larger group, we have to understand the variations in data introduced by randomness in our sampling methods. Each time we poll a different group of voters, for example, we will get a different estimate of the proportion of voters that will vote for Joe Biden in the next election. To understand variation, we first have to understand how probability was used to collect the data.\nSince classical probability came out of gambling games played with dice and coins, we can begin our study by thinking about those."
  },
  {
    "objectID": "3-probability/01-chance-intro/notes.html#de-mérés-paradox",
    "href": "3-probability/01-chance-intro/notes.html#de-mérés-paradox",
    "title": "Introducing Probability",
    "section": "De Méré’s Paradox",
    "text": "De Méré’s Paradox\n\n\n\n\nIn 17th century France, gamblers would bet on anything. In particular, they would bet on a fair six-sided die landing 6 at least once in four rolls. Antoine Gombaud, aka the Chevalier de Méré, was a gambler who also considered himself something of a mathematician. He computed the chance of a getting at least one six in four rolls as 2/3 \\((4 \\times (1/6) = 4/6)\\). He won quite often by betting on this event, and was convinced his computation was correct. Was it?\nThe next popular dice game was betting on at least one double six in twenty-four rolls of a pair of dice. De Méré knew that there were 36 possible outcomes when rolling a pair of dice, and therefore the chance of a double six was 1/36. Using this he concluded that the chance of at least one double six in 24 rolls was the same as that of at least one six in four rolls, that is, 2/3 (\\(24 \\times 1/36\\)). He happily bet on this event (at least one double six in 24 rolls) but to his shock, lost more often than he won! What was going on?\nWe will see later how to compute this probability, but for now we can estimate the value by simulating the game many times (1000 times each) and looking at the proportion of times we see at least one six in 4 rolls of a fair die, and do the same with at least one double six in 24 rolls.\n\n\nNumber of simulations = 1000\n\n\n  prop_wins_game_1\n1            0.514\n\n\n  prop_wins_game_2\n1            0.487\n\n\nYou can see here that the poor Chevalier wasn’t as good a mathematician as he imagined himself to be, and didn’t compute the chances correctly. The simulated probabilities are nowhere close to 4/6 and 2/3, the probabilities that he computed for the first and second game, respectively. 🧐\nBy the end of this unit, you’ll be able to conduct simulations like these yourself in R! For today, we are going to begin by introducing the conceptual building blocks behind probability."
  },
  {
    "objectID": "3-probability/01-chance-intro/notes.html#basics",
    "href": "3-probability/01-chance-intro/notes.html#basics",
    "title": "Introducing Probability",
    "section": "Basics",
    "text": "Basics\nFirst, let’s establish some terminology:\n\n\nExperiment\n\nAn action, involving chance, that can result in a finite number of possible outcomes (results of the experiment). For example, a coin toss is an experiment, and the possible outcomes are the coin landing heads or tails.\n\n\n\n\n\nOutcome space\n\nThis is just a set. It is the collection of all the possible outcomes of an experiment is called an outcome space or sample space, and we denote it by the upper case Greek letter \\(\\Omega\\) (``Omega”). For example, if we toss a coin, then the corresponding outcome space is \\(\\Omega = \\{\\text{Heads, Tails}\\}\\). If we roll a die, then the corresponding outcome space \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\). We will denote a set by enclosing the elements of the set in braces: \\(\\{ \\}\\).\n\n\n\n\n\nEvent\n\nA collection of outcomes as a result of the experiment being performed, perhaps more than once. For example, we could toss a coin twice, and consider the event of both tosses landing heads. We usually denote events by upper case letters from the beginning of the alphabet: \\(A, B, C, \\ldots\\). An event is a subset of the outcome space, and we denote this by writing \\(A \\subset \\Omega\\).\n\n\n\n\n\nP(A)\n\nFor any event \\(A\\), we write the probability of \\(A\\) as \\(P(A)\\).\n\n\n\n\n\nEqually likely outcomes\n\nWhen all the possible outcomes in a finite outcome space of size \\(n\\) happen with the same probability, which is \\(\\displaystyle \\frac{1}{n}\\).\n\n\nLet’s say that there are \\(n\\) possible outcomes in the outcome space \\(\\Omega\\), and an event \\(A\\) has \\(k\\) possible outcomes out of those \\(n\\). If all the outcomes are equally likely to happen (as in a die roll or coin toss), then we say that the probability of \\(A\\) occuring is \\(\\displaystyle \\frac{k}{n}\\).\n\\[\nP(A) = \\frac{k}{n}\n\\]\n\nExample: Tossing a fair coin\n\n\n\n\nSuppose we toss a fair coin, and I ask you what is the chance of the coin landing heads. Like most people, you reply 50%. Why? Well… (you reply) there are two possible things that can happen, and if the coin is fair, then they are both equally likely, so the probability of heads is 1/2 or 50%.\nHere, we have thought about an event (the coin landing heads), seen that there is one outcome in that event, and two outcomes in the outcome space, so we say the probability of the event, \\(P(\\text{Heads})\\), is 1/2.\nExample: Tossing a fair six-sided die6\n\nConsider rolling a fair six-sided die: six outcomes are possible so \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\). Since the die is fair, each outcome is equally likely, with probability \\(= \\displaystyle \\frac{1}{6}\\). We can list the outcomes and their probabilities in a table.\n\n\n\n\n\n\n\n\n\n\n\nOutcome\n\\(1\\)\n\\(2\\)\n\\(3\\)\n\\(4\\)\n\\(5\\)\n\\(6\\)\n\n\nProbability\n\\(\\displaystyle \\frac{1}{6}\\)\n\\(\\displaystyle \\frac{1}{6}\\)\n\\(\\displaystyle \\frac{1}{6}\\)\n\\(\\displaystyle \\frac{1}{6}\\)\n\\(\\displaystyle \\frac{1}{6}\\)\n\\(\\displaystyle \\frac{1}{6}\\)\n\n\nLet \\(A\\) be the event that an even number is rolled. Then the set \\(A\\) can be written \\(\\{2,4,6\\}\\). Since all of these outcomes are equally likely:\n\\[P(A) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{3}{6}\\]"
  },
  {
    "objectID": "3-probability/01-chance-intro/notes.html#axioms-of-probability",
    "href": "3-probability/01-chance-intro/notes.html#axioms-of-probability",
    "title": "Introducing Probability",
    "section": "Axioms of probability",
    "text": "Axioms of probability\nIn order to compute the probabilities of events, we need to set some basic mathematical rules called axioms (which are intuitively clear if you think of the probability of an event as the proportion of the outcomes that are in it). There are three basic rules that will help us compute probabilities:\n\nAxiom 1\n\nThe chance of any event is at least \\(0\\): \\(P(A) \\ge 0\\) for any event \\(A\\).\n\nAxiom 2\n\nThe chance of an outcome being in \\(\\Omega\\) is \\(1\\): \\(P(\\Omega) = 1\\). This is true because we can consider that the probability of \\(\\Omega\\) is the number of outcomes in \\(\\Omega\\) divided by \\(n\\), which is \\(n/n = 1\\).\n\n\nBefore we write the third rule, we need some more definitions and notation:\n\n\nImpossible event\n\nAn event with no outcomes in it. Denoted by either empty braces \\(\\{\\}\\) or the symbol for the empty set \\(\\emptyset\\). The probability of the impossible event is \\(0\\).\n\n\n\n\n\nUnion of events\n\nGiven events \\(A\\), \\(B\\), we can define a new event called \\(A\\) or \\(B\\), which consists of all the outcomes that are either in \\(A\\) or in \\(B\\) or in both. This is also written as \\(A \\cup B\\), read as ``\\(A\\) union \\(B\\)’’.\n\n\n\n\n\nIntersection of events\n\nGiven events \\(A\\), \\(B\\), we can define a new event called \\(A\\) and \\(B\\), which consists of all the outcomes that are both in \\(A\\) and in \\(B\\). This is also written as \\(A \\cap B\\), read as ``\\(A\\) intersect \\(B\\)’’.\n\n\n\nNow we consider events that don’t intersect or overlap at all, that is, they are disjoint from each other, or mutually exclusive:\n\n\nMutually exclusive events\n\nIf two events \\(A\\) and \\(B\\) do not overlap, that is, they have no outcomes in common, we say that the events are mutually exclusive.\n\n\nIf \\(A\\) and \\(B\\) are mutually exclusive, then we know that if one of them happens, the other one cannot. We denote this by writing \\(A \\cap B = \\emptyset\\) and read this as \\(A\\) intersect \\(B\\) is empty. Therefore, we have that\n\\[P(A \\cap B) = P(\\emptyset) = 0\\].\n\nFor example, if we are playing De Méré’s second game, the event \\(A\\) that we roll a pair of sixes and the event \\(B\\) that we roll a pair of twos cannot happen on the same roll. These events \\(A\\) and \\(B\\) are mutually exclusive.\n\nHowever, if we roll a die, the event \\(C\\) that we roll an even number and the event \\(D\\) that we roll a prime number are not mutually exclusive, since the number 2 is both even and prime.\n\nHere’s another example that might interest soccer fans: The event that Manchester City wins the English Premier League (EPL) in 2024, and the event that Liverpool wins the EPL in 2024 are mutually exclusive, but the events that Manchester City are EPL champions in 2024 and Manchester City are EPL champions in 2023 are not mutually exclusive.\nNow for the third axiom:\n\nAxiom 3\n\nIf \\(A\\) and \\(B\\) are mutually exclusive (\\(A \\cap B = \\emptyset\\)), then\n\n\n\\[P(A \\cup B) = P(A) + P(B)\\] That is, for two mutually exclusive events, the probability that either of the two events might occur is the sum of their probabilities. This is called the addition rule.\nFor example, consider rolling a fair six-sided die, and the two events \\(A\\) and \\(B\\), where \\(A\\) is the event of rolling a multiple of \\(5\\), and \\(B\\) is the event that we roll a multiple of \\(2\\).\nThe only outcome in \\(A\\) is \\(\\{5\\}\\), while \\(B\\) consists of \\(\\{2, 4, 6\\}\\). \\(P(A) = 1/6\\), and \\(P(B) = 3/6\\). Since \\(A \\cap B =\\emptyset\\), that is, \\(A\\) and \\(B\\) have no outcomes in common, we have that\n\\[P(A \\cup B) = P(A) + P(B) = \\frac{1}{6} + \\frac{3}{6} = \\frac{4}{6}\\]\n\nThe complement rule\nHere is an important consequence of axiom 3. Let \\(A\\) be an event in \\(\\Omega\\). The complement of \\(A\\), written as \\(A^C\\), consists of all those outcomes in \\(\\Omega\\) that are not in \\(A\\). Then we have the following rule:\n\\[P(A) + P(A^C) =  1\\]\nThis is because \\(A  \\cup A^C = \\Omega\\), and \\(A  \\cap A^C \\emptyset\\)).\n\nAn example with the axioms: penguins\nConsider the penguins dataset, which has 344 observations, of which 152 are Adelie penguins and 68 are Chinstrap penguins. Suppose we pick a penguin at random, what is the probability that we would pick an Adelie penguin? What about a Gentoo penguin?\n\nCheck your answer\nLet \\(A\\) be the event of picking an Adelie penguin, \\(C\\) be the event of picking a Chinstrap penguin, and \\(G\\) be the event of picking a Gentoo penguin.\nAssuming that all the penguins are equally likely to be picked, we see that then \\(P(A) = 152/344\\), and \\(P(C) = 68/344\\).\nSince only one penguin is picked, we see that \\(A, C\\), and \\(G\\) are mutually exclusive. This means that \\(P(A)+P(C)+P(G) = 1\\), since \\(A, C\\), and \\(G\\) together make up all of \\(\\Omega\\).\nTherefore the complement of \\(G\\), \\(G^C\\), which is a penguin that is not Gentoo, consists of Adelie and Chinstrop penguins, and by the addition rule,\n\\[P(G^C) = P(A \\cup C) = P(A) + P(C) = (152+68)/344 = 220/344\\]\nFinally, the complement rule tells us that\n\\[P(G) = 1 - P(G^C) = 1 - 220/344 = 124/344\\].\n\n\n\n\n\n\nWARNING!!\n\n\n\nWe use \\(A\\) to denote an event or a set, while P(A) is a number - you can think of \\(P(A)\\) as representing the relative size of \\(A\\). This means that the following types of statements don’t make sense as we haven’t defined what it means to add sets or union numbers etc.:\n\n\n\\(P(A) \\cup P(B)\\) or \\(P(A) \\cap P(B)\\)\n\n\n\\(A + B\\), or \\(A - B\\), or \\(A \\times B\\) etc"
  },
  {
    "objectID": "3-probability/01-chance-intro/notes.html#venn-diagrams",
    "href": "3-probability/01-chance-intro/notes.html#venn-diagrams",
    "title": "Introducing Probability",
    "section": "Venn Diagrams",
    "text": "Venn Diagrams\nWe often represent events using Venn diagrams. The outcome space \\(\\Omega\\) is usually represented as a rectangle, and events are represented as circles inside \\(\\Omega\\). Here is a Venn diagram showing two events \\(A\\) and \\(B\\), their intersection, and their union:\n\n\n\n\nHere is a Venn diagram showing two mutually exclusive events (no overlap):"
  },
  {
    "objectID": "3-probability/01-chance-intro/notes.html#further-examples",
    "href": "3-probability/01-chance-intro/notes.html#further-examples",
    "title": "Introducing Probability",
    "section": "Further examples",
    "text": "Further examples\n1. Tossing a fair coin\nSuppose we toss a coin twice and record the equally likely outcomes. What is \\(\\Omega\\)? What is the chance of at least one head?\nSolution: \\(\\Omega = \\{HH, HT, TH, TT\\}\\), where \\(H\\) represents the coin landing heads, and \\(T\\) represents the coin landing tails. Note that since we can get exactly one head and one tail in two ways, we have to write out both ways so that all the outcomes are equally likely.\nNow, let \\(A\\) be the event of getting at least one head in two tosses. We can do this by listing the outcomes in \\(A\\): \\(A = \\{HH, HT, TH\\}\\) and so \\(P(A) = 3/4\\).\nAlternatively, we can consider \\(A^C\\) which is the event of no heads, so \\(A^C = \\{TT\\}\\) and \\(P(A^C) = 1/4\\).\nIn this case, \\(P(A) = 1- P(A^C) = 1-1/4 = 3/4\\).\nNow you try: Let \\(\\Omega\\) be the outcome space of tossing a coin three times. What is the probability of at least one head? What about exactly one head?\n\nCheck your answer\n\\(\\Omega = \\{HHH, HHT, HTH, THH, HTT, THT, TTH, TTT \\}\\).\nLet \\(A\\) be the event of at least one head. Then \\(A^C\\) is the event of no heads, so \\(A^C = \\{TTT\\}\\), and \\(P(A^C) = 1/8\\). Therefore \\(P(A) = 1-1/8 = 7/8\\). Note that this is much quicker than listing and counting the outcomes in \\(A\\).\nIf \\(B\\) is the event of exactly one head, then \\(B = \\{HTT, THT, TTH\\}\\) and \\(P(B) = 3/8\\).\n2. A box of tickets\n\n\n\n\nConsider the box above which has five almost identical tickets. The only difference is the value written on them. Imagine that we shake the box to mix the tickets up, and then draw one ticket without looking so that all the tickets are equally likely to be drawn7.\nWhat is the chance of drawing an even number?\n\nCheck your answer\nSolution:\nLet \\(A\\) be the event of drawing an even number, then \\(A = \\{2, 2, 4\\}\\): we list 2 twice because there are two tickets marked 2, making it twice as likely as any other number. \\(P(A) = 3/5\\)\n3. Tossing a biased coin\nSuppose I have a coin that is twice as likely to land heads as it is to land tails. This means that I cannot represent \\(\\Omega\\) as \\(\\{H, T\\}\\) since heads and tails are not equally likely. How should I write \\(\\Omega\\) so that the outcomes are equally likely?\n\nCheck your answer\nSolution:\nIn this case, we want to represent equally likely outcomes, and want \\(H\\) to be twice as likely as \\(T\\). We can therefore represent \\(\\Omega\\) as \\(\\{H, H, T \\}\\). Now the chance of the coin landing \\(H\\) can be written as \\(P(A)\\) where \\(A\\) is the event the coin lands \\(H\\) is given by 2/3.\nSuppose we toss the coin twice. How would we list the outcomes so that they are equally likely? Now we have to be careful, and think about all the things that can happen on the second toss if we have \\(H\\) on the first toss.\nThis is much easier to imagine if we imagine drawing twice from a box of tickets, but putting the first ticket back before drawing the second (to represent the fact that the probabilities of landing \\(H\\) or \\(T\\) stay the same on the second toss.)\nNow, imagine the box of tickets that represents \\(\\Omega\\) to be \\(\\fbox{H, H, T}\\). We draw one ticket at first, which could be one of three tickets (there are two tickets that could be \\(H\\), and one \\(T\\)). We can represent it using the following picture:\n\n\n\n\nFrom this picture, where we use color to distinguish the two different outcomes of heads and one outcome of tails, we can see that there are 9 possible outcomes that are equally likely, and we get the following probabilities (where \\(HT\\), for example, represents the event that the first toss is heads, followed by the second toss being tails.)\n\\(P(HH) = 4/9,\\; P(HT) = P(TH) = 2/9, P(TT) = 1/9\\) (Check that the probabilities sum to 1!)\n\n\n\n\n\n\nAsk yourself\n\n\n\nWhat box would we use if the coin is not a fair coin, but lands heads \\(5\\) out of \\(6\\) times?\n\n\n4. Betting on red in roulette\n\n\n\n\nAn American roulette wheel has 38 pockets8, of which 18 are red, 18 black, and 2 are green. The wheel is spun, and a small ball is thrown on the wheel so that it is equally likely to land in any of the 38 pockets. Players bet on which colored or numbered pocket the ball will come to rest in. If you bet one dollar that the ball will land on red, and it does, you get your dollar back, and you win one more dollar, so your net gain is $1. If it doesn’t, and lands on a black or green number, you lose your dollar, and your net “gain” is -$1.\nWhat is the chance that we will win one dollar on a single spin of the wheel?\nHint Write out the chance of the ball landing in a red pocket, and not landing in a red pocket."
  },
  {
    "objectID": "3-probability/01-chance-intro/notes.html#the-ideas-in-code",
    "href": "3-probability/01-chance-intro/notes.html#the-ideas-in-code",
    "title": "Introducing Probability",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nOur first step toward simulating experiments is introducing randomness in R. The following three functions are a good start.\nThree useful functions\n1. sample(): randomly picks out elements (items) from a vector\nDrawing from a box of tickets is easily simulated in R, since there is a convenient function sample() that does exactly what we need: draw tickets from a “box” (which needs to be a vector).\n\n\nArguments\n\n\nx: the vector to be sampled from, this must be specified\n\nsize: the number of items to be sampled, the default value is the length of x\n\n\nreplace: whether we replace a drawn item before we draw again, the default value is FALSE, indicating that we would draw without replacement.\n\n\n\nExample: one sample of size 2 from a box with tickets from 1 to 6\n\ndie &lt;- c(1, 2, 3, 4, 5, 6)\nsample(die, size = 2, replace = FALSE)\n\n[1] 6 3\n\n\nWhat would happen if we don’t specify values for size and replace?\n\ndie &lt;- c(1, 2, 3, 4, 5, 6)\nsample(die)\n\n[1] 2 6 3 4 1 5\n\n\nWhat would we do differently if we wanted to simulate two rolls of a die?\n\nCheck your answer\nWe would sample twice from the vector die with replacement:\n\ndie &lt;- c(1, 2, 3, 4, 5, 6)\nsample(die, size = 2, replace = TRUE)\n\n[1] 6 4\n\n\n2. set.seed(): returns the random number generator to the point given by the seed number\nThe random number generator in R is called a “Pseudo Random Number Generator”, because the process can be controlled by a “seed number”. These are algorithmic random number generators, which means that if you provide the same seed (a starting number), R will generate the same sequence of random numbers. This makes it easier to debug your code, and reproduce your results if needed.\n\n\nArguments\n\n\nn: the seed number to use. You can use any number you like, for example 1, or 31415 etc You might have noticed that each time you run sample in the code chunk above, it gives you a different sample. Sometimes we want it to give the same sample so that we can check how the code is working without the sample changing each time. We will use the set.seed function for this, which ensures that we will get the same random sample each time we run the code.\n\n\n\nExample: one sample of size 2 from a box with tickets from 1 to 6\n\nset.seed(1)\nsample(die, size = 2, replace = TRUE)\n\n[1] 1 4\n\n\nExample: another sample of size 2 from a box with tickets from 1 to 6\n\nset.seed(1)\nsample(die, size = 2, replace = TRUE)\n\n[1] 1 4\n\n\nNotice that we get the same sample. You can try to run sample(die) without using set.seed() and see what happens.\nThough we used set.seed() twice here to demonstrate its purpose, generally, you will only need to run set.seed() once time per document. This is a line of code that fits perfectly at the beginning of your work, when you are also loading libraries and packages.\n3. seq(): creates a sequence of numbers\nAbove, we created the vector die using die &lt;- c(1, 2, 3, 4, 5, 6), which is fine, but this method would be tedious if we wanted to simulate a 20-sided die, for instance. The function seq() allows us to create any sequence we like by specifying the starting number, how we want to increment the numbers, and either the ending number or the length of the sequence we want.\n\n\nArguments\n\n\nfrom: where to start\n\nby: size of jump from number to number (the increment)\n\n\n\nYou can end a sequence in one of two ways: - to: at what number should the sequence end - length: how long should the sequence be\nExample: sequence with the to argument\n\nodds_1 &lt;- seq(from = 1, by = 2, to = 9)\nodds_1\n\n[1] 1 3 5 7 9\n\n\nExample: sequence with the length argument\n\nodds_2 &lt;- seq(from = 1, by = 2, length = 5)\nodds_2\n\n[1] 1 3 5 7 9"
  },
  {
    "objectID": "3-probability/01-chance-intro/notes.html#summary",
    "href": "3-probability/01-chance-intro/notes.html#summary",
    "title": "Introducing Probability",
    "section": "Summary",
    "text": "Summary\n\nIn this lecture, we introduced equally likely outcomes,and defined the outcome space of an experiment.\nThen, using equally likely outcomes, we defined the probability of an event as the ratio of the number of outcomes in the event to the number of total outcomes in the outcome space.\nWe wrote down the axioms (fundamental rules) of probability, after defining unions, intersections, and mutually exclusive events and Venn diagrams.\nIn the “Ideas in Code” section, explored how to simulate probabilities using sample() and replicate(), and learned another useful function seq()"
  },
  {
    "objectID": "3-probability/01-chance-intro/notes.html#footnotes",
    "href": "3-probability/01-chance-intro/notes.html#footnotes",
    "title": "Introducing Probability",
    "section": "Footnotes",
    "text": "Footnotes\n\nPaper is at https://www.astro.sunysb.edu/fwalter/AST248/why_i_dont_have_a_girlfriend.pdf and a talk by Backus at https://www.youtube.com/watch?v=ClPPSry8bBw↩︎\nhttps://berkeleytime.com/grades/0-7077-all-all&1-7077-fall-2022-all↩︎\nhttps://www.freep.com/betting/sports/nfl-49ers-vs-chiefs-odds-moneylines-spreads-totals-best-nfl-odds-this-week↩︎\nhttps://www.thelines.com/odds/election/↩︎\nThis website was begun as poll aggregation site, by the statistician Nate Silver.↩︎\nThe singular is die and the plural is dice. If we use the word “die” without any qualifiers, we will mean a fair, six-sided die.↩︎\nWe call the tickets equally likely when each ticket has the same chance of being drawn. That is, if there are \\(n\\) tickets in the box, each has a chance of \\(1/n\\) to be drawn. We also refer to this as drawing a ticket uniformly at random, because the chance of drawing the tickets are the same, or uniform.↩︎\nPhoto via unsplash.com↩︎"
  },
  {
    "objectID": "3-probability/05-ev-se/slides.html",
    "href": "3-probability/05-ev-se/slides.html",
    "title": "Expected Value and Variance",
    "section": "",
    "text": "Concept review\nConcept questions\nPS 10\nBreak\nLab 3 slides\nLab 3"
  },
  {
    "objectID": "3-probability/05-ev-se/slides.html#section",
    "href": "3-probability/05-ev-se/slides.html#section",
    "title": "Expected Value and Variance",
    "section": "",
    "text": "Concept review\nConcept questions\nPS 10\nBreak\nLab 3 slides\nLab 3"
  },
  {
    "objectID": "3-probability/05-ev-se/slides.html#section-1",
    "href": "3-probability/05-ev-se/slides.html#section-1",
    "title": "Expected Value and Variance",
    "section": "",
    "text": "Let \\(X\\) be a random variable such that \\[\nX = \\begin{cases}\n            -1, & \\text{ with probability } 1/3\\\\\n            0, &  \\text{ with probability } 1/6\\\\\n            1, &  \\text{ with probability } 4/15 \\\\\n            2, & \\text{ with probability } 7/30 \\\\\n         \\end{cases}\n\\]\n\nDraw the graph of the cdf of \\(X\\)\n\n\n\nCodecountdown::countdown(8, bottom = 0)\n\n\n\n−&plus;\n\n08:00\n\n\n\n\n\nCompute the expected value and variance of \\(X\\)\n\n\n\nwrite down pmf with denominator 30, and draw cdf on the board. P(X = -1)= 5/30, P(X = 0)= 10/30, P(X = 1)= 8/30, P(X = 2)= 7/30 E(X) = (-1)(5/30) + 0(10/30) + 1(8/30) + 2(7/30) (-10 + 0 + 8 + 14)/30 = 12/30 = 2/5 Var(X) = E(X^2) - 4/25 = (10 + 0 + 8 +28)/30 -4/25 = 23/15- 4/25 ~~ 1.37333 Graph the pmf and mark the expectation The numbers are not pretty but the point is to show them the computation and graph The rest of the ideas can be recapped as you go over the concept questions You may want to review linearity of expectation here, or save it for when going over the PS"
  },
  {
    "objectID": "3-probability/05-ev-se/slides.html#section-2",
    "href": "3-probability/05-ev-se/slides.html#section-2",
    "title": "Expected Value and Variance",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\\(X\\) is a random variable with the distribution shown below:\n\\[\nX = \\begin{cases}\n3, \\; \\text{ with prob } 1/3\\\\\n4, \\; \\text{ with prob } 1/4\\\\\n5, \\; \\text{ with prob } 5/12\n\\end{cases}\n\\]\nConsider the box with tickets: \\(\\fbox{3}\\, \\fbox{3}\\, \\fbox{3} \\,\\fbox{4}  \\,\\fbox{4} \\,\\fbox{4} \\,\\fbox{4} \\,\\fbox{5} \\,\\fbox{5}\\, \\fbox{5} \\,\\fbox{5} \\,\\fbox{5}\\)\nSuppose we draw once from this box and let \\(Y\\) be the value of the ticket drawn. Which random variable has a higher expectation?\n\nThe expected value of \\(X\\) is ____ the expected value of \\(Y\\).\n\n\nThis is a reading question. Note that the pmf implied by the box is not the same as the given pmf. In the box, 4 has a higher probability, so the average is higher. They do not need to actually do the computation. At this point, ask them what box would they use so that if \\(X\\) is the value of a randomly selected ticket, the distribution of \\(X\\) is exactly the distribution shown here."
  },
  {
    "objectID": "3-probability/05-ev-se/slides.html#section-3",
    "href": "3-probability/05-ev-se/slides.html#section-3",
    "title": "Expected Value and Variance",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nProf. Stoyanov’s Zoom office hours are not too crowded this spring. She observes that number of Stat 20 students coming to her Thursday office hours have a Poisson(2) distribution. There is one Data 88 student from a previous semester who is always there (they want a letter of recommendation).\n\nWhat is the expected value (EV) and variance (V) of the number of students in her Zoom office hours?\n\n\nThis is also a reading question, with &lt; 50% getting it right. Let \\(X\\) be the number of Stat 20 students who go to the office hours. Then \\(E(X) = 2 = Var(X)\\). But the number of students in the office is \\(X+1\\) since that one student is always there. \\(E(X+1) = 2+1 = 3\\) and \\(Var(X+1) = Var(X) = 2\\)"
  },
  {
    "objectID": "3-probability/05-ev-se/slides.html#section-4",
    "href": "3-probability/05-ev-se/slides.html#section-4",
    "title": "Expected Value and Variance",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nLet \\(X\\) be a discrete uniform random variable on the set \\(\\{-1, 0, 1\\}\\).\n\nIf \\(Y=X^2\\), what is \\(E(Y)\\)?"
  },
  {
    "objectID": "3-probability/05-ev-se/slides.html#section-5",
    "href": "3-probability/05-ev-se/slides.html#section-5",
    "title": "Expected Value and Variance",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nLet \\(X\\) be a discrete uniform random variable on the set \\(\\{-1, 0, 1\\}\\).\n\nIf \\(W = \\min(X, 0.5)\\), what is \\(E(W)\\)?\n\n\nbegin by asking them what is What is \\(E(X)\\)? And pointing out that if we have a symmetric distribution then EX is in the middle. Emphasize the idea of it being a “typical” value and an average or a mean. This is so they practice computing the EV of a function of a rv. Have them make a table (or perhaps you draw a table) of \\(X\\), \\(Y\\), \\(W\\) and \\(f(x)\\)"
  },
  {
    "objectID": "3-probability/06-normal-approx/slides.html",
    "href": "3-probability/06-normal-approx/slides.html",
    "title": "Normal Approximations",
    "section": "",
    "text": "Concept Questions\nQuiz 2 review\nBreak\nLab 3.1 and 3.2"
  },
  {
    "objectID": "3-probability/06-normal-approx/slides.html#section",
    "href": "3-probability/06-normal-approx/slides.html#section",
    "title": "Normal Approximations",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nA die will be rolled \\(n\\) times and the object is to guess the total number of spots in \\(n\\) rolls, and you choose \\(n\\) to be either 50 or 100. There is a one-dollar penalty for each spot that the guess is off. For instance, if you guess 200, and the total is 215, then you lose 15 dollars. Which do you prefer? 50 throws, or 100?*\n\nWhich do you prefer? \\(n = 50\\) rolls, or \\(n = 100\\) rolls?\n\n * From the text Statistics by Freedman, Pisani, and Purves \n\nPrefer 50 because the square root law tells us the the spread of the distribution will be smaller for fewer rolls. If we were computing the average number of spots, then we would pick larger \\(n\\) since \\(\\sqrt{n}\\) goes in the denominator of SE."
  },
  {
    "objectID": "3-probability/06-normal-approx/slides.html#section-1",
    "href": "3-probability/06-normal-approx/slides.html#section-1",
    "title": "Normal Approximations",
    "section": "",
    "text": "Codecountdown::countdown(2, bottom = 0)\n\n\n\n−&plus;\n\n02:00\n\n\n\nOne hundred draws will be made with replacement from a box with tickets \\(\\fbox{0}\\, \\fbox{2}\\, \\fbox{3} \\,\\fbox{4}  \\,\\fbox{6}\\). Which of the following statements are true? *\n\n\nThe expected value of the sum of the one hundred draws is 300, give or take 20 or so.\nThe expected value of the sum of the one hundred is 300.\nThe sum of the one hundred draws is 300, give or take 20 or so.\nThe sum of the one hundred draws is 300.\n\n\n * From the text Statistics by Freedman, Pisani, and Purves \n\nThe EV is a constant, and note that the box average is (0+2+3+4+6)/5 = 3, so the EV of a single draw is 3, and the EV of the sum of 100 draws is \\(100\\times 4 = 300\\). The give or take number given here is the box SD \\(\\times 10 = 2\\times 10 = 20\\). You can let \\(X\\) be a random variable that is the value of one ticket drawn at random from this box, and then we want \\(S_{100}\\). This can be boardwork."
  },
  {
    "objectID": "3-probability/06-normal-approx/slides.html#section-2",
    "href": "3-probability/06-normal-approx/slides.html#section-2",
    "title": "Normal Approximations",
    "section": "",
    "text": "Codecountdown::countdown(1, right = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nWe have two random variables: \\(X \\sim\\) Binomial(\\(10, 0.2\\)) and \\(Y\\) is the random variable that is the value of one ticket drawn at random from a box with tickets \\(\\fbox{0}\\, \\fbox{2}\\, \\fbox{3} \\,\\fbox{4}  \\,\\fbox{6}\\).\nWe take the sum of 100 iid random variables for each of \\(X\\) and \\(Y\\), called \\(SX_{100}\\) and \\(SY_{100}\\). The empirical distributions of \\(SX_{100}\\) and \\(SY_{100}\\) are plotted below.\n\n\n\n\n\n\n\n\n\nWhich distribution belongs to which random variable?\n\n\nThey need to remember that the expected value of S_x will be 200, and for S_y 300."
  },
  {
    "objectID": "3-probability/06-normal-approx/extra-practice.html",
    "href": "3-probability/06-normal-approx/extra-practice.html",
    "title": "Continuous Distributions and Normal Approximations",
    "section": "",
    "text": "The function below is a valid probability density function. Hint: The area of a circle is \\(\\pi r^2\\) where \\(r\\) is the radius of the circle.\n\n\n\n\n\n\n\n\n\n\\(\\textcolor{white}{space space space}\\) \\(\\bigcirc\\) True \\(\\textcolor{white}{space space space}\\) \\(\\bigcirc\\) False\nWe are going to study what I would call a “pessimistic random walk”. Say you are in a one-dimensional space, and you can do one of three things, randomly, at any given moment:\n\nGo forward one step\nStay put\nGo backwards two steps\n\n\nLet \\(X_1\\) be the number of steps taken forward/backward. Write down \\(f(x)\\), the probability mass function for \\(X_1\\).\n\n\n\nCalculate \\(\\mathbb{E}(X_1)\\) and \\(SD(X_1)\\).\n\n\n\n\n\nSuppose you repeat the experiment \\(n = 10,000\\) times, and each decision you make is independent of the last. Calculate \\(\\mathbb{E}(S_{10,000})\\) and \\(SD(S_{10,000})\\), as well as \\(\\mathbb{E}(\\bar{X}_{10,000})\\) and \\(SD(\\bar{X}_{10,000})\\).\n\n\n\n\n\nRemember that as \\(n\\) gets large, \\(S_n\\) is approximately \\(Normal(\\mu = E(S_n), \\sigma = SD(S_n))\\). Find an approximate probability that after \\(10,000\\) experiments, you have taken less than 3200 steps total in the “wrong direction.” You may use R to help you calculate this probability.\n\n\n\n\nIn R, create a vector of 10,000 observations. Each of these observations will be one \\(S_{10,000} = X_1 + X_2 + ... + X_{10,000}\\). In other words, to obtain one observation, you should sample 10,000 random variables from the distribution of the pessimistic random walk, and take their sum.\n\n\n\n\nNow, write R code to create a vector of another 10,000 observations. This time, generate each observation from the approximate distribution of \\(S_{10,000} = X_1 + X_2 + ... + X_{10,000}\\).\n\n\n\nPut the two vectors from Question 6 and Question 7 into a dataframe called q8. Then, make one histogram of the observations from Question 6 and another of the observations from Question 7. Finally, save each of these histograms into plot objects. Write the R code you used below.\n\n\n\n\nFinally, use syntax from patchwork library to stack the two plots on top of one another. Write the code you used below.\n\n\n\nWhat should the two distributions look like in relation to one another? Answer this question and then state if your results match your expectations. If they don’t match your expectations, give a possible reason.\n\n\n\n\nUsing the column in q8 of the \\(S_{10,000}\\)’s generated from the Question 6, write R code to roughly estimate the probability that you have taken less than 3200 steps in the “wrong direction” after 10,000 repetitions of the experiment that you found in Question 5. Hint: see if you can calculate a proportion."
  },
  {
    "objectID": "3-probability/04-random-variables/slides.html",
    "href": "3-probability/04-random-variables/slides.html",
    "title": "Random Variables",
    "section": "",
    "text": "PS 8: time to work on it and then review\nBrief lecture on random variables\nBreak\nConcept Questions\nPS 9\n\n\nsuggested times:\n\nPS 8: time to work on it and then review (25 minutes)\nBrief lecture on random variables (20 minutes)\nBreak (5 mins)\nConcept Questions (20 minutes)\nPS 9 (40 minutes)"
  },
  {
    "objectID": "3-probability/04-random-variables/slides.html#agenda",
    "href": "3-probability/04-random-variables/slides.html#agenda",
    "title": "Random Variables",
    "section": "",
    "text": "PS 8: time to work on it and then review\nBrief lecture on random variables\nBreak\nConcept Questions\nPS 9\n\n\nsuggested times:\n\nPS 8: time to work on it and then review (25 minutes)\nBrief lecture on random variables (20 minutes)\nBreak (5 mins)\nConcept Questions (20 minutes)\nPS 9 (40 minutes)"
  },
  {
    "objectID": "3-probability/04-random-variables/slides.html#random-variables",
    "href": "3-probability/04-random-variables/slides.html#random-variables",
    "title": "Random Variables",
    "section": "Random variables",
    "text": "Random variables\n\n\nLet \\(X\\) be the number of heads in three tosses of a fair coin. What is the distribution of \\(X\\)? That is, what is \\(f(x) = P(X = x)\\)? What values will \\(x\\) take?\nWhat about if \\(X\\) is the number of heads in 3 tosses of a biased coin, where the chance of heads is \\(\\frac{2}{3}\\)? Now what is \\(f(x) = P(X = x)\\)?\nNow suppose we toss a fair coin until the first time it lands heads, and let \\(X\\) be the number of tosses (including the last one, which is the first time the coin lands heads). What is the pmf of \\(X\\)? Is it binomial?\nFinally, let’s consider a deck of cards, and we are interested in the number of hearts dealt in a hand of five. Call this number \\(X\\). What is the pmf of \\(X\\)?\n\n\n\nWrite out outcome space of 3 coin flips of a fair coin and use this to define rv X = number of heads. Write out the pmf and cdf, and draw the cdf. Show why \\(X\\) is binomial and what are \\(n\\) and \\(p\\)\nShow what happens if coin is unfair. How the probabilities change.\nDemo dbinom and pbinom and connection to pmf and cdf. Show how we can use rbinom to get an empirical histogram for three flips of fair or biased coin."
  },
  {
    "objectID": "3-probability/04-random-variables/slides.html#fx-and-fx",
    "href": "3-probability/04-random-variables/slides.html#fx-and-fx",
    "title": "Random Variables",
    "section": "\n\\(f(x)\\) and \\(F(x)\\)\n",
    "text": "\\(f(x)\\) and \\(F(x)\\)\n\n\n\n\\(f(x)\\) is the probability mass function of \\(X\\). What does that mean? What is the connection to the distribution table? The probability histogram?\n\\(F(x)\\) is the cumulative distribution function.\nWhat is the connection between \\(f\\) and \\(F\\)?\n\n\n\nDraw an example on the board. Maybe box of tickets { 0, 0, 0 , 1, 1, 2, 2, 2} draw one ticket, and let \\(X\\) be the value of the ticket. Write out both \\(f(x)\\) and \\(F(x)\\) in function notation, then draw the cdf and show how we can get from the cdf to the pmf."
  },
  {
    "objectID": "3-probability/04-random-variables/slides.html#section",
    "href": "3-probability/04-random-variables/slides.html#section",
    "title": "Random Variables",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nRoll a pair of fair six-sided dice and let \\(X = 1\\) if the dice land showing the same number of spots, and \\(0\\) otherwise. For example, if both dice land \\(2\\), then \\(X = 1\\), but if one lands \\(2\\) and the other lands \\(3\\), then \\(X = 0\\).\n\nWhat is \\(P(X=1)\\)?\n\n\nThe chance that \\(X = 1\\) can be broken up into six mutually exclusive situations, that the dice both show one spot, or both show two spots, etc. Each of these has probability \\(1/36\\) so the total probability by the addition rule is \\(6/36 = 1/6\\)"
  },
  {
    "objectID": "3-probability/04-random-variables/slides.html#section-1",
    "href": "3-probability/04-random-variables/slides.html#section-1",
    "title": "Random Variables",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nThe graph of the cdf of a random variable \\(X\\) is shown below. What is \\(F(2)\\)? What about \\(f(2)\\)?\n\n\n\n\n\n\\(F(2) = 2/3, f(2) = 0\\)"
  },
  {
    "objectID": "3-probability/04-random-variables/slides.html#section-2",
    "href": "3-probability/04-random-variables/slides.html#section-2",
    "title": "Random Variables",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nYou have \\(10\\) people with a cold and you have a remedy with a \\(20\\%\\) chance of success. What is the chance that your remedy will cure at least one sufferer? (Let \\(X\\) be the number of people cured among the 10. We are looking for the probability that \\(X \\ge 1\\))\n\nWhat is the chance that at least one person is cured?\n\n\n\\(P(X \\ge 1) = 1 - P(X &lt; 1) = 1 - P(X = 0) = 1 - (1-p)^{10}\\), where \\(p = 0.2\\), because \\(X \\sim Bin(10, p)\\). \\(1-p = 0.8 \\: \\& \\: 1-0.8^{10} = 0.8926\\)"
  },
  {
    "objectID": "3-probability/04-random-variables/slides.html#section-3",
    "href": "3-probability/04-random-variables/slides.html#section-3",
    "title": "Random Variables",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nThere are 4 histograms of different Poisson distributions below. Match each distribution to its parameter \\(\\lambda\\). Recall that \\(\\lambda\\) is how many occurrences we think will happen in a given period of time.\n\\[\n(1)\\: \\lambda = 0.5 \\hspace{2cm} (2)\\: \\lambda = 1 \\hspace{2cm} (3) \\: \\lambda = 2 \\hspace{2cm} (4) \\: \\lambda = 4\\hspace{2cm}\n\\]\n\nCodex1 &lt;- rpois(500, 0.5)\nx2 &lt;- rpois(500, 1)\nx3 &lt;- rpois(500, 2)\nx4 &lt;- rpois(500, 4)\n\n  p1 &lt;- data.frame(x1 = x1) |&gt; \n  group_by(x1) |&gt;\n  summarise(props = n()/500) |&gt;\n    ggplot(aes(x = factor(x1), y = props)) + \n    geom_col(width = 0.98, fill = \"blue\") +\n     labs(x = \"x\", \n          y = \"P(X = x)\") + \n    annotate(\"text\", x = 4, y = 0.4, label = \"(B)\", size = 5)\n   \n   p2 &lt;- data.frame(x2 = x2) |&gt; \n     group_by(x2) |&gt;\n     summarise(props = n()/500) |&gt;\n     ggplot(aes(x = factor(x2), y = props)) + \n     geom_col(width = 0.98, fill = \"blue2\") +\n     labs(x = \"x\", \n          y = \"P(X = x)\") + \n      annotate(\"text\", x = 5, y = 0.3, label = \"(D)\", size = 5)\n   \n   p3 &lt;- data.frame(x3 = x3) |&gt; \n     group_by(x3) |&gt;\n     summarise(props = n()/500) |&gt;\n     ggplot(aes(x = factor(x3), y = props)) + \n     geom_col(width = 0.98, fill = \"blue3\")+\n     labs(x = \"x\", \n          y = \"P(X = x)\") + \n      annotate(\"text\", x = 7, y = 0.2, label = \"(C)\", size = 5)\n   \n   p4 &lt;- data.frame(x4 = x4) |&gt; \n     group_by(x4) |&gt;\n     summarise(props = n()/500) |&gt;\n     ggplot(aes(x = factor(x4), y = props)) + \n     geom_col(width = 0.98, fill = \"blue4\")+\n     labs(x = \"x\", \n          y = \"P(X = x)\") +\n      annotate(\"text\", x = 9.5, y = 0.15, label = \"(A)\", size = 5)\n\n   (p4+p1+p3+p2)  \n\n\n\n\n\n\n\n\nexplain the smaller the rate, the lower the probability of higher values. A-4, B-1, C-3, D-2"
  },
  {
    "objectID": "3-probability/03-probability-dsns/ps.html",
    "href": "3-probability/03-probability-dsns/ps.html",
    "title": "Probability Distributions",
    "section": "",
    "text": "Stat 20 is super fun!\n\nSuppose a character is picked at random from this sentence (not including punctuation marks). Make a table for the probability distribution of the character that is picked (either a vowel, consonant, or number).\n\n\n\n \n\n\nConsider a box with four balls in it, two of which are red and two are blue. The balls are identical, except for their color. Now suppose we draw three times at random (“at random” means that all the balls are equally likely) from this box. For each of the following scenarios, list all the possible outcomes, and make a table showing the probability distribution of these outcomes.\n\nWe draw with replacement.\n\n\n\n\n\n\nWe draw without replacement.\n\n\n\n\n\n\nConsider tossing a fair coin four times. Sketch a probability histogram for the number of heads in the four tosses. Make sure to label and title your plot (How many total equally likely outcomes are there in four tosses of a coin? Make sure you get your probabilities correct!)\n\n\n\n\n\n\n\nA test consists of 20 multiple choice questions. Each question has 4 answer choices, of which only one is correct, and three are incorrect. You haven’t studied at all for this test, and decides to answer all the questions by picking one of the four answer choices at random. What is the probability that\n\nyou answer eight questions correctly?\n\n \n\n\n\nyou answer more than two questions correctly?\n\n\nA committee of three is to be selected from among five teachers and ten students (a total of fifteen people).\n\nWhat is the probability that the committee consists of only teachers?\n\n\n\n\n\n\nWhat is the probability that the committee has at least one student?\n\n\n\n\n\n\nWhat is the probability that the committee has exactly one student?\n\n\n\n\n\n\nLet’s suppose we deal 5 cards from a standard deck. What is the probability that the hand of 5 cards contains a pair of aces?\n\n\n\n\n\nLet’s revisit Question 4 about tossing a fair coin four times.\n\nWrite code to create the probability histogram for the number of heads in four tosses (that you sketched earlier). Do it in R and then copy the code here.\n\n\n\n\n\n\nWrite code to simulate this experiment (tossing a coin four times) and counting the number of heads. Repeat this experiment 500 times, and create an empirical histogram for the number of heads in four tosses of a fair coin. Copy the code here.Do the two plots (from the previous question and this one) look similar?\n\n\n\n\n\n\n\n\nConsider the American roulette wheel (38 pockets: 18 red, 18 black, 2 green). Write code to simulate the number of times you land on a red space during 100 spins of a roulette wheel."
  },
  {
    "objectID": "3-probability/03-probability-dsns/notes.html",
    "href": "3-probability/03-probability-dsns/notes.html",
    "title": "Probability Distributions",
    "section": "",
    "text": "So far we have seen examples of outcome spaces, and descriptions of how we might compute probabilities, along with tabular representations of the probabilities. In this set of notes, we are going to talk about how to visualize probabilities using tables and histograms, as well as how to visualize simulations of outcomes from actions such as tossing coins or rolling dice."
  },
  {
    "objectID": "3-probability/03-probability-dsns/notes.html#probability-distributions-and-histograms",
    "href": "3-probability/03-probability-dsns/notes.html#probability-distributions-and-histograms",
    "title": "Probability Distributions",
    "section": "Probability distributions and histograms",
    "text": "Probability distributions and histograms\nProbability distributions\nRecall the example in which we drew a ticket from a box with 5 tickets in it:\n\n\n\n\nIf we draw one ticket at random from this box, we know that the probabilities of the four distinct outcomes can be listed in a table as:\n\n\n\n\n\n\n\n\n\nOutcome\n\\(1\\)\n\\(2\\)\n\\(3\\)\n\\(4\\)\n\n\nProbability\n\\(\\displaystyle \\frac{1}{5}\\)\n\\(\\displaystyle \\frac{2}{5}\\)\n\\(\\displaystyle \\frac{1}{5}\\)\n\\(\\displaystyle \\frac{1}{5}\\)\n\n\nWhat we have described in the table above is a probability distribution. We have shown how the total probability of one or 100% is distributed among all the possible outcomes. Since the ticket \\(\\fbox{2}\\) is twice as likely as any of the other outcomes, it gets twice as much of the probability.\nProbability histograms\nA table is nice, but a visual representation would be even better.\n\n\n\n\n\n\n\n\n\nWe have represented the distribution in the form of a histogram, with the areas of the bars representing probabilities. Notice that this histogram is different from the ones we have seen before, since we didn’t collect any data. We just defined the probabilities based on the outcomes, and then drew bars with the heights being the probabilities. This type of theoretical histogram is called a probability histogram.\nEmpirical histograms\nWhat about if we don’t know the probability distribution of the outcomes of an experiment? For example, what if we didn’t know how to compute the probability distribution above? What could we do to get an idea of what the probabilities might be? Well, we could keep drawing tickets over and over again from the box, with replacement (that is, we put the selected tickets back before choosing again), keep track of the tickets we draw, and make a histogram of our results. This kind of histogram, which is the kind we have seen before, is a visual representation of data, and is called an empirical histogram.\n\n\n\n\n\n\n\n\n\n\n\nOn the x-axis of this histogram, we have the ticket values; on the y-axis, we have the proportion of times that this ticket was selected out of the 50 with-replacement draws we took.We can see that the sample proportions looks similar to the values given by the probability distribution, but there are some differences. For example, we appear to have drawn more \\(3\\)s and less \\(4\\)s than what was to be expected. It turns out that the counts and proportions of the drawn tickets are:\n\n\n\n\nTicket\nNumber of times drawn\nProportion of times drawn\n\n\n\n\\(\\fbox{1}\\)\n10\n0.2\n\n\n\\(\\fbox{2}\\)\n24\n0.48\n\n\n\\(\\fbox{3}\\)\n10\n0.2\n\n\n\\(\\fbox{4}\\)\n6\n0.12\n\n\n\n\n\nWhat we have seen here is how when we draw at random, we get a sample that resembles the population, that is, a representative sample, but it isn’t exactly the true probabilities. If we increase our sample, however, say to 500, we will get something that more closely aligns with the truth.\nExamples\n\nRolling a pair of dice and summing the spots\nThe outcomes are already numbers, so we don’t need to represent them differently. We know that there are \\(36\\) total possible equally likely outcomes when we roll a pair of dice, but when we add the spots, we have only 11 possible outcomes, which are not equally likely (the chance of seeing a \\(2\\) is \\(1/36\\), but \\(P(6)=5/36\\)).\nThe probability histogram will have the possible outcomes listed on the x-axis, and bars of width \\(1\\) over each possible outcome. The height of these bars will be the probability, so that the areas of the bars represent the probability of the value under the bar. The height, which is the probability, is written on the top of each bar.\n\n\n\n\nWhat about the probability distribution? Make a table showing the probability distribution for rolling a pair of dice and summing the spots.\n\nCheck your answer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\n\\(2\\)\n\\(3\\)\n\\(4\\)\n\\(5\\)\n\\(6\\)\n\\(7\\)\n\\(8\\)\n\\(9\\)\n\\(10\\)\n\\(11\\)\n\\(12\\)\n\n\nProbability\n\\(\\displaystyle \\frac{1}{36}\\)\n\\(\\displaystyle \\frac{2}{36}\\)\n\\(\\displaystyle \\frac{3}{36}\\)\n\\(\\displaystyle \\frac{4}{36}\\)\n\\(\\displaystyle \\frac{5}{36}\\)\n\\(\\displaystyle \\frac{6}{36}\\)\n\\(\\displaystyle\\frac{5}{36}\\)\n\\(\\displaystyle \\frac{4}{36}\\)\n\\(\\displaystyle \\frac{3}{36}\\)\n\\(\\displaystyle \\frac{2}{36}\\)\n\\(\\displaystyle \\frac{1}{36}\\)\n\nTossing a fair coin 3 times and counting the number of heads\nWe have seen that there are 8 equally likely outcomes from tossing a fair coin three times: \\(\\{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\\}\\). If we count the number of \\(H\\) in each outcome, and write down the probability distribution of the number of heads, we get:\n\n\n\n\n\n\n\n\n\nOutcome\n\\(0\\)\n\\(1\\)\n\\(2\\)\n\\(3\\)\n\n\nProbability\n\\(\\displaystyle \\frac{1}{8}\\)\n\\(\\displaystyle \\frac{3}{8}\\)\n\\(\\displaystyle \\frac{3}{8}\\)\n\\(\\displaystyle \\frac{1}{8}\\)\n\n\nWhat would the probability histogram look like?\n\nCheck your answer\n\n\n\n\nWe are going to introduce some special distributions. We have seen most of these distributions, but will introduce some names and definitions. Before we do this, let’s recall how to count the number of outcomes for various experiments such as tossing coins or drawing tickets from a box (both with and without replacement)."
  },
  {
    "objectID": "3-probability/03-probability-dsns/notes.html#basic-rule-of-counting",
    "href": "3-probability/03-probability-dsns/notes.html#basic-rule-of-counting",
    "title": "Probability Distributions",
    "section": "Basic rule of counting",
    "text": "Basic rule of counting\nRecall that if we have multiple steps (say \\(n\\)) of some action, such that the \\(j^{\\text{th}}\\) step has \\(k_j\\) outcomes; then the total number of outcomes is \\(k_1 \\times k_2 \\times \\ldots k_n\\) and is obtained by multiplying the number of outcomes at each step. This principle is illustrated in the picture below: Geni the Gentoo penguin1 is trying to count how many outfits they have, if each outfit consists of a t-shirt and a pair of pants. The tree diagram below shows the number of possible outfits Geni can wear. In this example, \\(n=2\\), \\(k_1 = 3\\), and \\(k_2 = 2\\), since Geni has three t-shirts to choose from, and for each t-shirt, they have two pairs of pants, leading to a total of \\(3 \\times 2 = 6\\) outfits.\n\n\n\n\nThis example seems trivial, but it illustrates the basic principle of counting: we get the total number of possible outcomes of an action that has multiple steps, by multiplying together the number of outcomes for each step. All the counting that follows in our notes applies this rule. For example, let’s suppose we are drawing tickets from a box which has tickets marked with the letters \\(\\fbox{C}\\), \\(\\fbox{R}\\), \\(\\fbox{A}\\), \\(\\fbox{T}\\), \\(\\fbox{E}\\), and say we draw three letters with replacement (that means that we put each drawn ticket back, so the box is the same for each draw). How many possible sequences of three letters can we get? Using the counting rule,since we have \\(5\\) choices for the first letter, \\(5\\) for the second, and \\(5\\) for the third, we will have a total of \\(5\\times 5 \\times 5 = 125\\) possible words, allowing for repeated letters (that means that \\(CCC\\) is a possible outcome).\nHow many possible words are there if we draw without replacement? That is, we don’t put the drawn ticket back?\n\nCheck your answer\nWe have \\(5\\) choices for the first letter, \\(4\\) for the second, and \\(3\\) for the third, leading to \\(5 \\times 4 \\times 3 = 60\\) possible outcomes or words. Note that here we count the word \\(CRA\\) as different from the word \\(CAR\\). That is, the order in which we draw the letters matters.\nWe usually write the quantity \\(5 \\times 4 \\times 3\\) as \\(\\displaystyle \\frac{5!}{2!}\\) where \\(n! = n\\times(n-1)\\times(n-2)\\times \\ldots \\times 3 \\times 2 \\times 1\\)\nCounting the number of ways to select a subset\nWhat if, in this example of selecting \\(3\\) letters without replacement from \\(\\fbox{C}\\), \\(\\fbox{R}\\), \\(\\fbox{A}\\), \\(\\fbox{T}\\), \\(\\fbox{E}\\), the order does not matter - we don’t count the order in which the letters , just which letters were selected, that is, only the letters themselves matter. For example, the words \\(CRA,\\,CAR,\\,ARC,\\,ACR,\\,RAC,\\,RCA\\) all count as the same word, so we will count all \\(6\\) words as the same subset of letters. We have to take the number that we got from earlier and divide it by the number of words that can be made from \\(3\\) letters (number of rearrangements), which is \\(3 \\times 2\\times 1\\). This gives us the number of ways that we can choose \\(3\\) letters out of \\(5\\), which is \\[\n\\frac{\\left(5!/2!\\right)}{3!} = \\frac{5!}{2!\\; 3!},\n\\]\nand is called the number of combinations of \\(5\\) things taken \\(3\\) at a time.\nTo recap: when we draw \\(k\\) items from \\(n\\) items without replacement, we have two cases: either we care in what order we draw the \\(k\\) items and the different arrangements of the same set of \\(k\\) items have to be counted separately. The number of such arrangements is called the permutations of \\(n\\) things taken \\(k\\) at a time. In the example above, we have \\(5 \\times 4\\times 3 = 60\\) ways of choosing \\(3\\) things out of \\(5\\) when we count every sequence as different.\n\n\nPermutations\n\nThe number of possible arrangements or sequences of \\(n\\) things taken \\(k\\) at a time which is given by (the ordering matters): \\[\n\\frac{n!}{(n-k)!}\n\\]\n\n\n\n\n\nCombinations\n\nNumber of ways to choose a subset of \\(k\\) things out of \\(n\\) possible things which is given by \\[\n\\frac{n!}{k!\\; (n-k)!}\n\\] This number is just the number of distinct arrangements or permutations of \\(n\\) things taken \\(k\\) at a time divided by the number of arrangements of \\(k\\) things. It is denoted by \\(\\displaystyle \\binom{n}{k}\\), which is read as “n choose k”.\n\n\n\nExample How many ways can I deal \\(5\\) cards from a standard deck of 52 cards?\n\nCheck your answer\nWhen we deal cards, order does not matter, so this number is \\(\\displaystyle \\binom{52}{5} = \\frac{52!}{(52-5)!5!} = 2,598,960\\)."
  },
  {
    "objectID": "3-probability/03-probability-dsns/notes.html#special-distributions",
    "href": "3-probability/03-probability-dsns/notes.html#special-distributions",
    "title": "Probability Distributions",
    "section": "Special distributions",
    "text": "Special distributions\nThere are some important special distributions that every student of probability must know. Here are a few, and we will learn some more later in the course. We have already seen most of these distributions. All we are doing now is identifying their names. First, we need a vocabulary term:\n\n\nParameter of a probability distribution\n\nA constant(s) number associated with the distribution. If you know the parameters of a probability distribution, then you can compute the probabilities of all the possible outcomes.\n\n\n\nEach of the distributions we will cover below has a parameter(s) associated with it.\nDiscrete uniform distribution\nThis is the probability distribution over the numbers \\(1, 2, 3 \\ldots, n\\). We have seen it for dice above. This probability distribution is called the discrete uniform probability distribution, since each possible outcome has the same probability, that is, \\(1/n\\). We call \\(n\\) the parameter of the discrete uniform distribution.\nBernoulli distribution\nThis is a probability distribution describing the probabilities associated with binary outcomes that result from one action, such as one coin toss that can either land Heads or Tails. We can represent the action as drawing one ticket from a box with tickets marked \\(\\fbox{1}\\) or \\(\\fbox{0}\\), where the probability of \\(\\fbox{1}\\) is \\(p\\), and therefore, the probability of \\(\\fbox{0}\\) is \\((1-p)\\). We have already seen some examples of probability histograms for this distribution. We usually think of the possible outcomes of a Bernoulli distribution as success and failure, and represent a success by \\(\\fbox{1}\\) and a failure by \\(\\fbox{0}\\).\n\n\n\n\nFor the Bernoulli distribution, our parameter is \\(p = P\\left(\\fbox{1}\\right)\\). If we know \\(p\\), we also know the probability of drawing a ticket marked \\(\\fbox{0}\\).\nIn the figure above, the first histogram is for a Bernoulli distribution with parameter \\(p = 1/2\\), the second \\(p=3/4\\), and the third has \\(p = 2/3\\).\nBinomial Distribution\nThe binomial distribution, which describes the total number of successes in a sequence of \\(n\\) independent Bernoulli trials, is one of the most important probability distributions. For example, consider the outcomes from tossing a coin \\(n\\) times and counting the total number of heads across all \\(n\\) tosses, where the probability of heads on each toss is \\(p\\). Each toss is one Bernoulli trial, where a success would be the coin landing heads. The binomial distribution describes the probabilities of the total number of heads in three tosses. We saw what this distribution looks like in the case where \\(n = 3\\) for three tosses of a fair coin.\nWhat would the probability distribution and histogram for the number of heads in three tosses of a biased coin like, where \\(P(H) = 2/3\\)? Make sure you know how the probabilities are computed. For example, \\(P(HHH) = (2/3)^3 = 8/27\\).\n\nCheck your answer\n\n\n\n\nNote that the outcomes \\(HHT, HTH, THH\\) all have the same probability, as do the outcomes \\(TTH, THT, HTT\\), since the probability only depends on how many heads and how many tails we see in three tosses, not the order in which we see them. We get the probability of 2 heads in 3 tosses by adding the probabilities of all three outcomes \\(HHT, HTH, THH\\), since they are mutually exclusive. (In three tosses, we can see exactly one of the possible 8 sequences listed above.)\nMore generally, suppose that we have \\(n\\) independent trials, where each trial can either result in a “success” (like drawing a ticket marked \\(\\fbox{1}\\)) with probability \\(p\\); or a “failure” (like drawing \\(\\fbox{0}\\)) with probability \\(1-p\\). In the case of the Bernoulli distribution, \\(n = 1\\).\nThe multiplication rule for independent events tells us how to compute the probability of a sequence that consisted of the first \\(k\\) trials being successes and the rest of the \\(n-k\\) trials being failures. The probability of this particular sequence of \\(k\\) successes followed by \\(n-k\\) failures is (by multiplying their probabilities) given by: \\[\np^k \\times (1-p)^{n-k}\n\\] Now this is the probability of one particular sequence: \\(SSS\\ldots SSFF \\ldots FFF\\), but as we saw in the example above, only the number of successes and failures matter, not the particular order. So every sequence of \\(n\\) trials in which we have \\(k\\) successes and \\(n-k\\) failures has the same probability.\nHow many such sequences are there? We can count them using our rules above. We have \\(n\\) spots in the sequence, of which \\(k\\) have to be successes. The number of such sequences of length \\(n\\) consisting of \\(k\\) \\(S\\)’s and \\(n-k\\) \\(F\\)’s) is given by \\(\\displaystyle \\binom{n}{k}\\). Each such sequence has probability \\(\\displaystyle p^k \\times (1-p)^{n-k}\\). Adding up all these \\(\\displaystyle \\binom{n}{k}\\) probabilities (of each such sequence) gives us the formula for the probability of \\(k\\) successes in \\(n\\) trials: \\[\n\\binom{n}{k} \\times p^k \\times (1-p)^{n-k}\n\\]\nThe probability distribution described by the above formula is called the binomial distribution. It is named after \\(\\displaystyle \\binom{n}{k}\\), which is called the binomial coefficient. The binomial distribution has two parameters: the number of trials \\(n\\) and the probability of success on each trial, \\(p\\).\nExample\nToss a weighted coin, where \\(P(\\text{heads}) = .7\\) five times. What is the probability that you see exactly four heads across these five tosses?\n\nCheck your answer\nAcross five trials, we need to see four heads and one tails. Since each toss is independent, one possible way to obtain what we are looking for is \\(HHHHT\\). This is given by\n\\[\n(.7)^4 \\times (.3)^1\n\\]\nHowever, we need to consider all the possible orderings of tosses that involve four heads and one tail. This is given by the binomial coefficient \\(\\binom{5}{4}\\). Our final probability is therefore:\n\\[\n\\binom{5}{4} (.7)^4 \\times (.3)^1 \\approx 0.36\n\\]\nHypergeometric distribution\nIn the binomial scenario described above, we had \\(n\\) independent trials, where each trial resulted in a success or a failure. This is like sampling with replacement from a box of \\(0\\)’s and \\(1\\)’s. Now consider the situation when we have a box with \\(N\\) tickets marked with either \\(\\fbox{0}\\) or \\(\\fbox{1}\\).\nAs usual, the ticket marked \\(\\fbox{1}\\) represents a success. Say the box has \\(G\\) tickets marked \\(\\fbox{1}\\) (and therefore \\(N-G\\) tickets marked \\(\\fbox{0}\\) representing failures). Suppose we draw a simple random sample of size \\(n\\) from this box. A simple random sample is a sample drawn without replacement, and on each draw, every ticket is equally likely to be selected from among the remaining tickets. Then, the probability of drawing a ticket marked \\(\\fbox{1}\\) changes from draw to draw.\nWhat is the probability that we will have exactly \\(k\\) successes among these \\(n\\) draws? The probability distribution that gives us this answer is given by\n\\[\n\\frac{\\binom{G}{k}\\times \\binom{N-G}{n-k}}{\\binom{N}{n}}\n\\]\nand is called the hypergeometric distribution. It has three parameters, \\(n\\), \\(N\\) and \\(G\\). This is a wacky formula, so let’s explain it piece by piece, starting with the numerator and then moving to the denominator!\nNumerator\nWe count the number of samples drawn without replacement that have \\(k\\) tickets marked \\(\\fbox{1}\\). Since there are \\(G\\) tickets marked \\(\\fbox{1}\\) in the box, and \\(\\displaystyle \\binom{G}{k}\\) ways to choose exactly \\(k\\) of them. Similarly, there are \\(N-G\\) tickets marked \\(\\fbox{0}\\) in the box, and \\(\\displaystyle \\binom{N-G}{n-k}\\) ways to choose exactly \\(n-k\\) of them. The total number of ways to have \\(k\\) \\(\\fbox{1}\\)s and \\(n-k\\) \\(\\fbox{0}\\)s is therefore (by multiplication):\n\\[\n\\binom{G}{k}\\times \\binom{N-G}{n-k}\n\\]\nDenominator\nWe count the total number of simple random samples of size \\(n\\) that can be drawn from a pool of \\(N\\) observations. This is given by \\(\\displaystyle \\binom{N}{n}\\).\nExample\nSay we have a box of \\(10\\) tickets consisting of \\(4\\) tickets marked \\(\\fbox{0}\\) and \\(6\\) tickets marked \\(\\fbox{1}\\), and draw a simple random sample of size \\(3\\) from this box.\nWhat is the probability that two of the tickets drawn are marked \\(\\fbox{1}\\)?\n\nCheck your answer\nWe draw \\(3\\) tickets without replacement. We need two of these tickets to be marked \\(\\fbox{1}\\) and there are six in total to choose from; we need one of them to be marked \\(\\fbox{0}\\) and there are four in total to choose from. Therefore, there are\n\\[\n\\binom{6}{2}\\times \\binom{4}{1} = 60\n\\] different ways to pick three tickets in this manner.\nHow many total ways are there to draw \\(n=3\\) tickets from a box of \\(N=10\\)? This is given by \\(\\binom{10}{3} = 120\\).\nTherefore, our final answer is given by\n\\[\n\\frac{\\binom{6}{2}\\times \\binom{4}{1}}{\\binom{10}{3}} = \\frac{60}{120} =  \\frac{1}{2}\n\\]\nBinomial vs Hypergeometric distributions\nBoth these distributions deal with:\n\na fixed number of trials, or instances of the random experiment;\noutcomes that are deemed either successes or failures.\n\nThe difference is that for a binomial random variable, the probability of a success stays the same for each trial, and for a hypergeometric random variable, the probability changes with each trial."
  },
  {
    "objectID": "3-probability/03-probability-dsns/notes.html#the-ideas-in-code",
    "href": "3-probability/03-probability-dsns/notes.html#the-ideas-in-code",
    "title": "Probability Distributions",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nBefore discussing how to simulate the distributions, we are going to introduce three more useful functions.\nThree useful functions\n1. rep(): replicates values in a vector\nSometimes we need to create vectors with repeated values. In these cases, rep() is very useful.\n\n\nArguments\n\n\nx: the vector or list that is to be repeated. This must be specified\n\ntimes: the number of times we should repeat the elements of x. This could be a vector the same length as x detailing how many times each element is to be repeated, or it could be a single number, in which case the entire x is repeated that many times.\n\neach: the default is 1, and if specified, each element of x is repeated each times.\n\n\n\n2. replicate(): repeat a specific set of tasks a large number of times.\n\n\nArguments\n\n\nn: the number of times we want to repeat the task. This must be specified\n\nexpr: the task we want to repeat, usually an expression that is some combinations of functions, for example, maybe we take a sample from a vector, and then sum the sample values.\n\n\n\n3. geom_col(): plotting with probability\nWhen plotting probability histograms, we know exactly what the the height of each bar should be. This is as opposed to the bar charts you have seen before (and empirical histograms), where we are just trying to visualize the data that we have collected.\ngeom_col() creates a bar chart in which the heights represent numbers that can be specified via an aesthetic. In other words, the y variable will appear in our call to aes()!\n\nExample: Rolling a die twice and summing the spots\n\n\n\n\n\n\nCode along\n\n\n\nAs you read through the code in this section, keep RStudio open in another window to code along at the console. Keep in mind that we use set.seed() more than once for demonstration purposes only.\n\n\nSuppose we want to simulate the task of rolling a pair of die and summing the two spots. We can accomplish this task and examine our results using the functions we have just introduced. First, we will make a vector representing a fair, six-sided die.\n\ndie &lt;- seq(from = 1, to = 6, by = 1)\n\nObtaining a sum\nMethod 1 - replicate()\n\nWe can use the sample() function to roll the die twice; this will output a vector with two die numbers. Then, we can take the sum of this vector by nesting the call to sample() inside of sum.\n\nset.seed(214)\nsum(sample(die, size = 2, replace = TRUE))\n\n[1] 7\n\n\nIf we would like to repeat this action many times (for instance, in a game of Monopoly, each player has to roll two dice on their turn and sum the spots), the replicate() function will come in handy. In the following line of code, we obtain 10 sums.\n\nreplicate(n= 10, expr = sum(sample(die, size = 2, replace = TRUE)))\n\n [1] 11  8  8 12  5  7  8  7 10  7\n\n\nMethod 2 - rep()\n\nWe could also roll the die in advance and then sample from the possible sums: 2 through 12. However, when rolling the two die, there is only one way to get a sum of \\(2\\) (both dice need to be one), but six ways to get a sum of \\(7\\). This shows that if we want to represent this action of rolling a pair of dice and taking the sum of spots, we have to use a box in which values will be repeated to reflect their probability. We can use the times argument of the rep() function to make such a box. The number \\(2\\) is repeated once, the number \\(3\\) is repeated twice, and so on until the number \\(7\\) is repeated six times. We can then sample once from this box.\n\npossible_sums &lt;- seq(from = 2, by = 1, to = 12)\ncorrect_sums &lt;- rep(possible_sums, \n                    times = c(1,2, 3, 4, 5, 6, 5, 4, 3, 2, 1))\ncorrect_sums\n\n [1]  2  3  3  4  4  4  5  5  5  5  6  6  6  6  6  7  7  7  7  7  7  8  8  8  8\n[26]  8  9  9  9  9 10 10 10 11 11 12\n\n\nTo get 10 sums as we did before, we just need to sample with ten times with replacement from this new box, correct_sums.\n\nsample(x = correct_sums, size = 10, replace = TRUE)\n\n [1] 7 4 8 9 2 7 4 7 6 8\n\n\nVisualizing our results\nMaking a probability histogram with geom_col()\n\nFirst, let’s create a vector with the probabilities associated with each possible that can be obtained from rolling two dice. We are taking these probabilities from the drawn probability histogram earlier in the notes.\n\nprob_sums &lt;- c(1,2,3,4,5,6,5,4,3,2,1)/36\n\nNow, using the above and the possible_sums vector from before, we can make a data frame with the information about the probability distribution and create a probability histogram, which in turn can be used to make a plot with geom_col().\n\nprob_hist &lt;- data.frame(possible_sums, prob_sums) |&gt;\n  ggplot(mapping = aes(x = factor(possible_sums),\n                       y = prob_sums)) +\n  geom_col(fill = \"goldenrod\") +\n  labs(x = \"sum value\",\n       y = \"probability\")\nprob_hist\n\n\n\n\n\n\n\nThe use of factor() is to make sure that for the purposes of the plot, that the sum values are treated categorically.\nPerforming a simulation and making an empirical histogram\nLet’s simulate rolling two die and and computing a sum fifty times Then, we can make a data frame out of our results and find the total amount of rolls, grouped by face. This can be done with the n() summary function– and if we divide by 50, we can get the sample proportions of each sum.\n\nset.seed(214)\n\nresults &lt;- replicate(n= 50, \n                     expr = sum(sample(die, size = 2, replace = TRUE)))\n\n\nempirical &lt;- data.frame(results) |&gt;\n  group_by(results) |&gt; \n  summarise(props = n()/50)\nempirical\n\n# A tibble: 11 × 2\n   results props\n     &lt;dbl&gt; &lt;dbl&gt;\n 1       2  0.04\n 2       3  0.02\n 3       4  0.06\n 4       5  0.1 \n 5       6  0.08\n 6       7  0.24\n 7       8  0.1 \n 8       9  0.12\n 9      10  0.14\n10      11  0.08\n11      12  0.02\n\n\nNow, we can construct an empirical histogram using the empirical data frame.\n\nemp_50 &lt;- empirical |&gt;\n  ggplot(mapping = aes(x = factor(results),\n                       y = props)) +\n  geom_col(fill = \"blue\") +\n  labs(x = \"sum value\",\n       y = \"sample proportion\")\nemp_50\n\n\n\n\n\n\n\nComparing our results to the truth\nYou may have wondered why we bothered to save the plot objects. The reason is that we can use a nifty library called patchwork which will help us to more easily visualize multiple plots at once by using mathematical and logical syntax. For instance, using + will put plots side by side.\n\nlibrary(patchwork)\nprob_hist + emp_50\n\n\n\n\n\n\n\nWith only 50 experiments run, we see that the empirical histogram doesn’t quite match. However, modify the above code by increasing the number of repetitions, and you will see the empirical histogram begin to resemble more closely true probability distribution. This is an example of long-run relative frequency."
  },
  {
    "objectID": "3-probability/03-probability-dsns/notes.html#summary",
    "href": "3-probability/03-probability-dsns/notes.html#summary",
    "title": "Probability Distributions",
    "section": "Summary",
    "text": "Summary\n\nDefined probability distributions\nStated the basic counting principle and introduced permutations and combinations\nDefined some famous named distributions (Bernoulli, discrete uniform, binomial, hypergeometric)\nVisualized probability distributions using probability histograms\nLooked at the relationship between empirical histograms and probability histograms.\nIntroduced functions rep(), replicate(), geom_col()\n\nSimulated random experiments such as die rolls and coin tosses to visualize the distributions."
  },
  {
    "objectID": "3-probability/03-probability-dsns/notes.html#footnotes",
    "href": "3-probability/03-probability-dsns/notes.html#footnotes",
    "title": "Probability Distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\nPenguin taken from art by @allison_horst↩︎"
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/slides.html",
    "href": "3-probability/02-cond-prob-indep/slides.html",
    "title": "Computing Probabilities",
    "section": "",
    "text": "Codeknitr::include_graphics(\"https://imgs.xkcd.com/comics/monty_hall.png\")\n\n\n\n\n\n\n\n\nOf course, this is the Monty Hall problem. The text for this comic is “A few minutes later, the goat behind door C drives away in the car. Can have some fun and ask them to if they chose door A and door B had a goat, would they stay with A or would they switch to C? (pollev) Setup: There are three doors A, B, C. Car behind one of them, goats behind other two. You pick a door at random (say you pick A), host opens one of the doors you didn’t pick (say they open B), and shows you a goat. Now should you stick with your original choice of A or should you switch to C. Will be fun to see what they input. The right answer is that they should switch. Waving hands, the total mass of 1 was split 1/3 to 2/3 (win to not win) with the choice of A. A is kept aside, and B is opened.Now the P(A doesn’t win) = 2/3 is moved to C.Therefore better to switch. Not worth doing it with formal notation even after going over Bayes"
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/slides.html#do-you-want-a-car-or-do-you-want-a-goat",
    "href": "3-probability/02-cond-prob-indep/slides.html#do-you-want-a-car-or-do-you-want-a-goat",
    "title": "Computing Probabilities",
    "section": "",
    "text": "Codeknitr::include_graphics(\"https://imgs.xkcd.com/comics/monty_hall.png\")\n\n\n\n\n\n\n\n\nOf course, this is the Monty Hall problem. The text for this comic is “A few minutes later, the goat behind door C drives away in the car. Can have some fun and ask them to if they chose door A and door B had a goat, would they stay with A or would they switch to C? (pollev) Setup: There are three doors A, B, C. Car behind one of them, goats behind other two. You pick a door at random (say you pick A), host opens one of the doors you didn’t pick (say they open B), and shows you a goat. Now should you stick with your original choice of A or should you switch to C. Will be fun to see what they input. The right answer is that they should switch. Waving hands, the total mass of 1 was split 1/3 to 2/3 (win to not win) with the choice of A. A is kept aside, and B is opened.Now the P(A doesn’t win) = 2/3 is moved to C.Therefore better to switch. Not worth doing it with formal notation even after going over Bayes"
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/slides.html#agenda",
    "href": "3-probability/02-cond-prob-indep/slides.html#agenda",
    "title": "Computing Probabilities",
    "section": "Agenda",
    "text": "Agenda\n\nAnnouncements\nPS 6\nConcept Review\nConcept Questions\nBreak\nPS 7 (computing probabilities)\nBreak\n\n\n\nPS 7 ~ 20/25 minutes: time for them to work, and to review the PS (If you have already given enough time, do the coin flipping activity)\nLecture on conditional probability and independence\nConcept Review\nConcept questions\nBreak ~3 mins\nHandout: PS 8 (computing probabilities)"
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/slides.html#announcements",
    "href": "3-probability/02-cond-prob-indep/slides.html#announcements",
    "title": "Computing Probabilities",
    "section": "Announcements",
    "text": "Announcements\n\nProblem Sets 6 and 7 (paper, max. 3) due Tuesday at 9am\n\n. . .\n\nNo lab this week.\n\n. . .\n\nRQ: Probability Distributions due Mon/Tues 11:59pm"
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/slides.html#rules",
    "href": "3-probability/02-cond-prob-indep/slides.html#rules",
    "title": "Computing Probabilities",
    "section": "Rules",
    "text": "Rules\n\nConditional Probabilty\n\nFor two events \\(A\\) and \\(B\\), \\(P(A \\vert B) = \\displaystyle \\frac{P(A \\text{ and } B)}{P(B)}\\)\n\nMultiplication rule\n\nFor two events \\(A\\) and \\(B\\), \\(P(A \\text{ and } B) = P(A \\vert B) P(B)\\)\n\nComplement rule\n\n\\(P(A^C) = 1 - P(A)\\)\n\nYou can tell them here that \\(A^C\\) is “not A”, and then maybe use dice or coin toss examples. Review mutually exclusive vs independent events, emphasizing that these are very different ideas, though both apply to pairs of events.\n\nMutually exclusive events means that the occurrence of one event prevents the occurrence of the other. (that is, it reduces the chance of the other occurring to 0.)\n\nIndependent events means that the occurrence of one event does not change the chance of the other occurring."
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/slides.html#concept-question-1",
    "href": "3-probability/02-cond-prob-indep/slides.html#concept-question-1",
    "title": "Computing Probabilities",
    "section": "Concept Question 1",
    "text": "Concept Question 1\n\nCodecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nFlip 3 coins, one at a time. Define the following events:\n\\(A\\) is the event that the first coin flipped shows a head\n\\(B\\) is the event that the first two coins flipped both show heads\n\\(C\\) is the event that the last two coins flipped both show tails\n\nThe events A and B are: ________\n\n\nAfter they do this and next, write out the outcome space for the results of flipping 3 coins. And go through what are A, B, and C. A and B are neither independent nor mutually exclusive."
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/slides.html#concept-question-2",
    "href": "3-probability/02-cond-prob-indep/slides.html#concept-question-2",
    "title": "Computing Probabilities",
    "section": "Concept Question 2",
    "text": "Concept Question 2\n\nCodecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nFlip 3 coins, one at a time. Define the following events:\n\\(A\\) is the event that the first coin flipped shows a head\n\\(B\\) is the event that the first two coins flipped both show heads\n\\(C\\) is the event that the last two coins flipped both show tails\n\nThe events \\(A\\) and \\(C\\) are: ________\n\n\nAfter they do this and next, write out the outcome space for the results of flipping 3 coins. And go through what are A, B, and C. A and C are independent. Show that P(A & C ) = 1/8 = P(A)P(C) = 1/4 * 1/2"
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/slides.html#concept-question-3",
    "href": "3-probability/02-cond-prob-indep/slides.html#concept-question-3",
    "title": "Computing Probabilities",
    "section": "Concept Question 3",
    "text": "Concept Question 3\nSuppose we draw 2 tickets at random without replacement from a box with tickets marked {1, 2, 3, . . . , 9}. Let A be the event that at least one of the tickets drawn is labeled with an even number, let B be the event that at least one of the tickets drawn is labeled with a prime number (recall that the number 1 is not regarded as a prime number). Suppose the numbers on the tickets drawn are 3 and 9.\n\nWhich of the following events occur?\n\n\\(A\\)\n\\(B\\)\n\\(A\\) and \\(B\\) (\\(A \\cap B\\))\n\\(A\\) and \\(B^C\\)\n\\(A^C\\) and \\(B\\)\n\n\n\nii and v, I told them they can immediately see that A^C is true since no even number, so definitely (v) and then we do have a prime number and both are odd so (ii) is true.\n\n\nCodecountdown::countdown(3, bottom = 0)\n\n\n\n−&plus;\n\n03:00"
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/slides.html#section",
    "href": "3-probability/02-cond-prob-indep/slides.html#section",
    "title": "Computing Probabilities",
    "section": "",
    "text": "Codecountdown::countdown(2, top = 0)\n\n\n\n−&plus;\n\n02:00\n\n\n\n\n\nThe Houston Astros beat the Philadelphia Phillies in the 2022 World Series. The winners in the World Series have to win a majority of 7 games, so the first team to win 4 games wins the series (best of 7). The Astros were heavily favored to win, so the outcome wasn’t really a suprise. Suppose we assumed that the probability that the Astros would have beaten the Phillies in any single game was estimated at 60%, independently of all the other games. What was the probability that the Astros would have won in a clean sweep?\n(Clean sweep means that they won in the first 4 games - which didn’t happen, they won in 6 games.)\n\n\nStraightforward application of multiplication rule for independent events. probability is 0.6^4 = 0.1296. Follow up comment/question: if the teams were more evenly matched, say probability of either team winning is 50%, the probability of going to 7 games and probability of winning in 6 games is equal given 5 games are played."
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/slides.html#concept-question-5",
    "href": "3-probability/02-cond-prob-indep/slides.html#concept-question-5",
    "title": "Computing Probabilities",
    "section": "Concept Question 5",
    "text": "Concept Question 5\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\n\nSuppose we assume, instead, that the probability that the Astros would have beaten the Phillies in any single game was 50%, independently of all the other games. In this case, was the probability that the series would have gone to 6 games higher than the probability that the series would have gone to 7 games, given that 5 games were played?\n\n\nif 5 games are played it means one of the teams is leading 3-2. Therefore there are two scenarios: the team that is ahead wins game 6 and the series, or the team that is behind wins game 6 and they go to 7 games. Both are equally likely."
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/notes.html",
    "href": "3-probability/02-cond-prob-indep/notes.html",
    "title": "Computing Probabilities",
    "section": "",
    "text": "Sally Clark after her successful appeal\n\nIn November 1999, Sally Clark, an English solicitor, was convicted of murdering her infant sons1. The first, Christopher, had been 11 weeks old when he died, in 1996, and the second, Harry, 8 weeks old, in January 1998, when he was found dead. Christopher was believed to have been a victim of “cot death”, called SIDS (Sudden Infant Death Syndrome) in the US. After her second baby, Harry, also died in his crib, Sally Clark was arrested for murder. The star witness for the prosecution was a well known pediatrician and professor, Sir Roy Meadow, who authored the infamous Meadow’s Law :“One sudden infant death is a tragedy, two is suspicious and three is murder until proved otherwise”2. Unfortunately it was easier to comprehend this “crude aphorism” than make the effort to understand the subtleties of conditional probability. The Royal Statistical Society protested the misuse of statistics in courts, but not early enough to prevent Sally Clark’s conviction. She was eventually acquitted and released, only to die at the age of 42 through alcohol poisoning3 The math presented by Meadow, in brief: Based on various studies, there is a probability of 1 in 8,543 of a baby dying of SIDS in a family such as the Clarks. As the Clarks suffered two deaths, Meadow multiplied 8,543 by 8,543 to arrive at 73 million. He told the jury that the chance or probability that the event of two “cot deaths” was 1 in 73 million. The defense did not employ a statistician to refute her claim, a choice that may have been disastrous for Sally Clark.\nWe will revisit this case at the end of these notes. In order to think about the probabilities involved, we need some more concepts. Let’s go back to one of the examples with die rolls that we have seen, the outcomes that puzzled the seventeenth century gambler, the Chevalier De Méré."
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/notes.html#sally-clark-a-tragic-victim-of-statistical-illiteracy",
    "href": "3-probability/02-cond-prob-indep/notes.html#sally-clark-a-tragic-victim-of-statistical-illiteracy",
    "title": "Computing Probabilities",
    "section": "",
    "text": "Sally Clark after her successful appeal\n\nIn November 1999, Sally Clark, an English solicitor, was convicted of murdering her infant sons1. The first, Christopher, had been 11 weeks old when he died, in 1996, and the second, Harry, 8 weeks old, in January 1998, when he was found dead. Christopher was believed to have been a victim of “cot death”, called SIDS (Sudden Infant Death Syndrome) in the US. After her second baby, Harry, also died in his crib, Sally Clark was arrested for murder. The star witness for the prosecution was a well known pediatrician and professor, Sir Roy Meadow, who authored the infamous Meadow’s Law :“One sudden infant death is a tragedy, two is suspicious and three is murder until proved otherwise”2. Unfortunately it was easier to comprehend this “crude aphorism” than make the effort to understand the subtleties of conditional probability. The Royal Statistical Society protested the misuse of statistics in courts, but not early enough to prevent Sally Clark’s conviction. She was eventually acquitted and released, only to die at the age of 42 through alcohol poisoning3 The math presented by Meadow, in brief: Based on various studies, there is a probability of 1 in 8,543 of a baby dying of SIDS in a family such as the Clarks. As the Clarks suffered two deaths, Meadow multiplied 8,543 by 8,543 to arrive at 73 million. He told the jury that the chance or probability that the event of two “cot deaths” was 1 in 73 million. The defense did not employ a statistician to refute her claim, a choice that may have been disastrous for Sally Clark.\nWe will revisit this case at the end of these notes. In order to think about the probabilities involved, we need some more concepts. Let’s go back to one of the examples with die rolls that we have seen, the outcomes that puzzled the seventeenth century gambler, the Chevalier De Méré."
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/notes.html#simulating-de-mérés-dice-games.",
    "href": "3-probability/02-cond-prob-indep/notes.html#simulating-de-mérés-dice-games.",
    "title": "Computing Probabilities",
    "section": "Simulating De Méré’s dice games.",
    "text": "Simulating De Méré’s dice games.\nRecall that De Méré wanted to bet on at least one six in four rolls of a fair die, and also at least one double six in twenty-four rolls of a pair of dice.\nIn earlier notes, you have seen the result of simulating these two games to estimate the probability of De Méré winning his bet. The simulation used the idea of thinking of the probability of an event as the long-run relative frequency, or the proportion of times we observe that particular event (or the outcomes in the event) if we repeat the action that can result in the event over and over again. Let’s do that again - that is, we estimate the probabilities using the long-run proportions. We will simulate playing the game over and over again and count the number of times we see at least one six in four rolls, and similarly for the second game. (De Méré did this by betting many times, and noticed that the number of times he won wasn’t matching the probability he had computed. Long-run relative frequency in real life!)\n\n\nNumber of simulations for game 1 with 4 rolls of a die = 1000\n\n\nThe chance of at least one six in 4 rolls of a die is about 0.514\n\n\nNumber of simulations for game 2 with 24 rolls of a pair of dice = 1000\n\n\nThe chance of at least one double six in 24 rolls of a pair of dice is about 0.487\n\n\nNow the question is how do we figure out this probability without simulations?\nWe know that if two events are mutually exclusive, we can compute the probability of at least one of the events occurring (\\(A\\cup B\\) aka \\(A \\text{ or } B\\)) using the addition rule \\(P(A \\cup B) = P(A) + P(B)\\). We cannot use the addition rule to compute the probabilities that we have simulated above, since rolling a six on the first roll and rolling a six on the second roll (for example) are not mutually exclusive. So how do we compute them? Read on…\nExample: Drawing red and blue tickets from a box\nConsider a box with four tickets in it, two colored red and two blue. Except for their color, they are identical: . Suppose we draw three times at random from this box, with replacement, that is, every time we draw a ticket from the box, we put it back before drawing the next ticket. List all the possible outcomes. What is the probability of seeing exactly 2 red cards among our draws?\n\nCheck your answer\nNote that since each of the cards is equally likely to be drawn, therefore all the sequences of three cards are equally likely. We can count the number of possible outcomes that contain exactly 2 red cards, and divide that number by the number of total possible outcomes to get the probability of drawing exactly 2 red cards:\n There are three outcomes that have exactly two cards, out of a total of 8 possible outcomes, so the probability of exactly two red cards in three draws at random with replacement is 3/8.\nNow suppose we repeat the procedure, but draw without replacement (we don’t put any tickets back). What is the probability of exactly 2 red cards in 3 draws?\n\nCheck your answer\n\n\n\n\nNotice that we have fewer possible outcomes (6 instead of 8, why?), though they are still equally likely. Again, there are 3 outcomes that have exactly 2 red cards, and so the probability of 2 red cards in three draws is now 3/6.\nWhat about the probabilities for the number of red cards in three draws? Write down all the possible values for the number of red cards in three draws from a box with 2 red cards and 2 blue cards, while drawing with replacement, and their corresponding probabilities. Repeat this exercise for the same quantity (number of red cards in three draws from a box with 2 red cards and 2 blue cards), when you draw the tickets without replacement:\n\nCheck your answer\n\n\n\n\n\n\n\n\n\nNumber of reds in 3 draws\nprobability, with replacement\nprobability, without replacement\n\n\n\n0 red tickets\n\\(\\displaystyle \\frac{1}{8}\\)\n\\(0\\)\n\n\n1 red ticket\n\\(\\displaystyle \\frac{3}{8}\\)\n\\(\\displaystyle \\frac{3}{6}\\)\n\n\n2 red tickets\n\\(\\displaystyle \\frac{3}{8}\\)\n\\(\\displaystyle \\frac{3}{6}\\)\n\n\n3 red tickets\n\\(\\displaystyle \\frac{1}{8}\\)\n\\(0\\)\n\n\n\n\n\nWhy are the numbers different? What is going on?\nBelow you see an illustration of what happens to the box when we draw without replacement, with the box at each stage being shown with one less ticket.\n\n\n\n\nWe see that the box reduces after each draw. After two draws, if the first 2 draws are red (as on the left most sequence) you can’t get another red ticket, whereas if you are drawing with replacement, you can keep on drawing red tickets. (Note that the outcomes in the bottom row are not equally likely, since on the left branch of the tree, blue is twice as likely as red to be the second card, so the outcome RB is twice as likely as RR, and the outcome BR on the right branch of the tree is twice as likely as BB.) Before going further, let’s recall what we know about the probabilities of events.\nRules of probability (recap)\n\n\n\\(\\Omega\\) is the set of all possible outcomes.\nWhat is the probability of \\(\\Omega\\)?\nThe probability of \\(\\Omega\\) is 1. It is called the certain event.\n\n\n\nWhen an event has no outcomes in it, it is called the impossible event, and denoted by \\(\\emptyset\\) or \\(\\{\\}\\).\nWhat is the probability of the impossible event?\nThe probability of the impossible event is 0.\n\n\n\nLet \\(A\\) be a collection of outcomes (for example, from the example above, \\(A\\) could be the event of two red tickets in 3 draws with replacement).\nThen the probability of \\(A\\) has to be ______ (fill in the blank with a suitable phrase)\nbetween \\(0\\) and \\(1\\) (inclusive of \\(0\\) and \\(1\\)).\n\n\n\nIf \\(A\\) and \\(B\\) are two events with no outcomes in common,\nthen they are called ______. (fill in the blanks with suitable phrases)\nmutually exclusive\n\n\nIf \\(A\\) and \\(B\\) have no outcomes in common, that is, \\(A \\cap B =  \\{\\}\\), then \\(P(A \\cup B) = P(A) + P(B)\\).\nConsider an event \\(A\\). The complement of \\(A\\) is not \\(A\\), and denoted by \\(A^C\\). The complement of \\(A\\) consists of all outcomes in \\(\\Omega\\) that are not in \\(A\\) and \\(P(A^C) = 1-P(A)\\). (Why?)"
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/notes.html#conditional-probabilities",
    "href": "3-probability/02-cond-prob-indep/notes.html#conditional-probabilities",
    "title": "Computing Probabilities",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\nIn the first example above, we saw that the probability of a red ticket on a draw changes if we sample without replacement. If we get a blue ticket on the first draw, the probability of a red ticket on the second draw is 2/3 (since there are 3 tickets left, of which 2 are blue). If we get a red ticket on the first draw, the probability of a red ticket on the second draw is 1/3. These probabilities, that depend on what has happened on the first draw, are called conditional probabilities. If \\(A\\) is the event of a blue ticket on the first draw, and \\(B\\) is the event of a red ticket on the second draw, we say that the probability of \\(B\\) given \\(A\\) is 2/3, which is a conditional probability, because we put a condition on the first card, that it had to be blue.\nWhat about if we don’t put a condition on the first card? What is the probability that the second card is red?\n\nCheck your answer\n\nThe probability that the second card drawn is red is 1/2, if we don’t have any information about the first card drawn. To see this, it is easier to imagine that we can shuffle all the cards in the box and they are put in some random order in which each of the 4 positions is equally likely. There are 2 red cards, so the probability that a red card will occupy any of the 4 positions, including the second, is 2/4.\nThis kind of probability, where we put no condition on the first card, is called an unconditional probability - we don’t have any information about the first card."
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/notes.html#the-multiplication-rule-computing-the-probability-of-an-intersection",
    "href": "3-probability/02-cond-prob-indep/notes.html#the-multiplication-rule-computing-the-probability-of-an-intersection",
    "title": "Computing Probabilities",
    "section": "The Multiplication Rule: computing the probability of an intersection",
    "text": "The Multiplication Rule: computing the probability of an intersection\nWe often want to know the probability that two (or more) events will both happen: What is the probability if we roll a pair of dice, that both will show six spots; or if we deal two cards from a standard 52 card deck, that both would be kings, or in a family with two babies, both would suffer SIDS. What do we know? We can draw a Venn diagram to represent intersecting events:\n\n\n\n\nThis picture tells us that \\(A\\cap B\\) (the purple shaded part) is inside both \\(A\\) and \\(B\\), so its probability should be less than each of \\(P(A)\\) and \\(P(B)\\): \\(P(A\\cap B) \\le P(A), P(B)\\). In fact, we write the probability of the intersection as:\n\\[P(A \\cap B) = P(A) \\times P(B|A)\\] We read the second probability on the right-hand side of the equation as the conditional probability of \\(B\\) given \\(A\\). Note that \\(B\\vert A\\) is not an event, but we use \\(P(B\\vert A)\\) as a shorthand for the conditional probability of \\(B\\) given \\(A\\).\nFor example, in the example with the box with two red and two blue cards, let \\(A\\) is the event of drawing a red card on the first draw, and \\(B\\) is the event of drawing a blue card on the second draw. If we draw two cards without replacement, then we have that \\(P(A) = \\displaystyle \\frac{2}{4}\\), \\(P(B | A) =  \\displaystyle \\frac{2}{3}\\) (the denominator reduces by one, since there are only 3 cards left in the box, of which 2 are blue). Therefore:\n\\[P(A \\cap B) = P(A) \\times P(B|A) = \\frac{2}{4} \\times \\frac{2}{3} = \\frac{1}{3}\\] This becomes more clear if we think about the long run frequencies. We draw a red card first about half the time in the long run (if we think about drawing a card over and over again). Of those times, we would also draw a blue card second about two-thirds of the time, since a drawing a blue card would be twice as likely as drawing a red card. Therefore drawing a red card first and then a blue card would happen two thirds of one half of the time, which is about a third of the time.\nNote that the roles of \\(A\\) and \\(B\\) could be reversed in the expression above:\n\\[ P(A \\cap B) = P(A) \\times P(B | A) = P(B) \\times P(A | B)\\] This gives us a way to define the conditional probability, as long as we are not dividing by \\(0\\):\n\n\nConditional probability of \\(B\\) given \\(A\\)\n\nThis is defined to be the probability of the intersection of \\(A\\) and \\(B\\), normalized by dividing by \\(P(A)\\):\n\n\n\\[  P(B | A) = \\frac{ P(A \\cap B)}{P(A)} \\]\n\nThe idea here is that we know that \\(A\\) happened, therefore the only outcomes we are concerned about are the ones in \\(A\\) - this is our new outcome space. We compute the relative size of the part of \\(B\\) that happen (\\(A\\cap B\\)) to the size of the new outcome space.\n\n\n\n\n\n\nImportant\n\n\n\nNote that \\(P(A | B)\\) and \\(P(B | A)\\) can be very different. Consider the scenario shown in the Venn diagram here:"
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/notes.html#independence",
    "href": "3-probability/02-cond-prob-indep/notes.html#independence",
    "title": "Computing Probabilities",
    "section": "Independence",
    "text": "Independence\n\n\nIndependent events\n\nWe say that two events are independent if the probabilities for the second event remain the same even if you know that the first event has happened, no matter how the first event turns out. Otherwise, the events are said to be dependent.\n\n\n\nIf \\(A\\) and \\(B\\) are independent, \\(P(B\\vert A) = P(B)\\).\nConsequently, the multiplication rule reduces to:\n\\[ P(A \\cap B) = P(A) \\times P(B | A) = P(A) \\times P(B) \\]\nUsually the fastest and most convenient way to check if two events are independent is to see if the product of their probabilities is the same as the probability of their intersection.\n\nComputational check for independence Check if \\(P(A \\cap B) = P(A)\\times P(B)\\)\n\nFor example, consider our box of red and blue tickets. When we draw with replacement, the probability of a red ticket on the second draw given a blue ticket on the first draw remains at 1/2. If we had a red ticket on the first draw, the probability of the second ticket being red is still 1/2. The probability doesn’t change because it does not depend on the outcome of the first draw, since we put the ticket back.\nIf we draw the tickets without replacement, we have seen that the probabilities of draws change. The probability of a blue ticket on the second draw given a red ticket on the first draw is 2/3, but the probability of a red ticket on the second draw given a red ticket on the first is 1/3.\nThe lesson here is that when we draw tickets with replacement, the draws are independent - the outcome of the first draw does not affect the second. If we draw tickets without replacement, the draws are dependent. The outcome of the first draw changes the probabilities of the tickets for the second draw.\nExample: Selecting 2 people out of a group of 5\n(drawing without replacement)\nWe have a group of 5 people: Alex, Emi, Fred, Max, and Nan. Two of the five are to be selected at random to form a two person committee. Represent this situation using draws from a box of tickets.\n\nCheck your answer\nWe only care about who is picked, not the order in which they are picked. For instance, picking Alex first and then Emi results in the same committee as picking first Emi and then Alex.\n\n\n\n\nAll the ten pairs are equally likely. On the first draw, there are 5 tickets to choose from, and on the second there are 4, making \\(5 \\times 4 = 20\\) possible draws of two tickets, drawn from this box, one at a time, without replacement. We have only 10 pairs here because of those 20 pairs, there are only 10 distinct ones. When we count 20 pairs, we are counting Alex \\(+\\) Emi as one pair, and Emi \\(+\\) Alex as another pair.\nWhat is the probability that Alex and Emi will be selected? Guess! (Hint: you have seen all the possible pairs above, and they are equally likely. What will be the probability of any one of them?)\nWe could use the multiplication rule to compute this probability, which is much simpler than writing out all the possible outcomes. The committee can consist of Alex and Emi either if Alex is drawn first and Emi second, or Emi is drawn first and Alex second. The probability that Alex will be drawn first is \\(1/5\\). The conditional probability that Emi will be drawn second given that Alex was drawn is \\(1/4\\) since there are only 4 tickets left in the box. Using the multiplication rule, the probability that Alex will be drawn first and Emi second is \\((1/4) \\times (1/5) = 1/20\\). Similarly, the probability that Emi will be drawn first and Alex second is \\(1/20\\). This means that the probability that Alex and Emi will be selected for the committee is \\(1/20 + 1/20 = 1/10\\).\nExample: Colored and numbered tickets\nI have two boxes that with numbered tickets colored red or blue as shown below.\n\n\n\n\nAre color and number independent or dependent for box 1? What about box 2?\nFor example, is the probability of a ticket marked 1 the same whether the ticket is red or blue?\n\nCheck your answer\nFor box 1, color and number are dependent, since the probability of 3 given that the ticket is red is 1/3, but the probability of 3 given that the ticket is blue is 0 (and similarly for the probability of 4).\nEven though the probability for 1 or 2 given the ticket is red is the same as the probability for 1 or 2 given the ticket is blue, we say that color and number are dependent because of the tickets marked 3 or 4.\nNow you work it out for box 2.\nExample: Tickets with more than one number on them\nNow I have two boxes that with numbered tickets, where each ticket has two numbers on them, as shown. For each box, are the two numbers independent or dependent? For example, if I know that the first number is 1 does it change the probability of the second number being 6 (or the other way around: if I know the second number is 6, does it change the probability of the first number being 1)?\n\n\n\n\n\nCheck your answer\nFor box 1, the first number and second number are independent, as shown below, using 1 and 6 as examples. If we know that the first number is 1, the box reduces as shown. The probability of the second number being 6 does not change for box 1. The probability does change for box 2, increasing from 1/2 to 2/3, since the second number is more likely to be 6 if the first number is 1."
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/notes.html#back-to-sally-clark",
    "href": "3-probability/02-cond-prob-indep/notes.html#back-to-sally-clark",
    "title": "Computing Probabilities",
    "section": "Back to Sally Clark",
    "text": "Back to Sally Clark\nProfessor Roy Meadow claimed that the probability of two of Sally Clark’s sons dying of SIDS was 1 in 73 million. He obtained this number by multiplying 8543 by 8543, using the multiplication rule, treating the two events as independent. The question is, are they really independent? Was a crime really committed? Unfortunately for Sally Clark, two catastrophic errors were committed in her case by the prosecution, and not caught by the defense. (She appealed the decision, and was acquitted and released, but after spending 4 years in prison after being accused of murdering her babies. You can imagine how she was treated.)\nThe first error was in treating the deaths as independent, and the second was in looking at the wrong probability. Let’s look at the first mistake. It turns out that the probability of a second child dying of “cot death” or SIDS is 1/60 given that the first child died of SIDS. This was a massive error, and it turned out that the prosecution suppressed the pathology reports for the second baby, who had a very bad infection and might have died of that. It is also believed that male infants are more likely to suffer cot death.\nThe second error is an example of what is called the Prosecutor’s Fallacy. What is needed is \\(P(\\text{innocence }\\vert \\text{ evidence})\\), but it is often confused with (the much smaller) \\(P(\\text{evidence }\\vert \\text{ innocence})\\). They should have actually compared the probability of innocence given the evidence with the probability of murder given the evidence. These multiple errors ruined many lives. Though Sally Clark was eventually acquitted, helped by the Royal Statistical Society’s evidence, her life was shattered, and she died soon after being released. The moral of this story is to be very careful while multiplying probabilities. You must check to see if the events are actually independent."
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/notes.html#de-mérés-paradox",
    "href": "3-probability/02-cond-prob-indep/notes.html#de-mérés-paradox",
    "title": "Computing Probabilities",
    "section": "De Méré’s paradox",
    "text": "De Méré’s paradox\nLet’s finally compute the probability of rolling at least one six in 4 rolls of a fair six-sided die. This is much easier to compute if we use the complement rule. The complement of at least one six is no sixes in 4 rolls. Each roll is independent of the other rolls because what you roll does not affect the values of future rolls. This means that we can use the multiplication rule to figure out the chance of no sixes in any of the rolls. The chance of no six in any particular roll is \\(5/6\\) (there are five outcomes that are not six).\nThe chance of no sixes in any of the 4 rolls is therefore \\(\\displaystyle \\left( \\frac{5}{6}\\right)^4\\) (because the rolls are independent). Using the complement rule, we get that: \\[ P(\\text{at least one six in 4 rolls}) = 1 - P(\\text{no sixes in any of the 4 rolls}) = 1 - \\left( \\frac{5}{6}\\right)^4 \\approx 0.518\\]\nSimilarly, the probability of at least 1 double six in 24 rolls of a pair of dice is given by \\[ 1- P(\\text{no double sixes in any of the 24 rolls}) = 1 - \\left( \\frac{35}{36}\\right)^{24} \\approx 0.491\\]\nBy the way, notice that the simulation was pretty accurate!\nBefore we wrap up these notes, let’s review two important things. The first has to do with a very common misconception, confusing mutually exclusive and independent events."
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/notes.html#mutually-exclusive-vs-independent-events",
    "href": "3-probability/02-cond-prob-indep/notes.html#mutually-exclusive-vs-independent-events",
    "title": "Computing Probabilities",
    "section": "Mutually exclusive vs independent events",
    "text": "Mutually exclusive vs independent events\nNote that if two events \\(A\\) and \\(B\\), both with positive probability, are mutually exclusive, they cannot be independent. If \\(P(A \\cap B) = 0\\), but neither \\(P(A) =0\\) nor \\(P(B) = 0\\), then \\(P(A \\cap B) = 0 \\ne P(A)\\times P(B)\\). However, if two events are not independent, that does not mean they are mutually exclusive.\nThe second item generalizes the multiplication rule that we have already seen, by extending it to events that are not mutually exclusive. It is called the “inclusion-exclusion formula”."
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/notes.html#inclusion-exclusion-generalized-addition-rule",
    "href": "3-probability/02-cond-prob-indep/notes.html#inclusion-exclusion-generalized-addition-rule",
    "title": "Computing Probabilities",
    "section": "Inclusion-exclusion (generalized addition rule)",
    "text": "Inclusion-exclusion (generalized addition rule)\nNow that we know how to compute the probability of the intersection of two events, we can compute the probability of the union of two events:\n\n\n\n\n\\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\]\nYou can see that if we just add the probabilities of \\(A\\) and \\(B\\), we double count the overlap. By subtracting it once, we can get the correct probability, and we know how to compute the probability of \\(A\\cap B\\). This is known as the inclusion-exclusion principle."
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/notes.html#summary",
    "href": "3-probability/02-cond-prob-indep/notes.html#summary",
    "title": "Computing Probabilities",
    "section": "Summary",
    "text": "Summary\nIn this lecture, we do a deep dive into computing probabilities. It is well known that people are just not good at estimating probabilities of events, and we saw the tragic example of Sally Clark (who, even more sadly, is not a unique case)4.\nWe defined conditional probability and independence, and the multiplication rule, considering draws at random with and without replacement. We finally computed the probabilities in the dice games from 17th century France by combining the multiplication rule and the complement rule.\nWe noted that independent events are very different from mutually exclusive events, and finally we learned how to compute probabilities of unions of events that may not be mutually exclusive with the inclusion-exclusion or generalized addition rule."
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/notes.html#footnotes",
    "href": "3-probability/02-cond-prob-indep/notes.html#footnotes",
    "title": "Computing Probabilities",
    "section": "Footnotes",
    "text": "Footnotes\n\n(https://www.theguardian.com/uk-news/2021/nov/20/sally-clark-cot-death-mothers-wrongly-jailed)↩︎\nFrom the archives of The Guardian newspaper https://www.theguardian.com/uk/2001/jul/15/johnsweeney.theobserver↩︎\nThe thumbnail image of the headline from the Manchester Evening News and some details of the case are from http://www.inference.org.uk/sallyclark↩︎\nhttps://www.theguardian.com/uk-news/2021/nov/20/sally-clark-cot-death-mothers-wrongly-jailed↩︎"
  },
  {
    "objectID": "3-probability/labs/04-elections/learning-objectives.html",
    "href": "3-probability/labs/04-elections/learning-objectives.html",
    "title": "Stat 20",
    "section": "",
    "text": "Write down a probability model for vote counts.\nReview\n\n\nUnit of observation\n\n\n\nCodelibrary(rvest)\n# Reading in the table from Wikipedia\ntab &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_tallest_buildings\") %&gt;% \n  html_node(\".wikitable\") %&gt;%\n  html_table(fill = TRUE)\n\n\nReferences:\nOriginal iran analysis:\nhttps://www.researchgate.net/publication/45856921_A_first-digit_anomaly_in_the_2009_Iranian_presidential_election\nhttps://arxiv.org/pdf/0906.2789.pdf\nFollow up by walter mebane:\nhttp://websites.umich.edu/~wmebane/note18jun2009.pdf\nGelman’s take:"
  },
  {
    "objectID": "3-probability/labs/04-elections/slides.html",
    "href": "3-probability/labs/04-elections/slides.html",
    "title": "Lab 3: Elections",
    "section": "",
    "text": "Many naturally occurring numerical variables have a recurring pattern in the distribution of the first digit.\n\n. . .\n\nFirst digits of stock prices, populations of cities, and election results have been observed to follow this pattern.\n\nLet \\(X\\) be the first digit of a randomly selected number. \\(X \\sim Benfords()\\) if\n\\[P(X = x) = \\log_{10}\\left(1 + 1/x \\right)\\]"
  },
  {
    "objectID": "3-probability/labs/04-elections/slides.html#benfords-law",
    "href": "3-probability/labs/04-elections/slides.html#benfords-law",
    "title": "Lab 3: Elections",
    "section": "",
    "text": "Many naturally occurring numerical variables have a recurring pattern in the distribution of the first digit.\n\n. . .\n\nFirst digits of stock prices, populations of cities, and election results have been observed to follow this pattern."
  },
  {
    "objectID": "3-probability/labs/04-elections/slides.html#benfords-law-1",
    "href": "3-probability/labs/04-elections/slides.html#benfords-law-1",
    "title": "Lab 3: Elections",
    "section": "",
    "text": "Let \\(X\\) be the first digit of a randomly selected number. \\(X \\sim Benfords()\\) if\n\\[P(X = x) = \\log_{10}\\left(1 + 1/x \\right)\\]"
  },
  {
    "objectID": "3-probability/labs/04-elections/slides.html#iran-election-1",
    "href": "3-probability/labs/04-elections/slides.html#iran-election-1",
    "title": "Lab 3: Elections",
    "section": "2009 Iran Election",
    "text": "2009 Iran Election\n. . .\n\n\nBackground\n\nOngoing public sentiment that previous election was fraudulent\nThe highest voter turnout in Iran’s history\n\nLeading candidates\n\nMahmoud Ahmadinejad: Leader of conservatives and incumbent president.\nMir-Hossein Mousavi: Reformist and former prime minister. Seeking rapid political evolution.\n\n\n\n\n\n\n\n\n\nOutcome\nAhmadinejad won the election with 62.6% of the votes cast, while Mousavi received 33.75% of the votes cast."
  },
  {
    "objectID": "3-probability/labs/04-elections/slides.html#post-election-controversies-and-unrest",
    "href": "3-probability/labs/04-elections/slides.html#post-election-controversies-and-unrest",
    "title": "Lab 3: Elections",
    "section": "Post-election controversies and unrest",
    "text": "Post-election controversies and unrest\n\n\n\n\n\nAllegations of fraud\nPublic protests and unrests\nThe green wave movement, led by Mousavi, against the allegedly fraudulent election and Ahmadinejad’s regime"
  },
  {
    "objectID": "3-probability/labs/04-elections/slides.html#section",
    "href": "3-probability/labs/04-elections/slides.html#section",
    "title": "Lab 3: Elections",
    "section": "",
    "text": "Was the election fraudulent?"
  },
  {
    "objectID": "3-probability/labs/04-elections/slides.html#fraud-detection-using-benfords-law",
    "href": "3-probability/labs/04-elections/slides.html#fraud-detection-using-benfords-law",
    "title": "Lab 3: Elections",
    "section": "Fraud detection using Benford’s Law",
    "text": "Fraud detection using Benford’s Law\n\nA common theory is that in a normally occurring, fair, election, the first digit of the vote counts county-by-county should follow Benford’s Law. If they do not, that might suggest that vote counts have been manually altered.\n\n. . .\n\nThis theory was brought to bear to determine whether the 2009 presidential election in Iran showed irregularities1."
  },
  {
    "objectID": "3-probability/labs/04-elections/slides.html#lab-3",
    "href": "3-probability/labs/04-elections/slides.html#lab-3",
    "title": "Lab 3: Elections",
    "section": "Lab 3",
    "text": "Lab 3\nIn this lab we will:\n\nExamine the Benford’s Law probability distribution\n\n. . .\n\nCompare the first digits of vote counts in the 2009 Iranian election to this distribution\n\n. . .\n\nReach a conclusion on whether the election was fraudulent (or whether the Benford’s Law is a good tool at detecting fraud in the first place)."
  },
  {
    "objectID": "3-probability/labs/04-elections/slides.html#footnotes",
    "href": "3-probability/labs/04-elections/slides.html#footnotes",
    "title": "Lab 3: Elections",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://physicsworld.com/a/benfords-law-and-the-iranian-e/↩︎"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.\nThe source materials for the curriculum for this course and the software that builds this website are available under at:\nhttps://github.com/stat20/course-materials\n\nTesting zone\nThe content below are experimental tests for future functionality.\n\nDistributing assignments via nbgitpuller\nBelow is a test of a new method of distributing assignments. Click the R logo to synchronize your RStudio with the most recent assignments."
  },
  {
    "objectID": "office-hours.html",
    "href": "office-hours.html",
    "title": "Office Hours and Group Tutoring",
    "section": "",
    "text": "Office Hours and Group Tutoring\nPlease stop by introduce yourself! Stat 20 offers two types of session to ask chat about the course in person but outside of class.\nInstructor office hours are useful for to discuss topics that come in the reading or broader questions about statistics and data science. They are also a fine place to ask questions about the assignments you’re working on!\nGroup tutoring (GT) sessions are held once a week. They’re a great place to work on your assignments with fellow students and seek the help of tutors whenever you get stuck. One thing to know: tutors will expect that you have done the reading and generally been attending class; this is not meant to replace class time.\nYou’re welcome to attend whichever sessions work best for your schedule."
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html",
    "href": "6-prediction/05-logistic-regression/slides.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Announcements\nConcept Questions\nBreak\nLab 6.2\nAppendix: Logistic Regression with the penguins dataset"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#agenda",
    "href": "6-prediction/05-logistic-regression/slides.html#agenda",
    "title": "Logistic Regression",
    "section": "",
    "text": "Announcements\nConcept Questions\nBreak\nLab 6.2\nAppendix: Logistic Regression with the penguins dataset"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#announcements",
    "href": "6-prediction/05-logistic-regression/slides.html#announcements",
    "title": "Logistic Regression",
    "section": "Announcements",
    "text": "Announcements\n\nQuiz 4:\n\nMonday in class.\n\nWrong By Design through Logistic Regression\n\n\n\n\n. . .\n\nProblem Sets:\n\nPS 18 (Overfitting) due next Tuesday at 9am\nExtra Practice (Logistic Regression)\n\n\n\n. . .\n\nLab 6:\n\nboth parts due Tuesday at 9am"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#section",
    "href": "6-prediction/05-logistic-regression/slides.html#section",
    "title": "Logistic Regression",
    "section": "",
    "text": "A logistic regression model was fit in an attempt to predict the sex of a penguin \"male\" or \"female\" based on its body mass (grams).\n\nAssuming that no change to the penguins dataset was made, will the model be predicting the probability of the penguin being male or the probability of the penguin being female?\n\n\nCodecountdown::countdown(minutes = 1, top = 0)\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#section-1",
    "href": "6-prediction/05-logistic-regression/slides.html#section-1",
    "title": "Logistic Regression",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(stat20data)\n\n\n\nCodem1 &lt;- glm(sex ~ body_mass_g, data = penguins, family = \"binomial\")\n\n\n\nCodecoef(m1)\n\n (Intercept)  body_mass_g \n-5.162541644  0.001239819 \n\n\n\nWhich of the expressions given in the poll (math or code) will correctly calculate the predicted probability that a penguin that weighs 4000 g is a female? Select all that apply\n\n\nCodecountdown::countdown(1)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nTo answer this one correctly, they’ll need to know (or remember from the RQ) that the reference level is female, so this model is predicting the probability male.\nYou can show that they can either use R as a calculator or use the predict function.\n1/(1 + exp(-(-5.15 + .00124 * 4000))) predict(m1, newdata = data.frame(body_mass_g = 4000), type = “response”)\nThe latter is more precise, which is useful here since p-hat (prob of male) is .449 therefore prob of female is .55.\nAs a bonus, try sketching this function on a scatterplot!\nWhen sketching this, the positive slope means the s-curve goes up and to the right. The negative intercept shifts the whole curve a tiny bit to the left, reflecting the base rate of a couple more males than females (168 vs 165)."
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#section-2",
    "href": "6-prediction/05-logistic-regression/slides.html#section-2",
    "title": "Logistic Regression",
    "section": "",
    "text": "What is the misclassification rate of this model?\n\n\nCodepenguins |&gt;\n  select(sex) |&gt;\n  mutate(p_hat = predict(m1, penguins, type = \"response\"),\n         y_hat = ifelse(p_hat &gt; .5, \"male\", \"female\")) |&gt;\n  group_by(sex, y_hat) |&gt;\n  summarise(n = n())\n\n# A tibble: 4 × 3\n# Groups:   sex [2]\n  sex    y_hat      n\n  &lt;fct&gt;  &lt;chr&gt;  &lt;int&gt;\n1 female female   109\n2 female male      56\n3 male   female    74\n4 male   male      94\n\n\n\nCodecountdown::countdown(minutes = 1, top = 0)\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#section-3",
    "href": "6-prediction/05-logistic-regression/slides.html#section-3",
    "title": "Logistic Regression",
    "section": "",
    "text": "Codem2 &lt;- glm(sex ~ body_mass_g + bill_length_mm, \n          data = penguins, family = \"binomial\")\n\n\n\nCodecoef(m2)\n\n   (Intercept)    body_mass_g bill_length_mm \n   -6.91208086     0.00101530     0.06112808 \n\n\n\nOpen up RStudio and fit the model here in the slides. What are the predicted sexes of these two penguins?\n\nbody mass = 3900 g, bill length = 50\nbody mass = 4100 g, bill length = 35\n\n\n\nCodecountdown::countdown(1)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nYou can have students work on their laptops for this question.\nThis one extends the task of the previous one by having them include another covariate and also to do the thresholding procedure to go from p-hat to y-hat. This doesn’t specify the threshold, so using .5 is probably a good default.\npredict(m2, newdata = data.frame(body_mass_g = 3900, bill_length_mm = 50), type = “response”) # male predict(m2, newdata = data.frame(body_mass_g = 4100, bill_length_mm = 35), type = “response”) # female"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#building-a-predictive-model",
    "href": "6-prediction/05-logistic-regression/slides.html#building-a-predictive-model",
    "title": "Logistic Regression",
    "section": "Building a predictive model",
    "text": "Building a predictive model\n\n\nDecide on the mathematical form of the model: logistic linear regression\n\n. . .\n\n\nSelect a metric that defines the “best” fit: the coefficients in logistic regression are the ones that minimize not the RSS function but a function called log-loss (which we don’t have time to cover)\n\n. . .\n\n\nEstimating the coefficients of the model that are best using the training data: we know how to do this: test + train + glm()!\n\n. . .\n\n\nEvaluating predictive accuracy using a test data set:\\(R^2\\) isn’t relevant here. We need a new metric!"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#example-penguins",
    "href": "6-prediction/05-logistic-regression/slides.html#example-penguins",
    "title": "Logistic Regression",
    "section": "Example: penguins",
    "text": "Example: penguins\n\nCodeset.seed(132)\n\n# randomly sample train/test set split\nset_type &lt;- sample(x = c('train', 'test'), \n                   size = nrow(penguins), \n                   replace = TRUE, \n                   prob = c(0.8, 0.2))"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#example-penguins-1",
    "href": "6-prediction/05-logistic-regression/slides.html#example-penguins-1",
    "title": "Logistic Regression",
    "section": "Example: penguins",
    "text": "Example: penguins\n\nCodeset.seed(132)\n\n# randomly sample train/test set split\nset_type &lt;- sample(x = c('train', 'test'), \n                   size = nrow(penguins), \n                   replace = TRUE, \n                   prob = c(0.8, 0.2))\n\ntrain &lt;- penguins |&gt;\n  filter(set_type == \"train\")\n\ntest &lt;- penguins |&gt;\n  filter(set_type == \"test\")"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#predicting-into-test-set",
    "href": "6-prediction/05-logistic-regression/slides.html#predicting-into-test-set",
    "title": "Logistic Regression",
    "section": "Predicting into test set",
    "text": "Predicting into test set\n\nCodem2 &lt;- glm(sex ~ body_mass_g + bill_length_mm,\n          data = train, family = \"binomial\")\np_hat &lt;- predict(m2, test, type = \"response\")"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#predicting-into-test-set-1",
    "href": "6-prediction/05-logistic-regression/slides.html#predicting-into-test-set-1",
    "title": "Logistic Regression",
    "section": "Predicting into test set",
    "text": "Predicting into test set\n\nCodem2 &lt;- glm(sex ~ body_mass_g + bill_length_mm,\n          data = train, family = \"binomial\")\np_hat &lt;- predict(m2, test, type = \"response\")\n\ntest |&gt;\n  select(sex)\n\n# A tibble: 70 × 1\n   sex   \n   &lt;fct&gt; \n 1 female\n 2 male  \n 3 female\n 4 male  \n 5 male  \n 6 female\n 7 male  \n 8 female\n 9 male  \n10 male  \n# ℹ 60 more rows"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#predicting-into-test-set-2",
    "href": "6-prediction/05-logistic-regression/slides.html#predicting-into-test-set-2",
    "title": "Logistic Regression",
    "section": "Predicting into test set",
    "text": "Predicting into test set\n\nCodem2 &lt;- glm(sex ~ body_mass_g + bill_length_mm,\n          data = train, family = \"binomial\")\np_hat &lt;- predict(m2, test, type = \"response\")\n\ntest |&gt;\n  select(sex) |&gt;\n  mutate(p_hat = p_hat)\n\n# A tibble: 70 × 2\n   sex    p_hat\n   &lt;fct&gt;  &lt;dbl&gt;\n 1 female 0.345\n 2 male   0.566\n 3 female 0.259\n 4 male   0.280\n 5 male   0.365\n 6 female 0.196\n 7 male   0.428\n 8 female 0.220\n 9 male   0.559\n10 male   0.279\n# ℹ 60 more rows"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#predicting-into-test-set-3",
    "href": "6-prediction/05-logistic-regression/slides.html#predicting-into-test-set-3",
    "title": "Logistic Regression",
    "section": "Predicting into test set",
    "text": "Predicting into test set\n\nCodem2 &lt;- glm(sex ~ body_mass_g + bill_length_mm,\n          data = train, family = \"binomial\")\n\ntest |&gt;\n  select(sex) |&gt;\n  mutate(p_hat = predict(m2, test, type = \"response\"),\n         y_hat = ifelse(p_hat &gt; .5, \"male\", \"female\"))\n\n# A tibble: 70 × 3\n   sex    p_hat y_hat \n   &lt;fct&gt;  &lt;dbl&gt; &lt;chr&gt; \n 1 female 0.345 female\n 2 male   0.566 male  \n 3 female 0.259 female\n 4 male   0.280 female\n 5 male   0.365 female\n 6 female 0.196 female\n 7 male   0.428 female\n 8 female 0.220 female\n 9 male   0.559 male  \n10 male   0.279 female\n# ℹ 60 more rows"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#classification-errors",
    "href": "6-prediction/05-logistic-regression/slides.html#classification-errors",
    "title": "Logistic Regression",
    "section": "Classification errors",
    "text": "Classification errors\n\nCodetest |&gt;\n  select(sex) |&gt;\n  mutate(p_hat = p_hat,\n         y_hat = ifelse(p_hat &gt; .5, \"male\", \"female\"),\n         FP = sex == \"female\" & y_hat == \"male\",\n         FN = sex == \"male\" & y_hat == \"female\")\n\n# A tibble: 70 × 5\n   sex    p_hat y_hat  FP    FN   \n   &lt;fct&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;lgl&gt; &lt;lgl&gt;\n 1 female 0.345 female FALSE FALSE\n 2 male   0.566 male   FALSE FALSE\n 3 female 0.259 female FALSE FALSE\n 4 male   0.280 female FALSE TRUE \n 5 male   0.365 female FALSE TRUE \n 6 female 0.196 female FALSE FALSE\n 7 male   0.428 female FALSE TRUE \n 8 female 0.220 female FALSE FALSE\n 9 male   0.559 male   FALSE FALSE\n10 male   0.279 female FALSE TRUE \n# ℹ 60 more rows"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/slides.html#misclassification-rate",
    "href": "6-prediction/05-logistic-regression/slides.html#misclassification-rate",
    "title": "Logistic Regression",
    "section": "Misclassification Rate",
    "text": "Misclassification Rate\n\nCodetest |&gt;\n  select(sex) |&gt;\n  mutate(p_hat = p_hat,\n         y_hat = ifelse(p_hat &gt; .5, \"male\", \"female\")) |&gt;\n  summarize(MCR = mean(sex != y_hat))\n\n# A tibble: 1 × 1\n    MCR\n  &lt;dbl&gt;\n1 0.371"
  },
  {
    "objectID": "6-prediction/02-improving-predictions/slides.html",
    "href": "6-prediction/02-improving-predictions/slides.html",
    "title": "Evaluating and Improving Predictions",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set\nLab"
  },
  {
    "objectID": "6-prediction/02-improving-predictions/slides.html#agenda",
    "href": "6-prediction/02-improving-predictions/slides.html#agenda",
    "title": "Evaluating and Improving Predictions",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set\nLab"
  },
  {
    "objectID": "6-prediction/02-improving-predictions/slides.html#announcements",
    "href": "6-prediction/02-improving-predictions/slides.html#announcements",
    "title": "Evaluating and Improving Predictions",
    "section": "Announcements",
    "text": "Announcements\n\nProblem Sets:\n\nPS 16 (one-side) released Tuesday and due next Tuesday at 9am\nPS 17 (one-side) released today and due next Tuesday at 9am\nExtra Practice released Thursday (non-turn in)\n\n\n\n. . .\n\nLab 5:\n\nLab 5.1 released Tuesday and due next Tuesday at 9am\nLab 5.2 released Thursday and due next Tuesday at 9am\nLab 5 Workshop next Monday"
  },
  {
    "objectID": "6-prediction/02-improving-predictions/slides.html#section",
    "href": "6-prediction/02-improving-predictions/slides.html#section",
    "title": "Evaluating and Improving Predictions",
    "section": "",
    "text": "Which four models will exhibit the highest \\(R^2\\)?\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nThis is a repeat of a previous CQ, but now with a linear model and a different characteristic of interest: R^2. The two highest R^2 values should be clearly B and E. The second two are very difficult to discern based on these plots.\nThis is a good time to mention that for a least squares linear models, R^2 is indeed just the square of the Pearson correlation coefficient. This isn’t true of non-linear models."
  },
  {
    "objectID": "6-prediction/02-improving-predictions/slides.html#section-1",
    "href": "6-prediction/02-improving-predictions/slides.html#section-1",
    "title": "Evaluating and Improving Predictions",
    "section": "",
    "text": "# A tibble: 4 × 5\n  name    hours cuteness food_eaten is_indoor_cat\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;lgl&gt;        \n1 castiel    12      9          175 TRUE         \n2 frank      18     10          200 TRUE         \n3 luna       19      9.5        215 FALSE        \n4 luca       10      8          218 FALSE        \n\n\n\nCodem1 &lt;- lm(formula = hours ~ cuteness + food_eaten + is_indoor_cat, \n         data = cats)\n\n\n\n\n      (Intercept)          cuteness        food_eaten is_indoor_catTRUE \n    -3.800000e+01      6.000000e+00      2.455729e-16     -4.000000e+00 \n\n\n\nHow many hours does the model predict Frank will sleep each day? Write out the linear equation of the model from the model output to help you.\n\n\n\n\n\n−&plus;\n\n03:00"
  },
  {
    "objectID": "6-prediction/02-improving-predictions/slides.html#section-2",
    "href": "6-prediction/02-improving-predictions/slides.html#section-2",
    "title": "Evaluating and Improving Predictions",
    "section": "",
    "text": "Which is the most appropriate non-linear transformation to apply to time_being_pet?\n\n\n\n\n\n\n\n\n\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "6-prediction/01-method-of-least-squares/ps.html",
    "href": "6-prediction/01-method-of-least-squares/ps.html",
    "title": "The Method of Least Squares",
    "section": "",
    "text": "Where applicable, answer the following questions using R code and write the code you used in the space below.\n\nCreate a scatter plot of the relationship between distance traveled and average speed in miles per hour of every flight that left the bay area in 2020 (recall the data set is called flights).\n\n\n\n\n\nIs the problem of predicting average speed by distance traveled a regression problem or a classification problem? How about using distance to predict whether a plane travels over 400 miles per hour?\n\n\n\n\nUse the lm() function to fit a linear model by least squares that predicts the average speed of the flight based on the distance and save it to m1.\n\n\n\nWrite out the equation of the linear model that you have fit.\n\n\n\nImagine that you wanted to predict the average speed for three flights: one in which a plane travels 10 miles, one in which a plane travels 500 miles and another in which the plane travels 5000 miles. Which of these three predictions, if any, do you expect to be accurate, and why? Comment on each of the three potential predictions.\n\n\n\n\n\nRemake your scatter plot but add a line representing your linear model by adding a layer with geom_smooth(method = \"lm\").\n\n\n\n\n\nHow could you modify your model to improve the predictions that it makes?"
  },
  {
    "objectID": "6-prediction/01-method-of-least-squares/notes.html",
    "href": "6-prediction/01-method-of-least-squares/notes.html",
    "title": "The Method of Least Squares",
    "section": "",
    "text": "As we near the end of the semester, we revisit one last time the four different claims that might be made based on the line plot found in the news story1.\nIn this unit use, we aim to make claims like the final one, “The Consumer Price Index will likely rise throughout the summer”. This is a prediction, a claim that uses the structure of the data at hand to predict the value of observations about which we have only partial information. In midsummer, we know the date will be July 15th, that’s the x-coordinate. But what will the y-coordinate be, the Consumer Price Index?\nThe realm of prediction is the subject of intense research activity and commercial investment at the moment. Falling under the terms “machine learning” and “AI”, models for prediction have become very powerful in recent years; they can diagnose diseases, help drive autonomous vehicles, and compose text with eerily human sensibilities. At the core of these complicated models, though, are a few simple ideas that make it all possible."
  },
  {
    "objectID": "6-prediction/01-method-of-least-squares/notes.html#key-concepts-in-prediction",
    "href": "6-prediction/01-method-of-least-squares/notes.html#key-concepts-in-prediction",
    "title": "The Method of Least Squares",
    "section": "Key Concepts in Prediction",
    "text": "Key Concepts in Prediction\nIn making predictions, the most important question is “what do you want to predict?” The answer to that question is called the response variable.\n\nResponse Variable\n\nThe variable that is being predicted. Also called the dependent or outcome variable. Indicated by \\(y\\) or \\(Y\\) when treated as a random variable.\n\n\nThe related question to ask is is, “what data will you use to predict your response?” The answer to that question is…\n\nPredictor Variable(s)\n\nThe variable or variables that used to predict the response. Also called the independent variables or the features. Indicated by \\(x_1, x_2, \\ldots\\) etc.\n\n\nAn analyst working at the Federal Reserve to predict the Consumer Price Index in midsummer would use the CPI as their response variable. Their predictor could be time (the x-coordinate in the plot from the newspaper article) but could also include the federal interest rate and the unemployment rate.\nA second analyst working in another office at the Federal Reserve is tasked with predicting whether or not the economy will be recession within six months; that event (recession or no recession) would be the response variable. To predict this outcome, they might use those same predictors - the interest rate and the unemployment rate - but also predictors like the real GDP, industrial production, and retail sales.\nThe prediction problems being tackled by these two analysts diverge in an important way: the first is trying to predict a numerical variable and the second a categorical variable. This distinction in the Taxonomy of Data defines the two primary classes of models used for prediction.\n\nRegression Model\n\nA statistical model used to predict a numerical response variable.\n\nClassification Model\n\nA statistical model used to predict a categorical response variable.\n\n\nThe past two decades have been a golden age for predictive models, with hundreds of new model types being invented that can address regression tasks or classifications tasks or often both. To take a deep dive into this diverse landscape, take a course in statistical or machine learning. For the purpose of this course, we’ll just take a dip and focus on two models: Least Squares Linear Model for Regression and a Logistic Linear Model for Classification."
  },
  {
    "objectID": "6-prediction/01-method-of-least-squares/notes.html#linear-regression",
    "href": "6-prediction/01-method-of-least-squares/notes.html#linear-regression",
    "title": "The Method of Least Squares",
    "section": "Linear Regression",
    "text": "Linear Regression\nWe first introduced linear regression as a method of explaining a continuous numerical \\(y\\) variable in terms of a linear function of \\(p\\) explanatory terms, \\(x_i\\). \\[ \\hat{y} = b_0 + b_1x_1 + b_2x_2 + \\ldots +b_px_p \\]\nEach of the \\(b_i\\) are called coefficients.\nBefore, we saw linear regression in a very particular context: we used it for its ability to help us describe the structure of the data set at hand. It calculates not just one but two or more summary statistics - the regression coefficients, that tell us about the linear relationships between the variables.\nWe’re now going to revisit to this model for its ability to also make predictions on unseen data. Before we do so, though, you should be sure you’re familiar with these concepts: correlation coefficient, slope, intercept, fitted values, and residuals. Take a moment to skim those notes:\n\nSummarizing Numerical Associations\nMultiple Linear Regression\n\nThe Method of Least Squares\nWhen we presented the equations to calculate the slope and intercept of a least squares linear model in Unit 1, we did so without any explanation of where those equations came from. The remainder of these notes will cast some light on this mystery.\nThe least squares linear model is so-called because it defines a line that has a particular mathematical property: it is the line that has the lowest residual sum of squares of all possible lines.\n\nResidual Sum of Squares (RSS)\n\nFor observations of an explanatory variable \\(y_i\\), a response variable \\(y_i\\), predictions of its value (fitted values) \\(\\hat{y}_i\\), and a data set with \\(n\\) observations, the RSS is \\[ RSS = \\sum_{i = 1}^n \\left(y_i - \\hat{y}_i \\right)^2\\] where \\[ \\hat{y_i} = b_{0} + b_1x_{1,i} + b_2x_{2,i} + \\ldots +b_px_{p,i} \\]\n\n\nPlugging this expression into the first equation gives us\n\\[ RSS = \\sum_{i = 1}^n \\left(y_i - (b_{0} + b_1x_{1,i} + b_2x_{2,i} + \\ldots +b_px_{p,i}  )\\right)^2\\]\nSo how can we use this definition to decide precisely which slope \\(b_0\\) and intercept \\(b_1\\) to use? Please watch the following 19 minute video to find out.\n\nAlgorithmic Methods\nAs we saw in the video, the problem of least squares can be defined as one of optimization. We would like to make \\(RSS\\) as small as possible. The values of the coefficients that do this are said to optimize the \\(RSS\\) equation. Although a direct way to find these values involves taking the partial derivatives of the third equation with respect to each coefficient \\(b_0\\), \\(b_1\\), …, \\(b_p\\), setting the resulting equation equal to 0, and solving for the coefficient, you could think about using a different, perhaps more intuitive method.\nConsider picking a starting value for each of \\(b_0\\) through \\(b_p\\), plugging these into the third equation, and seeing what comes out. Then, try to pick another set of numbers that makes the result of the \\(RSS\\) equation smaller. Provided that you have a concerted method to pick the next set of numbers, you can repeat this process over and over until you find the set of numbers which yields the smallest value for \\(RSS\\). This process can be described as an algorithmic method to find the values of the coefficients \\(b_0\\) through \\(b_p\\).\nThere are many iterative algorithms that accomplish the same task, some better than others. Two examples:\n\n\nNelder-Mead: an older and more general (and generally not as reliable!) algorithm. This is the one we showed you in the video.\n\nGradient Descent: the most-used algorithm currently. Used to fit deep learning models."
  },
  {
    "objectID": "6-prediction/01-method-of-least-squares/notes.html#the-ideas-in-code",
    "href": "6-prediction/01-method-of-least-squares/notes.html#the-ideas-in-code",
    "title": "The Method of Least Squares",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nWriting a function in R\nWe have been using functions that already exist in either the base installation of R or in another package, such as dplyr, ggplot2 or stat20data, but sometimes it can be useful to write our own functions.\nCustom functions in R can be created with the function function() (quite meta) and assigned to an object. The arguments that we would like the function to take go inside the parentheses of function(). The things that we would like the function to do go inside {}.\nHere is a representation of \\(f(x) = (x+.5)^2\\) in R, which we are saving into the object f. Once we run the following code, we’ll have access to f in our environment and can use it.\n\nf &lt;- function(x) {\n  (x + .5)^2\n}\n\n\nf(x = 1.5)\n\n[1] 4\n\n\nNelder-Mead on a simple function\nHere, we plot the function \\(f(x)\\) that we described above over the \\(x\\) values \\([-1,1]\\).\n\n\n\n\n\n\n\n\nWe can see that the minimum value of \\(f(x)\\) lies between \\(-1\\) and \\(1\\). Can we find the value of \\(x\\) that will give us this minimum using Nelder-Mead? We can try, using the optim() function!\noptim()\nThere are two main arguments to modify:\n\nThe function to optimize is passed to fn. In this case, the function we made is called f.\n\nYou provide a starting point for the algorithm with par.\n\nIn this situation, we are trying to find the value of \\(x\\) to minimize \\(f(x)\\), so we will enter a number. We’ll start with \\(x = .5\\), but you can tinker around yourself and try something different. Depending on what you start with, you may come up with something different as a final answer, so feel free to try out different numbers!\nIn a linear regression context, we are trying find the values of the coefficients \\(b_0\\) through \\(b_p\\) to minimize \\(\\hat{y}\\), so we need to input a vector of starting values, one for each coefficient. This can be done using c().\n\n\n\nNelder-Mead is a random algorithm; each time you run it you’ll get a (slightly) different answer. We therefore use set.seed() to make sure the same value is printed out for the purpose of publishing these notes.\n\nset.seed(5)\noptim(par = .5, fn = f)\n\n$par\n[1] -0.4\n\n$value\n[1] 0.01\n\n$counts\nfunction gradient \n      12       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nYou don’t need to understand the entirety of the output given by optim()– just focus on the number under $par. This is the value of \\(x\\) that optim() thinks will give the minimum value of \\(f(x)\\) after. In truth, the correct value is \\(-0.5\\), so we didn’t fare too badly!"
  },
  {
    "objectID": "6-prediction/01-method-of-least-squares/notes.html#summary",
    "href": "6-prediction/01-method-of-least-squares/notes.html#summary",
    "title": "The Method of Least Squares",
    "section": "Summary",
    "text": "Summary\nPrediction is the task of predicting the value of a response variable for an unseen observation using its values of other variables that are known, the predictors. The nature of the response variable determines the nature of the task (and corresponding model): regression concerns the prediction of numerical response variables, classification concerns the prediction of categorical response variables. One such regression model is the least squares linear model, which uses the line that minimizes the residual sum of squares. Finding the value of the coefficients is a task that can be solved directly using calculus (in simple settings like this one) and also with optimization algorithms (in more general settings). We gave an example of using such an algorithm, demonstrating how to write a function in R in the process."
  },
  {
    "objectID": "6-prediction/01-method-of-least-squares/notes.html#footnotes",
    "href": "6-prediction/01-method-of-least-squares/notes.html#footnotes",
    "title": "The Method of Least Squares",
    "section": "Footnotes",
    "text": "Footnotes\n\nSmialek, Jeanna (2022, May 11). Consumer Prices are Still Climbing Rapidly. The New York Times. https://www.nytimes.com/2022/05/11/business/economy/april-2022-cpi.html↩︎"
  },
  {
    "objectID": "6-prediction/03-overfitting/ps.html",
    "href": "6-prediction/03-overfitting/ps.html",
    "title": "Overfitting",
    "section": "",
    "text": "In this problem set you’ll practice fitting and evaluating different predictive models to see if you can detect overfitting. You can load in the data via a link on Ed. For the following questions, fill in the space below with the R code you used.\n\nVisualize the relationship between \\(x\\) and \\(y\\) with a ggplot and comment on what you see, describing the strength, direction, and shape/form of the association.\n\n\n\n\n\nSplit your data into training and testing sets; seventy percent of the data should be allocated to the training set.\n\n\n\n\n\n\nFill out the below table. You will fit a model with a polynomial having the degree specified, and report the testing and training \\(\\text{RMSE}\\) in each case.\n\n\nDegree\nTraining RMSE\nTesting RMSE\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n3\n\n\n\n\n4\n\n\n\n\n5\n\n\n\n\n10\n\n\n\n\n20\n\n\n\n\n25\n\n\n\n\n\nTo help you, here is some code that will calculate training \\(\\text{RMSE}\\) for you, provided you have fit a linear model called m1 and make a training set called train:\n\nCodetrain |&gt;\n  mutate(yhat = predict(object = m1, newdata = _____),\n         resid = _______) |&gt;\n  summarise(MSE = mean(resid^2))\n\n\nYou will need to modify this code slightly to help you find the testing RMSE. Write the modified code in the space below given a linear model m1.\n\nDescribe the pattern in the results you see and explain it."
  },
  {
    "objectID": "6-prediction/03-overfitting/notes.html",
    "href": "6-prediction/03-overfitting/notes.html",
    "title": "Overfitting",
    "section": "",
    "text": "Below is data we collected about the association between number of hours studied and students’ test scores in a math class. Our goal is to predict the exam score from number of hours studied. Both plots below show the same data, but show the predictions from two different predictive models.\nWhich model looks more appropriate: the blue, or the red? More specifically,\nThe blue model seems more reasonable: studying more should steadily increase your score. The predictive model on the right seems like it took the particular data points “too seriously!” This will be an issue if a new set of students from the same class comes along and we want to predict what their exam scores will be based on the amount of hours studied. Let’s use the blue and red models to predict scores from more students from this same class.\nWe see that the blue line is prepared to predict the exam scores well enough for these students–even though the model was not fit using them! The red model, however, does poorly. It is so beholden to the first group of students that it doesn’t know how to manage when the students are even slightly different. In statistics, we say that the red model was overfit."
  },
  {
    "objectID": "6-prediction/03-overfitting/notes.html#overfitting-with-polynomials",
    "href": "6-prediction/03-overfitting/notes.html#overfitting-with-polynomials",
    "title": "Overfitting",
    "section": "Overfitting with polynomials",
    "text": "Overfitting with polynomials\nUsually, overfitting occurs as a result of applying a model that is too complex, like the red one we saw for the math class data above. We created that overfitted predictive model on the right by fitting a polynomial with a high degree. Polynomials are quite powerful models and are capable of creating very complex predictive functions. The higher the polynomial degree, the more complex function it can create.\nLet’s illustrate by fitting polynomial models with progressively higher degrees to the data set above.\n\n\n\n\n\n\n\n\nThe higher the polynomial degree, the closer the prediction function comes to perfectly fitting the data1. Therefore, when it comes to evaluating which model is the best for prediction, we would say the degree seven polynomial is best. Indeed, based on our knowledge so far, it would have the highest \\(R^2\\). The true test is yet to come, though. Let’s measure these three models on how well they predict to the second group of students that weren’t used to fit the model.\n\n\n\n\n\n\n\n\nAs we increase the degree, the polynomial begins to perform worse on this new data as it bends to conform to the original data. For example, we see that for the student having studied around five and a half hours, the fifth degree polynomial does well, but the seven degree polynomial does horribly! To put a cherry on top, the red model we showed you in the beginnng of these notes was a twenty degree polynomial!\nWhat we see is that the higher the degree, the more risk we run of using a model that overfits."
  },
  {
    "objectID": "6-prediction/03-overfitting/notes.html#training-and-testing-sets-a-workflow-to-curb-overfitting",
    "href": "6-prediction/03-overfitting/notes.html#training-and-testing-sets-a-workflow-to-curb-overfitting",
    "title": "Overfitting",
    "section": "Training and testing sets: a workflow to curb overfitting",
    "text": "Training and testing sets: a workflow to curb overfitting\nWhat you should have taken away so far is the following: we should not fit the model (set the \\(b_0, b_1\\) coefficients) and evaluate the model (judge its predictive power) with the same data set!\nWe can further back up this idea quantitatively. The plot below shows the \\(R^2\\) value for math class models fit with different polynomial degrees.\n\n\n\n\n\n\n\n\nThe \\(R^2\\) value goes steadily upwards as the polynomial degree goes up. In fact this is mathematically guaranteed to happen: for a fixed data set the \\(R^2\\) value for a polynomial model with higher degree will always be higher than a polynomial model with lower degree.\nThis should be disconcerting, especially since we earlier saw that the model with the highest \\(R^2\\) did the worst on our unseen data. What you might also notice is that the \\(R^2\\) isn’t increasing by that much between degrees as the degree gets higher. This suggests that adding that additional degree isn’t improving our general predictive power much; it’s just helping the model tailor itself to the specific data we have.\nDoes that mean \\(R^2\\) is not a good metric to evaluate our model? Not necessarily. We can just change our workflow slightly. Instead of thinking in terms of a single data set, we can partition, or split the observations of the data set into two separate sets. We can use one of these data sets to fit the model, and the other to evaluate it.\n\nTraining Set\n\nThe set of observations used to fit a predictive model; i.e. estimate the model coefficients.\n\nTesting Set\n\nThe set of observations used to assess the accuracy of a predictive model. This set is disjoint from the training set.\n\n\nThe partition of a data frame into training and testing sets is illustrated by the diagram below.\n\n\n\n\n\n\ny\nx1\nx2\nx3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe original data frame consists of 10 observations. For each observation we have recorded a response variable, \\(y\\), and three predictors, \\(x_1, x_2\\), and \\(x_3\\). If we do an 80-20 split, then 8 of the rows will randomly be assigned to the training set (in blue). The 2 remaining rows (rows 2 and 6) are assigned to the testing set (in gold).\nSo to recap, our new workflow for predictive modeling involves:\n\nSplitting the data into a training and a testing set\nFitting the model to the training set\nEvaluating the model using the testing set\n\nMore on splitting the data\nAs in the diagram above, a standard partition is to dedicate 80% of the observations to the training set and the remainder to the testing set (a 80-20 split), though this is not a rule which is set in stone. The other question is how best to assign the observations to the two sets. In general, it is best to do this randomly to avoid one set that is categorically different than the other.\nMean square error: another metric for evaluation\nWhile \\(R^2\\) is the most immediate metric to evaluate the predictive quality of a linear regression, it is quite specific to linear modeling. Therefore, data scientists have come up with another, more general metric called mean square error (MSE). Let \\(y_i\\) be observations of the response variable in the testing set, and \\(\\hat{y}_i\\) be your model’s predictions for those observations. Then \\(\\text{MSE}\\) is given by\n\\[ \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y_i})^2\\] You may notice that for a linear regression model, \\(\\text{MSE} = \\frac{1}{n}\\text{RSS}\\).\nA common offshoot is root mean square error (\\(\\text{RMSE}\\)), which you can obtain by taking the square root of \\(\\text{MSE}\\). Much like what standard deviation does for variance, \\(\\text{RMSE}\\) allows you to think above the average error on a regular scale rather than on a squared scale."
  },
  {
    "objectID": "6-prediction/03-overfitting/notes.html#the-ideas-in-code",
    "href": "6-prediction/03-overfitting/notes.html#the-ideas-in-code",
    "title": "Overfitting",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nLet’s shift the subject to mathematics to biology, and illustrate the training and testing approaching to evaluating predictions for the exam scores from a biology class with 200 students using as a predictor the number of hours that they have studied. Let’s visualize these data first.\n\n\n\n\n\n\n\n\nHere we are going to compare two models: a simple linear model versus a 5th degree polynomial, both fit using the method of least squares.\n\n\\(\\textbf{Model 1:} \\quad \\widehat{score} = b_0 + b_1 \\times hours\\)\n\\(\\textbf{Model 2:} \\quad \\widehat{score} = b_0 + b_1 \\times hours + b_2 \\times hours^2 + b_3 \\times hours^3 + b_4 \\times hours^4 + b_5 \\times hours^5\\)\n\nStep 1: Split data\nWe’ll use an 80-20 split, with each observation assigned to its set randomly. There are many ways to do this via code: here is one using functions we’ve seen.\n\nGenerate a vector of \\(n\\) observations (in this case, our data has 200 observations) in which approximately 80 percent of the observations are \"train\" and 20 percent of the observations are \"test\". To do this, we can make use of the sample() function.\n\n\nset.seed(20)\n\ntrain_or_test &lt;- sample(x = c(\"train\", \"test\"), \n                   size = 200, \n                   replace = TRUE, \n                   prob = c(0.8, 0.2))\n\n\nmutate this vector onto our data frame (our data frame here is called biology). Below, you can see which rows in the data frame have been assigned to \"train\" and which have been assigned to \"test\".\n\n\nbiology &lt;- biology |&gt;\n    mutate(set_type = train_or_test)\n\n\n\n# A tibble: 6 × 3\n  hours score set_type\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1  6.30  74.6 test    \n2  6.30  73.4 train   \n3  7.40  76.6 train   \n4  9.97  95.1 test    \n5  9.58  82.4 train   \n6  8.19  84.0 train   \n\n\n\nsplit the data based on whether the observations are in the \"train\" or \"test\" set.\n\n\nbiology_train &lt;- biology |&gt;\n    filter(set_type == \"train\")\n\nbiology_test &lt;- biology |&gt;\n    filter(set_type == \"test\")\n\nStep 2: Fit the model to the training set\nNow fit two models on the training data. We will be using lm(), and for both models, the data argument is given by biology_train.\n\nlm_slr &lt;- lm(score ~ hours, data = biology_train)\nlm_poly &lt;- lm(score ~ poly(hours, degree = 20, raw = T),\n              data = biology_train)\n\n\nWe can evaluate the \\(R^2\\)’s for both models’ performance on the training data just like before with glance(). Which model do you expect to have a better training set \\(R^2\\) value?\n\nlibrary(broom)\n\nglance(lm_slr) %&gt;%\n    select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.693\n\nglance(lm_poly) %&gt;%\n    select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.715\n\n\nJust as we might have guessed from looking at the model fits, the polynomial model has a better \\(R^2\\) value when evaluated on the training set.\nStep 3: Evaluate the model on the testing set.\nThe real test of predictive power between the two models comes now, when we will make exam score predictions using the testing set: data which the model was not used to fit and hasn’t seen.\nWe will still be using the predict() function for this purpose. Now, we can just plug biology_test into the newdata argument!\n\nscore_pred_linear &lt;- predict(lm_slr, newdata = biology_test)\nscore_pred_poly &lt;- predict(lm_poly, newdata = biology_test)\n\nOnce these predictions \\(\\hat{y}_i\\) are made, we then can use dplyr code to:\n\nmutate on the predictions to our testing data\nset up the \\(R^2\\) formula and calculate2. In the code below, we are using the formula\n\n\\[R^2 = 1-\\frac{\\text{RSS}}{\\text{TSS}} = 1-\\frac{\\sum_{i=1}^n(y_i-\\hat{y_i})^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2}\\]\n\nWe can also calculate \\(\\text{MSE}\\) and \\(\\text{RMSE}\\) as \\(\\frac{1}{n}\\text{RSS}\\) and \\(\\frac{1}{n}\\sqrt{\\text{RSS}}\\), respectively.\n\n\nbiology_test %&gt;%\n    mutate(score_pred_linear = score_pred_linear,\n           score_pred_poly = score_pred_poly,\n           resid_sq_linear = (score - score_pred_linear)^2,\n           resid_sq_poly = (score - score_pred_poly)^2) %&gt;%\n    summarize(TSS = sum((score - mean(score))^2),\n              RSS_linear = sum(resid_sq_linear),\n              RSS_poly = sum(resid_sq_poly),\n              n = n()) %&gt;%\n    mutate(Rsq_linear = 1 - RSS_linear/TSS,\n           Rsq_poly = 1 - RSS_poly/TSS,\n           MSE_linear = RSS_linear/n,\n           MSE_poly = RSS_poly/n,\n           RMSE_linear = sqrt(MSE_linear),\n           RMSE_poly = sqrt(MSE_poly)) |&gt;\n  select(Rsq_linear, Rsq_poly, MSE_linear, MSE_poly)\n\n# A tibble: 1 × 4\n  Rsq_linear Rsq_poly MSE_linear MSE_poly\n       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1      0.664    0.629       26.6     29.3\n\n\nVoila the linear model’s test set \\(R^2\\) is better than the polynomial model’s test \\(R^2\\)! We also see the \\(\\text{MSE}\\) for the linear model is lower than that for the polynomial model.\nSo which is the better predictive model: Model 1 or Model 2? In terms of training, Model 2 came out of top, but Model 1 won out in testing.\nAgain, while training \\(R^2\\) can tell us how well a predictive model explains the structure in the data set upon which it was trained, it is deceptive to use as a metric of true predictive accuracy. The task of prediction is fundamentally one applied to unseen data, so testing \\(R^2\\) is the appropriate metric. Model 1, the simpler model, is the better predictive model. After all, the data we are using looks much better modeled by a line than a five degree polynomial."
  },
  {
    "objectID": "6-prediction/03-overfitting/notes.html#summary",
    "href": "6-prediction/03-overfitting/notes.html#summary",
    "title": "Overfitting",
    "section": "Summary",
    "text": "Summary\nThis lecture is about overfitting: what happens when your model takes the particular data set it was built on too seriously. The more complex a model is, the more prone to overfitting it is. Polynomial models are able to create very complex functions thus high-degree polynomial models can easily overfit. Fitting a model and evaluating it on the same data set can be problematic; if the model is overfitted the evaluation metric (e.g. \\(R^2\\)) might be very good, but the model might be lousy on predictions on new data.\nA better way to approach predictive modeling is to fit the model to a training set then evaluate it with a separate testing set."
  },
  {
    "objectID": "6-prediction/03-overfitting/notes.html#footnotes",
    "href": "6-prediction/03-overfitting/notes.html#footnotes",
    "title": "Overfitting",
    "section": "Footnotes",
    "text": "Footnotes\n\nWe say a function that perfectly predicts each data point interpolates the data. See the first red curve for the math class exams.↩︎\nBecause \\(\\hat{y}_i\\) involve information from the training data and \\(y_i\\) and \\(\\bar{y}\\) come from the testing data, the decomposition of the sum of squares does not work. So, we cannot interpret testing \\(R^2\\) as we would training \\(R^2\\), and you may have a testing \\(R^2\\) less than 0. However, higher \\(R^2\\) values still signal that the model has good predictive power.↩︎"
  },
  {
    "objectID": "6-prediction/labs/07-baseball/lab.html",
    "href": "6-prediction/labs/07-baseball/lab.html",
    "title": "Lab 5: Baseball",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "6-prediction/labs/07-baseball/lab.html#part-i-understanding-the-context-of-the-data",
    "href": "6-prediction/labs/07-baseball/lab.html#part-i-understanding-the-context-of-the-data",
    "title": "Lab 5: Baseball",
    "section": "Part I: Understanding the Context of the Data",
    "text": "Part I: Understanding the Context of the Data\n\nLab 5.1: Baseball"
  },
  {
    "objectID": "6-prediction/labs/07-baseball/lab.html#part-ii-computing-on-the-data",
    "href": "6-prediction/labs/07-baseball/lab.html#part-ii-computing-on-the-data",
    "title": "Lab 5: Baseball",
    "section": "Part II: Computing on the Data",
    "text": "Part II: Computing on the Data\nThe goal of this lab is to build three different regression models to predict the number of wins of a Major League Baseball team.\nUse the following code to load in the Teams dataset from the Lahman database (library). Recall that you can query the help file for a data set by running ?Teams at the console.\n\nSubset the Teams data set to only include years from 2000 to present day (this is the data set that you’ll use for the remainder of this lab. There might be another year post-2000 that you might want to filter out: why?). What are the dimensions of this filtered data set?\nPlot the distribution of wins. Describe the shape of the distribution and compare it to your speculations from part 1 of the lab.\nPlot the relationship between runs and wins. Describe the relationship (form, direction, strength of association, presence of outliers) and compare it to your speculations from part 1 of the lab.\nPlot the relationship between runs allowed and wins. Describe the relationship. How does it compare to the relationship between runs and wins?\nFit a simple linear model to predict wins by runs and call it model_1. Write out the equation for the linear model (using the estimated coefficients) and interpret the \\(R^2\\) in the context of the problem in at least one sentence.\nWhat is the average number of season runs and wins? Based on the previous model, how many games would you predict a team that scored the average number of runs would win?\nWhat about a team that scored 600 runs? What about 850 runs? What about 10,000 runs? Would any of these predictions be inaccurate and why?\nFit a multiple linear regression model to predict wins by runs and runs allowed (RA) and save it as model_2. Write out the equation for the linear model and report the \\(R^2\\). How does this model compare to the simple linear regression from the previous question?\n\nFit a third, more complex model to predict wins and call it model_3. This model should use\n\nat least three variables from this data set,\nat least one non-linear transformation or polynomial term.\n\nWrite out the equation for the resulting linear model and report the \\(R^2\\).\n\nRevisit the definition of causation. If your predictive model has a positive coefficient between one of the predictors and the response, is that evidence that if you increase that predictor variable for a given observation, the response variable will increase? That is, can you (or a sports management team) use this model to draw causal conclusions? Why or why not? Answer in at least three sentences."
  },
  {
    "objectID": "6-prediction/labs/08-cancer/lab-context.html",
    "href": "6-prediction/labs/08-cancer/lab-context.html",
    "title": "Cancer Diagnosis",
    "section": "",
    "text": "In this lab you will train and evaluate a classification algorithm to determine whether or not a fine needle aspiration biopsy is cancerous (malignant) or non-cancerous (benign). The data were downloaded from the UC Irvine Machine Learning Repository and lightly processed. Here is a brief glimpse at some of the columns. Use this glimpse to answer the following questions.\n\n\n\n\ndiagnosis\nradius_mean\narea_mean\nradius_sd\n\n\n\nB\n13.700\n571.1\n0.2431\n\n\nB\n12.720\n501.3\n0.2954\n\n\nB\n11.750\n422.9\n0.4384\n\n\nM\n13.440\n563.0\n0.2385\n\n\nM\n12.450\n477.1\n0.3345\n\n\nM\n19.590\n1214.0\n0.7364\n\n\nB\n12.060\n448.6\n0.1822\n\n\nM\n18.050\n1006.0\n0.9806\n\n\nB\n8.734\n234.3\n0.5169\n\n\nB\n13.210\n537.9\n0.2084\n\n\nM\n15.460\n731.3\n0.3331\n\n\nM\n14.220\n609.9\n0.2860\n\n\nB\n11.500\n407.4\n0.3927\n\n\nM\n14.780\n668.3\n0.3577\n\n\nB\n9.676\n272.5\n0.2744\n\n\nB\n12.580\n489.0\n0.2719\n\n\nB\n9.738\n288.5\n0.1988\n\n\nB\n10.750\n355.3\n0.2525\n\n\nB\n11.060\n366.5\n0.1779\n\n\nB\n12.880\n514.3\n0.2116\n\n\nM\n15.660\n773.5\n1.2920\n\n\nM\n23.090\n1682.0\n1.2910\n\n\nM\n19.450\n1169.0\n0.5959\n\n\n\n\n\n\nWhat is the unit of observation in this data frame?\n\n\n\nWe will be fitting models to output a diagnosis (“benign” or “malignant”). This is a categorical outcome. Which level will be considered the reference level by default in R and why?\n\n\n\n\n\nIf you were to deploy your method in a clinical setting to help diagnose cancer, would it be worse to misclassify a benign case or to misclassify a malignant case? Explain your rationale in at least two sentences.\n\n\n\nBased on the glimpse, use a plot to compare the radius_mean for benign vs. malignant biopsies, side-by-side. Make sure to give your label your axes and give your plot a title. Give a shape which matches your expectation of the phenomenon and explain your choice in at least one sentence.\n\n\n\n\nBased on your previous sketch, what biopsies are you prepared to classify as malignant versus benign? Fill in the blanks below to make a decision rule.\nIf radius_mean &gt; ________: predict ________\n  Otherwise predict ________\n\nModify the side-by-side plot you made earlier to visually represent the decision rule.\n\n\n\nBased on the glimpse, sketch a plot that examines the association between two predictors, radius_mean and area_mean. Make sure to give your label your axes and give your plot a title. Give a shape which matches your expectation of the phenomenon and explain your choice in at least one sentence.\n\n\n\n\nIn many realms of medicine, classification algorithms can be more accurate than the most well-trained medical doctors. What is gained and what is lost by shifting to algorithmic diagnoses? Although a book could be written about this topic, please answer in one paragraph."
  },
  {
    "objectID": "6-prediction/labs/08-cancer/slides.html",
    "href": "6-prediction/labs/08-cancer/slides.html",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "",
    "text": "Often when someone is suspected to have cancer (e.g. a bump is found) a fine needle aspiration biopsy is taken to determine whether or not the growth is cancerous (malignant – may grow dangerously out of control) or not (benign)."
  },
  {
    "objectID": "6-prediction/labs/08-cancer/slides.html#fine-needle-aspiration-biopsy",
    "href": "6-prediction/labs/08-cancer/slides.html#fine-needle-aspiration-biopsy",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "",
    "text": "Often when someone is suspected to have cancer (e.g. a bump is found) a fine needle aspiration biopsy is taken to determine whether or not the growth is cancerous (malignant – may grow dangerously out of control) or not (benign)."
  },
  {
    "objectID": "6-prediction/labs/08-cancer/slides.html#artificial-intelligence-in-medicine",
    "href": "6-prediction/labs/08-cancer/slides.html#artificial-intelligence-in-medicine",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "Artificial intelligence in medicine",
    "text": "Artificial intelligence in medicine\n\nAutomating certain diagnostic tasks can increase access to healthcare\n\nGlobal shortage of pathologists, especially outside of wealthy healthcare systems\n\nExpert pathologists take years to be fully trained (4 year medical school + 4 year residency)\n\n\n\n\nhttps://www.linkedin.com/pulse/how-ai-can-help-address-global-shortage-pathologists-colangelo/"
  },
  {
    "objectID": "6-prediction/labs/08-cancer/slides.html#lab-6-breast-cancer-diagnosis",
    "href": "6-prediction/labs/08-cancer/slides.html#lab-6-breast-cancer-diagnosis",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "Lab 6: breast cancer diagnosis",
    "text": "Lab 6: breast cancer diagnosis\n\nSamples are 568 biopsies\n\nEach biopsy has 30 features\n\n\nGoal: classify biopsy as benign or malignant\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe tissue is purple because it is stained with Hematoxylin and Eosin. These are purple and pink stains that make the tissue structure easier to see under a microscope.\nMost of the visible objects in these images are cell nuclei, not the full cell; the cytoplasm is mostly invisible on these images.\n(From https://stanfordhealthcare.org/medical-conditions/cancer/cancer.html): Tumors can be benign (noncancerous) or malignant (cancerous). Benign tumors tend to grow slowly and do not spread. Malignant tumors can grow rapidly, invade and destroy nearby normal tissues, and spread throughout the body."
  },
  {
    "objectID": "6-prediction/labs/08-cancer/slides.html#nuclear-morphology",
    "href": "6-prediction/labs/08-cancer/slides.html#nuclear-morphology",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "Nuclear morphology",
    "text": "Nuclear morphology\n\nMorphology = what the cell looks like under a microscope\n\nsize, shape, texture\n\n\nCells in malignant biopsies tend to\n\nbe larger\nirregularly shaped\nhighly variable\n\n\nOnly measure morphology of cell nucleus"
  },
  {
    "objectID": "6-prediction/labs/08-cancer/slides.html#nuclear-morphology-features",
    "href": "6-prediction/labs/08-cancer/slides.html#nuclear-morphology-features",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "10 nuclear morphology features",
    "text": "10 nuclear morphology features\n\n\n\n\n\nWe compute 10 morphological features for each cell nucleus in the biopsy image.\nIt may be helpful to first draw one unit (one glass slide, that you would see under the microscope). Then in this slide, you make a data frame with cells, and then group by-summarise. Then the resulting calculations become one row in the final dataset."
  },
  {
    "objectID": "6-prediction/labs/08-cancer/slides.html#biopsy-features",
    "href": "6-prediction/labs/08-cancer/slides.html#biopsy-features",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "30 biopsy features",
    "text": "30 biopsy features\n\n\n\n\n\nEach biopsy has 30 features; these come from computing 3 summary statistics (mean, max, standard deviation) of each of the 10 cell features to summarize the population of cells in the biopsy."
  },
  {
    "objectID": "6-prediction/labs/08-cancer/slides.html#lab-worktime",
    "href": "6-prediction/labs/08-cancer/slides.html#lab-worktime",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "Lab worktime",
    "text": "Lab worktime\n\nCodecountdown::countdown(25)\n\n\n\n−&plus;\n\n25:00"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "Introductions\nThe Data Science Lifecycle\nTypes of Claims with Practice\nCourse Structure and Syllabus\nIntro to R and RStudio\nLooking forward"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#agenda",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#agenda",
    "title": "Understanding the World with Data",
    "section": "Agenda",
    "text": "Agenda\n\nIntroductions\nThe Data Science Lifecycle\nTypes of Claims with Practice\nCourse Structure and Syllabus\nIntro to R and RStudio\nLooking forward"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "What’s going on here?"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-2",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-2",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "As a group, formulate at least three possible explanations for what’s going on in the picture.\n\nEnter them at pollev.com or upvote existing explanations if they are very similar to your own.\n\n\n\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-3",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-3",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "Does this image change which claims are more or less likely?\nUp and down vote explanations at pollev.com.  /"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-4",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-4",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "These three photos were taken in quick succession by a physician and amateur photographer who was vising the San Diego Zoo Safari Park. He was watching the shoebill as it was walking down a path in the reeds. As the shoebill was ambling along, it encountered a duck in the middle of the path. It leaned down, picked up the duck in its beak, turned to the side, dropped the duck in the reeds, then proceeded to amble down the path."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-5",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-5",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "Understand\nthe World\n\n\n\nData"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-6",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-6",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "Understand\nthe World\n\n\n\nData"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#takeaways-from-this-exercise",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#takeaways-from-this-exercise",
    "title": "Understanding the World with Data",
    "section": "Takeaways from this exercise",
    "text": "Takeaways from this exercise\nWe can call the process of:\n\nhaving a question,\n\n\n\nfinding data to investigate that question,\n\n\n\n\nreaching a conclusion,\n\n\n\n\nand then thinking of a next step which starts everything over again\n\n\n\n\nthe data science lifecycle.\n\nThis lifecycle involves constructing and critiquing claims made using data: which is the main goal of our course!"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#course-goal",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#course-goal",
    "title": "Understanding the World with Data",
    "section": "Course Goal",
    "text": "Course Goal\n\n\nTo learn to critique and construct\nclaims made using data."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-7",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-7",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "To learn to critique and construct\nclaims made using data."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-8",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-8",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "To learn to critique and construct\nclaims made using data."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-9",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-9",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "To learn to critique and construct\nclaims made using data."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-10",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-10",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "To learn to critique and construct\nclaims made using data."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-11",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-11",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "To learn to critique and construct\nclaims made using data."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-12",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-12",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "To learn to critique and construct\nclaims made using data."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-13",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-13",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "A numerical, graphical, or verbal description of an aspect of data that is on hand.\n\n\n\n\n\n\n\nExample\nUsing data from the Stat 20 class survey, the proportion of respondents to the survey who reported having no experience writing computer code is 70%."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-14",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-14",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "A numerical, graphical, or verbal description of a broader set of units than those on which data was been recorded.\n\n\n\n\n\n\n\n\nExample\nUsing data from the Stat 20 class survey, the proportion of Berkeley students who have no experience writing computer code is 70%."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-15",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-15",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "A claim that changing the value of one variable will influence the value of another variable.\n\n\n\n\n\n\n\nExample\nData from a randomized controlled experiment shows that taking a new antibiotic eliminates more than 99% of bacterial infections."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-16",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-16",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "A guess about the value of an unknown variable, based on other known variables.\n\n\n\n\n\n\n\nExample\nBased on reading the news and the price of Uber’s stock today, I predict that Uber’s stock price will go up 1.2% tomorrow."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#practice-concept-questions-1",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#practice-concept-questions-1",
    "title": "Understanding the World with Data",
    "section": "Practice Concept Questions",
    "text": "Practice Concept Questions\nWe will now re-examine a few pathways in the data science lifecycle:\n\nForming a question -&gt; collecting data\n\n\n\nCollecting data -&gt; making a claim\n\n\n\n\nNow, visit the PollEverywhere site provided by your instructor to answer the following questions."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#from-questions-to-data",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#from-questions-to-data",
    "title": "Understanding the World with Data",
    "section": "From Questions to Data",
    "text": "From Questions to Data\n\n\nIs the incidence of COVID on campus going up or down?\n\n\n\n\nWill this question be answered by a summary, a prediction, a generalization, or a causal claim?\n\n\n\n\n\nAlso discuss: what type of data can help answer this question? Consider:\n\nWhich different people / institutions collect relevant data\nIs certain data not available? Why not?\n\n\n\n\n−+\n06:00"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#from-data-to-claims",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#from-data-to-claims",
    "title": "Understanding the World with Data",
    "section": "From Data to Claims",
    "text": "From Data to Claims\nOne source of data:\n\n\n\n\n\n“The following dashboard provides information on COVID-19 testing performed at University Health Services or through the PCR Home Test Vending Machines on campus. It does not capture self-reported positive tests. It provides a look at new cases and trends, at a glance.”"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-17",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-17",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "Formulate one claim that is supported by this data1.\n\n\n\n\n−+\n03:00\n\n\n\nThe positivity rate is the number of positive tests over the total number of tests."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-23",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-23",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "Read lecture notes\nWork through reading questions\n\n\n\n\n\nWork through concept questions solo / in groups / as a class\nMake progress on assignments"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#section-24",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#section-24",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "All of the materials and links for the course can be found at:\nstat20.berkeley.edu/summer-2024"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#ed-discussion-forum",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#ed-discussion-forum",
    "title": "Understanding the World with Data",
    "section": "Ed Discussion Forum",
    "text": "Ed Discussion Forum\n\n\n\n\n\nForum to ask questions, answer questions, and course announcements\nPlease answer each other’s questions!\n\n\nPractice by asking/answering a question on the “Syllabus Discussion” thread on Ed via the link at the top right of https://stat20.berkeley.edu/summer-2024."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#components-of-rstudio",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#components-of-rstudio",
    "title": "Understanding the World with Data",
    "section": "Components of RStudio",
    "text": "Components of RStudio\n\nConsole\nEnvironment\nEditor\nFile Directory\n\n\nNow we are going to switch over to RStudio to understand these 4 components a bit better."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#components-of-rstudio-1",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#components-of-rstudio-1",
    "title": "Understanding the World with Data",
    "section": "Components of RStudio",
    "text": "Components of RStudio\n\nConsole: Where the live R session lives. Type commands into the prompt &gt; and press enter/return to run them. The Console is in the lower-left pane.\nEnvironment: The space that keeps track of all of the data and objects that you have created or loaded and have access to. Found in the upper right pane.\nEditor: Used to compose and edit text (.qmd files) and R code (.r files). Found in the upper left pane.\nFile Directory: Used to navigate between your files/folders on your Rstudio account. Can move, copy, rename, delete, etc. Found in the lower right pane."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#r-as-a-calculator",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#r-as-a-calculator",
    "title": "Understanding the World with Data",
    "section": "R as a calculator",
    "text": "R as a calculator\nR allows all of the standard arithmetic operations.\n\n\nAddition\n\n1 + 2\n\n[1] 3\n\n\nSubtraction\n\n1 - 2\n\n[1] -1\n\n\n\nMultiplication\n\n1 * 2 \n\n[1] 2\n\n\nDivision\n\n1 / 2\n\n[1] 0.5"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#r-as-a-calculator-cont.",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#r-as-a-calculator-cont.",
    "title": "Understanding the World with Data",
    "section": "R as a calculator, cont.",
    "text": "R as a calculator, cont.\nR allows all of the standard arithmetic operations.\n\n\nExponents\n\n2 ^ 3\n\n[1] 8\n\n\n\nParentheses for Order of Ops.\n\n2 ^ 3 + 1\n\n[1] 9\n\n\n\n2 ^ (3 + 1)\n\n[1] 16"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#your-turn",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#your-turn",
    "title": "Understanding the World with Data",
    "section": "Your turn",
    "text": "Your turn\n\nWhat is three times one point two raised to the quantity thirteen divided six?\n\n\n\n\n−+\n01:00"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#looking-forward",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#looking-forward",
    "title": "Understanding the World with Data",
    "section": "Looking forward",
    "text": "Looking forward\n\n\n\nRead the lecture notes for Summarizing Categorical Data; Summarizing Numerical Data.\nIf you have any questions, you may leave a comment/question on the dedicated threads on Ed.\nAnswer the Reading Questions for both of these readings on Gradescope by 12 pm on Thursday.\nLab: Getting Started and Problem Set 1 will be due next Monday at 12pmm!\n\n\n\n\n\n\n\n\n\nReadings will be posted Weds night"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/slides.html#footnotes",
    "href": "1-questions-and-data/01-understanding-the-world/slides.html#footnotes",
    "title": "Understanding the World with Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe positivity rate is the number of positive tests over the total number of tests.↩︎"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/extra-practice.html",
    "href": "1-questions-and-data/01-understanding-the-world/extra-practice.html",
    "title": "Understanding the World With Data",
    "section": "",
    "text": "For each of the following scenarios, identify which type(s) of claim is being made: a Summary, a Generalization, a Causal Claim, or a Prediction. Some scenarios may have more than one.\n\n\nNASA has published a data set containing total volume of each of the 8 planets of our solar system (Mercury is the smallest, Jupiter is the largest) and whether or not they are gaseous or rocky. Based on this data, an astronomer writes that, on average, gaseous planets are larger than rocky planets.\n\n\n\n\n\n\n\n\n\n\nA biology professor created two versions of a midterm, version A (all multiple choice) and version B (a mix of multiple choice and short answer), and passed them out to students randomly. After grading the exams, they consider a data set of the score and exam version (A or B) of all 100 students in the class, on seeing that the average score on version A is higher than on version B, conclude that short answer questions are more challenging than multiple choice questions.\n\n\n\n\n\n\n\n\n\n\nThe group Morning Consult conducted a poll of a representative sample of registered 1,256 Republican voters on August 24, 2023, asking them who they planned to vote for in the primary election. 58% of respondents replied “Trump” and 14% replied “DeSantis”. A major news outlet ran a headline, “Trump leads DeSantis by 44 points among registered Republican voters.\n\n\n\n\n\n\n\n\n\n\nAstronomers detect a new planet just beyond our solar system. Sensors aboard the James Webb Space Telescope determine its volume and, using the NASA data described above, determines that the planet is likely rocky.\n\n\n\n\n\n\n\n\n\n\n\nA 2020 study conducted in Sydney, Australia wanted to characterize the working arrangements of medical research scientists and support staff in Australia during the COVID‐19 pandemic, and to evaluate factors (in particular: wearing pyjamas) that influence the self‐assessed productivity and mental health of medical institute staff working from home. A newspaper article opened its article on the study with “Academics who are tempted to remain in pajamas during the working day should think again, says an Australian study that has linked the practice to a deterioration in mental health.”\n\n\n\n\n\n\n\n\n\n\n\nResilience is the ability to bounce back from adversity and stress, while the A1C test — also known as the hemoglobin A1C or HbA1c test — is a simple blood test that measures your average blood sugar levels over the past 3 months. A study published in July 2018 in Diabetes Care, that involved more than 470 adolescents ages 10 to 19 with type 1 diabetes, found that strengths in managing diabetes, such as confidence in care or support from others, were associated with resilience. This resilience was linked with a lower A1C.\n\n\n\n\n\n\n\n\n\n\n\nEli Lilly is a pharmaceutical company that is constantly on the hunt for new and effective drugs. They maintain an internal website called the Discovery Dashboard that allows researchers within the company to, at a glance, get a sense of the progress of various clinic trials. Pointing to a time-series line plot, a researcher exclaims, “Wow, I didn’t realize how many phase II trials we had running back in January!”\n\n\n\n\n\n\n\n\n\n\nAn online auction platform is rolling out a new feature called “Price Genie”. When a seller inputs all of the information about the object they wish to sell, the site generates a dollar amount that is their suggested price that the seller start the bidding at. This dollar amount is calculated from data on past sales of similar items."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html",
    "title": "Taxonomy of Data",
    "section": "",
    "text": "Announcements\nConcept Questions: Conceptual\nProblem Set 1: Taxonomy of Data\n\nBreak\nConcept Questions: Coding\nLab 0 work time"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#agenda",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#agenda",
    "title": "Taxonomy of Data",
    "section": "Agenda",
    "text": "Agenda\n\nTaxonomy of Data lecture\n\nData Types\nData Frames\n\nProblem Set 1 and Lab work time\nAppendix (extra material!)"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#announcements",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#announcements",
    "title": "Taxonomy of Data",
    "section": "Announcements",
    "text": "Announcements\n\nProblem Set 1 and Lab 0 are both due Tuesday, January 23rd at 9am on Gradescope\n\n\n\nRQ: Summarizing Categorical Data due Monday, January 22nd at 11:59pm on Gradescope\n\n\n\n\nPlease read the Ed expectations post here before posting on Ed!"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#concept-question-1---quick-refresher",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#concept-question-1---quick-refresher",
    "title": "Taxonomy of Data",
    "section": "Concept Question 1 - Quick Refresher",
    "text": "Concept Question 1 - Quick Refresher\n\nHead to PollEverywhere for a quick set of questions regarding Taxonomy of Data!\n\n\n\n\n\n−+\n01:00"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#images-as-data",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#images-as-data",
    "title": "Taxonomy of Data",
    "section": "Images as data",
    "text": "Images as data\n\n\n\nImages are composed of pixels (this image is 1520 by 1012)\nThe color in each pixel is in RGB\n\nEach band takes a value from 0-255\nThis image is data with 1520 x 1012 x 3 values."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#grayscale",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#grayscale",
    "title": "Taxonomy of Data",
    "section": "Grayscale",
    "text": "Grayscale\n\n\n\nGrayscale images have only one band\n0 is black, 255 is white\nThis image is data with 1520 x 1012 x 1 values."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#grayscale-1",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#grayscale-1",
    "title": "Taxonomy of Data",
    "section": "Grayscale",
    "text": "Grayscale\n\n\nTo simplify, assume our photos are 8 x 8 grayscale images."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#images-in-a-data-frame",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#images-in-a-data-frame",
    "title": "Taxonomy of Data",
    "section": "Images in a Data Frame",
    "text": "Images in a Data Frame\nConsider the following images which are our data:\n\n\n\n\n\n\n\n\n\nLet’s simplify them to 8 x 8 grayscale images"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#images-in-a-data-frame-1",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#images-in-a-data-frame-1",
    "title": "Taxonomy of Data",
    "section": "Images in a Data Frame",
    "text": "Images in a Data Frame\n\n\n\n\n\n\n\n\n\n\nIf you were to put the data from these (8 x 8 grayscale) images into a data frame, what would the dimensions of that data frame be in rows x columns? Answer at pollev.com.\n\n\n\n\n\n−+\n01:00"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#a-note-on-variables",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#a-note-on-variables",
    "title": "Taxonomy of Data",
    "section": "A note on variables",
    "text": "A note on variables\nThere are three things that “variable” could be referring to:\n\n\na phenomenon\nhow the phenomenon is being recorded or measured into data\n\nwhat values can it take? (this is often an intent- or value-laden exercise!)\nfor numerical units, what unit should we express it in?\n\nHow the recorded data is being analyzed\n\nmight you bin/discretizing income data? what are the consequences of this?\n\n\n\n\n\nFor the following question, you may work under the second definition."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#what-type-of-variable-is-age",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#what-type-of-variable-is-age",
    "title": "Taxonomy of Data",
    "section": "What type of variable is age?",
    "text": "What type of variable is age?\nFor each of the following scenarios where age could be a variable, choose the most appropriate taxonomy according to the Taxonomy of Data.\n\n\nAges of television audiences/demographics\nAges of UC Berkeley students\nThe weight of a rock\n\n\n\n\nAnswer at pollev.com.\n\n\n\n\n−+\n01:00"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-1",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-1",
    "title": "Taxonomy of Data",
    "section": "Educated Guess 1",
    "text": "Educated Guess 1\n\nWhat will happen here?\n\n\nAnswer at pollev.com/&lt;name&gt;\n\n\n\n\n1 + \"one\"\n\n\n\n\n−+\n01:00\n\n\n\n\n\n“one” is a string with no link at all to the number 1\nwithout that link, without two objects that are recognized for their numerical value, + doesn’t work."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-2",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-2",
    "title": "Taxonomy of Data",
    "section": "Educated Guess 2",
    "text": "Educated Guess 2\n\nWhat will happen here?\n\n\nAnswer at pollev.com/&lt;name&gt;\n\n\n\n\na &lt;- c(1, 2, 3, 4)\nsqrt(log(a))\n\n\n\n\n−+\n01:00\n\n\n\n\nTalking points - a is a vector of length four - log and sqrt are functions that will return vectors of length four - they’re nested and will be evaluated from the inside out"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-3",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-3",
    "title": "Taxonomy of Data",
    "section": "Educated Guess 3",
    "text": "Educated Guess 3\n\nWhat will happen here?\n\n\nAnswer at pollev.com/&lt;name&gt;\n\n\n\n\na &lt;- 1 + 2\na + 1\n\n\n\n\n−+\n01:00\n\n\n\n\n\na is a not a string, it’s the name of an object that’s a number\nto overwrite a with a + 1 requires re-assigning it to a: a &lt;- a + 1 (in some languages, a + 1 would change the value of a)\na &lt;- a + 1 is a good time to mention that while a = a + 1 works in R and they might see it online, its convention to use &lt;- for many reasons including that mathematically the statement with = is confusing."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-4",
    "href": "1-questions-and-data/02-taxonomy-of-data/slides.html#educated-guess-4",
    "title": "Taxonomy of Data",
    "section": "Educated Guess 4",
    "text": "Educated Guess 4\n\nWhat will happen here?\n\n\nAnswer at pollev.com/&lt;name&gt;\n\n\n\n\na &lt;- c(1, 3.14, \"seven\")\nclass(a)\n\n\n\n\n−+\n01:00\n\n\n\n\n\nthe definition of a vector requires every element to be of the same type\nbased on their reading, there are three classes that they’re familiar with: numeric, factor, and character\nthere’s no way to translate “seven” into 7, so instead 1 and 3.14 must be translated into strings\nthey will likely encounter this when looking at a data set in R (or other languages) and finding that vectors with what looked like numbers are stored as strings. This usually happens because there’s a single errant character that the language doesn’t know how to parse as a number."
  },
  {
    "objectID": "1-questions-and-data/labs/00-getting-started/lab.html",
    "href": "1-questions-and-data/labs/00-getting-started/lab.html",
    "title": "Lab 0: Getting Started with R, RStudio and Quarto",
    "section": "",
    "text": "Complete the following questions in a Quarto document (.qmd). Render this document to HTML, print the corresponding HTML to PDF, and then submit the final PDF to Gradescope by Tuesday, Janaury 23rd at 9am!"
  },
  {
    "objectID": "1-questions-and-data/labs/00-getting-started/lab.html#question-1",
    "href": "1-questions-and-data/labs/00-getting-started/lab.html#question-1",
    "title": "Lab 0: Getting Started with R, RStudio and Quarto",
    "section": "Question 1",
    "text": "Question 1\n\nWhere are you from or where would you like to visit?\n\n\npart a\nTell us the county and state that you are from. If you are from outside the United States, provide the name of a county and state that you would like to visit. This text needs to show up as a header!\n\n\npart b\nCopy 1-3 paragraphs about this place from its Wikipedia page, and cite the page using a hyperlink.\n\n\npart c\nWrite an enumerated list of your top three favorite things about this place."
  },
  {
    "objectID": "1-questions-and-data/labs/00-getting-started/lab.html#question-2",
    "href": "1-questions-and-data/labs/00-getting-started/lab.html#question-2",
    "title": "Lab 0: Getting Started with R, RStudio and Quarto",
    "section": "Question 2",
    "text": "Question 2\n\npart a\nIn an R chunk, perform any calculation that results in the number 20. Make sure the number 20 prints out to the screen.\n\n\npart b\nCreate the data frame of Problem Set 1, Question 3 using R code and save it into the object my_classmates."
  },
  {
    "objectID": "1-questions-and-data/labs/00-getting-started/lab.html#question-3",
    "href": "1-questions-and-data/labs/00-getting-started/lab.html#question-3",
    "title": "Lab 0: Getting Started with R, RStudio and Quarto",
    "section": "Question 3",
    "text": "Question 3\nSurvey respondents are often asked questions containing answer choices belonging to what is called Likert Scale.\n\npart a\nVisit the Wikipedia page for the Likert Scale and create a vector containing the options within a typical, five-level Likert Scale.\n\n\npart b\nThen, use the factor() function to sort the levels of the vector you made in part a from decreasing to increasing."
  },
  {
    "objectID": "docs/website.html",
    "href": "docs/website.html",
    "title": "Configuring this Website",
    "section": "",
    "text": "One repo to rule them all.\n\n\n\nOrganization: files should be organized in a way that prioritizes thinking of it as a curriculum (instead of a website or a series of assignments). (subpoint: modularity)\nReproducibility:\nDocumentation:\nAutomation: automate as many non-human tasks as possible."
  },
  {
    "objectID": "docs/website.html#design",
    "href": "docs/website.html#design",
    "title": "Configuring this Website",
    "section": "",
    "text": "One repo to rule them all.\n\n\n\nOrganization: files should be organized in a way that prioritizes thinking of it as a curriculum (instead of a website or a series of assignments). (subpoint: modularity)\nReproducibility:\nDocumentation:\nAutomation: automate as many non-human tasks as possible."
  },
  {
    "objectID": "docs/website.html#contributing",
    "href": "docs/website.html#contributing",
    "title": "Configuring this Website",
    "section": "Contributing",
    "text": "Contributing\n\nConfiguring your machine\n\nOn your machine, a new RStudio project from version control, linked directly to the repo at https://github.com/stat20/stat20.\nBe sure you’ve configured git on your machine (allowing you to make commits) and stored a Personal Access Token (PAT) (allowing your machine to push commits to your repo on GitHub).\n\n\n\nWorkflow\n\nIn RStudio on main, pull any new changes.\nCreate a new branch named with what you’re adding, prepended with your name (e.g. andrew-update-lab-6).\nEdit the files you wish to work on. As you go, the easiest way to see how your changes look is to run quarto preview --profile staff-site at the terminal. That will do a minimal render of the website to show you the document that you’re working on and will re-render that document every time you save. You can also run quarto preview if you want to see the student-facing site (with no staff guide).\nCommit your changes to your new branch and push them to GitHub.\nGo to your GitHub and make a pull request from your new branch into stat20/stat20, main branch. Once the PR is made, it will kick-off a test rendering of both the student site and the staff site (the staff site will take &gt; 10 min to fully render). Once they’re done, you can go to https://stat20-pr.netlify.app/ and https://stat20staff-pr.netlify.app/ to see how they look. Note: this action will kickoff anew for each commit that you add to the PR, so it can be good to make the PR when you have just one commit on the branch, then check back as you push more commits to see how it looks.\nOnce the PR is merged into main, it will kick-off an action that will render and publish the staff site to https://stat20staff.netlify.app/. https://stat20.org/ doesn’t get rendered and published when main is changed, but instead of a pre-programmed schedule. After merging the branch, that branch can be deleted.\n\n\n\nAdding a page\n\nCreate a new qmd file\nAdd your content like normal\n\n\n\nCourse Settings"
  },
  {
    "objectID": "docs/assignments.html",
    "href": "docs/assignments.html",
    "title": "Adding and Removing Assignments",
    "section": "",
    "text": "Assignments are hosted on the Assignments page and include a list of Labs and a list of Problem Sets. These lists build automatically if the assignment files follow a particular naming scheme. Below we discuss how to add assignments to the list. They can be removed by renaming the files or removing them entirely from the repository.\nFor further reading on how these lists are made, see the Quarto documentation on document listings."
  },
  {
    "objectID": "docs/assignments.html#problem-sets",
    "href": "docs/assignments.html#problem-sets",
    "title": "Adding and Removing Assignments",
    "section": "Problem Sets",
    "text": "Problem Sets\nProblem sets are sets of drill-style problems that provide practice with the concepts and skills of a particular day’s lesson. They are stored within the subdirectory containing all of the materials for that day.\n\nPDF handouts\n\nCreate a file called ps.qmd alongside the notes for a given day’s materials. Your directory should look like this:\nintro-to-probability\n├── notes.qmd\n├── ps.qmd\nAt least two YAML options should be specified in the front-matter, title and the custom handout format:\n---\ntitle: Calculating Chances\nformat: stat20handout-pdf\n---\nUpon rendering the site, this assignment should appear on the assignments page. The format of the assignment list is controlled by the assignments template, which will automatically assign numbers to the problem sets based on their order in the directory.\n\nThe pdf notes use the stat20handout custom pdf format. See Custom Formats for more information on its use.\n\n\nQmd handouts\n\nJust like with pdfs, create a file called ps.qmd alongside the notes for a given day’s materials. Your directory should look like this:\nintro-to-probability\n├── notes.qmd\n├── ps.qmd\nAdd the following to the front-matter of ps.qmd.\n---\ntitle: Simulation\nformat:\n  html:\n    code-tools: \n      source: true\n      toggle: false\nsidebar: false\n---\nThe title should have the name of topic, usually the same as the title of the notes. This name will be used in creating the PS name in the listing. The remaining yaml options will provide a link at the top right of the problem set webpage that, when clicked, will produce a pop-up of the source of the qmd file. Students can copy and paste this into a blank qmd file in RStudio (and can remove the extra yaml options).\n\nIf you have multiple problem set files on a single day, you can differentiate them by adding a single letter or digit following ps and hyphen. ps.qmd, ps-2.qmd, and ps-b.qmd should all work. See the source for the assignment page for the rule that determines which files show up in the listing."
  },
  {
    "objectID": "docs/assignments.html#labs",
    "href": "docs/assignments.html#labs",
    "title": "Adding and Removing Assignments",
    "section": "Labs",
    "text": "Labs\n\n\n\n\n\n\nWarning\n\n\n\nUnder Construction"
  },
  {
    "objectID": "5-causation/03-matching/slides.html",
    "href": "5-causation/03-matching/slides.html",
    "title": "Causal Effects in Observational Studies",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set 19"
  },
  {
    "objectID": "5-causation/03-matching/slides.html#agenda",
    "href": "5-causation/03-matching/slides.html#agenda",
    "title": "Causal Effects in Observational Studies",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set 19"
  },
  {
    "objectID": "5-causation/03-matching/slides.html#announcements",
    "href": "5-causation/03-matching/slides.html#announcements",
    "title": "Causal Effects in Observational Studies",
    "section": "Announcements",
    "text": "Announcements\n\nPS 19 and PS 20 both due Tuesday 4/29 at 9:00 AM\nFinal exam review sessions:\n\n\nSummarization: 12pm-1pm Monday 4/29, Stanley 105\n\nCausality: 3pm-4pm Monday 4/29, Stanley 105\n\nGeneralization: 3pm-4pm Wednesday 5/1, VLSB 2050\n\nProbability: 4pm-5pm Wednesday 5/1, VLSB 2050\n\nPrediction: 3pm-4pm Friday 5/3, Stanley 105\n\n\nFinal exam: 7pm-10pm, Thursday 5/9, room TBA.\nPlease fill out course evals!"
  },
  {
    "objectID": "5-causation/03-matching/slides.html#section",
    "href": "5-causation/03-matching/slides.html#section",
    "title": "Causal Effects in Observational Studies",
    "section": "",
    "text": "To study the impact of receiving permanent resident status on mental health, we compare answers to a psychiatric survey from people who entered and won the US green card lottery to answers from others who entered but did not win.\n\nWhat kind of study is this?\n\nA randomized trial.\nA natural experiment.\nAn observational study that requires matching.\nNone of the above.\n\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nThis is the first of a series of questions that each ask students to classify a study. You may want to consider going through all of these questions before reviewing answers and then having a broader discussion about all the examples.\nFor the first question, the best answer is B, since the lottery is purportedly random but the assignment mechanism is not under the researcher’s control. Note that it is important that the comparison is to those who entered the lottery but did not win; a more general comparison to immigrants who did not receive green cards would be more of an observational study."
  },
  {
    "objectID": "5-causation/03-matching/slides.html#section-1",
    "href": "5-causation/03-matching/slides.html#section-1",
    "title": "Causal Effects in Observational Studies",
    "section": "",
    "text": "To study the impact of childhood trauma on later academic performance, we compare GRE scores for students who lost a close family member in an automobile accident before the age of 8 to GRE scores for students who did not lose a close family member before age 8.\n\nWhat kind of study is this?\n\nA randomized trial.\nA natural experiment.\nAn observational study that requires matching.\nNone of the above.\n\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nThe answer is C, since automobile accidents to one’s parents are not assigned randomly. For example, we’d expect children whose parents work in driving-intensive blue-collar occupations such as trucking, driving for rideshare companies, and commercial delivery to be overrepresented in the treated group, and these subjects are also less likely to pursue graduate school at baseline giving that their parents may not have attended college at all."
  },
  {
    "objectID": "5-causation/03-matching/slides.html#section-2",
    "href": "5-causation/03-matching/slides.html#section-2",
    "title": "Causal Effects in Observational Studies",
    "section": "",
    "text": "To study the effectiveness of a blood pressure medication, we enroll 500 patients. We take the blood pressure of all patients before anyone receives medication. We assign the 200 patients with the highest blood pressure readings to get the medication, assigning the others to be controls.\n\nWhat kind of study is this?\n\nA randomized trial.\nA natural experiment.\nAn observational study that requires matching.\nNone of the above.\n\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nThis is a terrible study design, which builds in confounding on an important covariate. Although you could attempt matching here, it would likely be ineffective since it would be very hard to create many close matches on pre-treatment blood pressure. So “none of the above” seems like the most reasonable answer here.\nFor this study it’s a good idea to talk about what the randomized trial would have looked like – clearly you’d want to pick the 200 treaated subjects by simple random sampling rather than this deterministic rule."
  },
  {
    "objectID": "5-causation/03-matching/slides.html#section-3",
    "href": "5-causation/03-matching/slides.html#section-3",
    "title": "Causal Effects in Observational Studies",
    "section": "",
    "text": "In the next slide, you will see the first few rows of a dataset containing demographic information on California counties. Scroll to see all of the rows.\n. . .\n\nWe are interested in determining whether a difference in median_edu has a causal effect on homeownership using matching. Which county serves as the best counterfactual match to Fresno County?\n\nKern County\nAlameda County\nContra Costa County\nShasta County\nDel Norte County\n\n\n\nThe challenge in this question is to remember the distinction between covariates, treatment, and response. When forming matched pairs, you want to look for covariates that are the same and treatments that are different, while you should be ignoring the response completely.\nThe answer is (A), since both covariates (metro and smoking_ban) are identical to those of Fresno County, but the treatment (med_edu) is different.\nYou may want to mention that this was a final question in a previous semester.\n\n. . .\n\n\n\n\n−&plus;\n\n02:00"
  },
  {
    "objectID": "5-causation/03-matching/slides.html#section-4",
    "href": "5-causation/03-matching/slides.html#section-4",
    "title": "Causal Effects in Observational Studies",
    "section": "",
    "text": "name\nhomeownership\nmedian_edu\nmetro\nsmoking_ban\n\n\n\nFresno County\n55.0\nsome_college\nyes\nnone\n\n\nColusa County\n64.4\nhs_diploma\nno\nnone\n\n\nDel Norte County\n60.9\nhs_diploma\nno\nnone\n\n\nAlameda County\n55.1\nsome_college\nyes\nnone\n\n\nContra Costa County\n69.5\nsome_college\nyes\npartial\n\n\nGlenn County\n67.5\nhs_diploma\nno\nnone\n\n\nShasta County\n66.0\nsome_college\nyes\nnone\n\n\nKern County\n61.4\nhs_diploma\nyes\nnone\n\n\nSan Luis Obispo County\n61.4\nsome_college\nyes\nnone"
  },
  {
    "objectID": "5-causation/03-matching/slides.html#section-5",
    "href": "5-causation/03-matching/slides.html#section-5",
    "title": "Causal Effects in Observational Studies",
    "section": "",
    "text": "In this table there are nine counties, five with some_college values for median_edu and four with hs_diploma values.\n\nHow many counties of each type will remain after we conduct optimal matching on metro and smoking_ban?\n\n\nsome_college: 4, hs_diploma: 4.\n\nsome_college: 5, hs_diploma: 4.\n\nsome_college: 2, hs_diploma: 2.\n\nsome_college: 2, hs_diploma: 4.\nCan’t tell without more information.\n\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nThis question is designed to shift students away from thinking about individual matched pairs toward thinking about how matching reshapes an entire dataset. The correct answer is (A) since every county with hs_diploma is matched to a single county with some_college.\nThis question is also a good jumping off point for a mini-lecture about matching. The county example is not ideal because there are not a lot of close matches and because there are a lot of ties among the distances so the best match is not unique (although Contra Costa County is probably the one that will get dropped since it looks the least like any of the hs_diploma counties).\nA better source for material in the mini lecture is the “matching_mini_lecture.docx” file about final exam scores and attending review sessions. Eventually this example might be a good thing to incorporate into the notes."
  },
  {
    "objectID": "5-causation/03-matching/slides.html#section-6",
    "href": "5-causation/03-matching/slides.html#section-6",
    "title": "Causal Effects in Observational Studies",
    "section": "",
    "text": "Which R command correctly performs matching on covariates to measure the impact of median_edu on homeownership?\n\nmatchit(homeownership ~ median_edu, data = county, method = ‘optimal’, distance = ‘euclidean’)\nmatchit(median_edu ~ homeownership, data = county, method = ‘optimal’, distance = ‘euclidean’)\nmatchit(median_edu ~ metro + smoking_ban, data = county, method = ‘optimal’, distance = ‘euclidean’)\nmatchit(homeownership ~ median_edu + metro + smoking_ban, data = county, method = ‘optimal’, distance = ‘euclidean’)\n\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nThe correct answer is (C). This is a good chance to underline the idea that the response variable is ignored completely when creating matched pairs and also to drill students on the syntax “treatment ~ covariates,” which is shared by both matchit and bal.tab."
  },
  {
    "objectID": "5-causation/03-matching/slides.html#section-7",
    "href": "5-causation/03-matching/slides.html#section-7",
    "title": "Causal Effects in Observational Studies",
    "section": "",
    "text": "Assuming that metro and smoking_ban variables are the only ones we have measured, name an unmeasured variable that could introduce confounding between median_edu and homeownership.\n\n\n\n\n\n−&plus;\n\n02:00\n\n\n\n\nThere’s a free response question on PollEV if you want to use it; either I think this works best as a “pair-and-share” group discussion question followed by a brief class discussion (either with groups volunteering answers or you reviewing their submitted short-answers from Poll EV). Some interesting variables to discuss might include median age and whether or not a county contains a large university."
  },
  {
    "objectID": "5-causation/01-defining-causality/slides.html",
    "href": "5-causation/01-defining-causality/slides.html",
    "title": "Defining Causality",
    "section": "",
    "text": "Announcements\nConcept Questions \n\nLab: A Matter of Taste"
  },
  {
    "objectID": "5-causation/01-defining-causality/slides.html#section",
    "href": "5-causation/01-defining-causality/slides.html#section",
    "title": "Defining Causality",
    "section": "",
    "text": "Suppose that a prisoner is about to be executed by a firing squad. A certain chain of events must occur for this to happen. First, the judge orders the execution. The order goes to a captain, who signals the two soldiers of the firing squad (soldier 1 and soldier 2) to fire. They are obedient and expert marksmen, so they only fire on command, and if either one of them shoots, the prisoner dies.\n\nUsing the conditional counterfactual definition, who caused the death of the prisoner?\nA. The judge\nB. The captain\nC. Soldier 1\nD. Soldier 2\n\n\n\n\n\n−&plus;\n\n02:00\n\n\n\n\nThe goal of this example is to reinforce the definition of cause and effect by applying it to a fairly complicated causal graph. The correct answers are A and B, because had the judge not ordered, or had the captain not signaled, the prisoner would not have died. Each of the soldiers individually fail this definition because if one of them doesn’t fire (and they’re acting independently), the prisoner will still die.\nYou can debrief this one by drawing a causal diagram with judge -&gt; captain -&gt; -&gt; two arrows to separate solider 1 and soldier 2 -&gt; -&gt; two arrows back to prisoner. You can start at the top, drawing 2 by 3 data frames of first the judge, then captain, then soldiers, showing one world in the first row and the other world in the second row and showing that the potential outcomes would differ based on the action of the first two.\nYou can also use the graph method of: if you break the causal chain after a given node, is there still a connected path to the prisoner? The answer is no for judge and captain but yes for each solider.\nStudents may bring up good points about ambiguity in the wording of the problem or a different interpretation of the “rules of the game” around how the causal structure works. That’s good! If students suggest different interpretations, you could draw that graph or draw that data frame, and analyze that one to identify causes.\nAs a postscript, this is likely one reason why firing squads are have multiple shooters. Each one has plausible deniability that “the prisoner would have died anyways”, so they didn’t cause the death. Apparently, it’s common for marksmen to occasionally neglect to fire, so this also has redundancy. The judge and captain, though they might be insolated from guilt because they are higher up the causal chain, they are still a cause of the death by this definition."
  },
  {
    "objectID": "5-causation/01-defining-causality/slides.html#section-1",
    "href": "5-causation/01-defining-causality/slides.html#section-1",
    "title": "Defining Causality",
    "section": "",
    "text": "All of the four people in the table below need to travel from Berkeley to San Francisco today, and are trying to decide whether to take BART or to drive. The table below shows travel times for each individual (assume these are the true, unknown numbers, not just predictions).\n\n\nName\nTravel time by car (min)\nTravel time by BART (min)\n\n\n\nMaria\n35\n28\n\n\nFan\n30\n32\n\n\nAlice\n40\n55\n\n\nMuhammad\n30\n40\n\n\n\n. . .\n\nPick the correct way to fill in the following sentence: “Taking BART instead of driving will cause ___ to arrive more than ___ minutes later.”\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nThis is the first question in a sequence and is kind of a warmup. It should be quite easy and probably only needs 30 seconds to answer."
  },
  {
    "objectID": "5-causation/01-defining-causality/slides.html#section-2",
    "href": "5-causation/01-defining-causality/slides.html#section-2",
    "title": "Defining Causality",
    "section": "",
    "text": "All of the four people in the table below need to travel from Berkeley to San Francisco today, and are trying to decide whether to take BART or to drive. The table below shows travel times for each individual (assume these are the true, unknown numbers, not just predictions).\n\n\nName\nTravel time by car (min)\nTravel time by BART (min)\n\n\n\nMaria\n35\n28\n\n\nFan\n30\n32\n\n\nAlice\n40\n55\n\n\nMuhammad\n30\n40\n\n\n\n. . .\n\nWhat is the average treatment effect of taking BART on travel time for these four subjects?\n\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "5-causation/01-defining-causality/slides.html#section-3",
    "href": "5-causation/01-defining-causality/slides.html#section-3",
    "title": "Defining Causality",
    "section": "",
    "text": "All of the four people in the table below need to travel from Berkeley to San Francisco today. The table below shows the actual travel times for each individual using the method they chose to use.\n\n\nName\nTravel time by car (min)\nTravel time by BART (min)\n\n\n\nMaria\n?\n28\n\n\nFan\n?\n32\n\n\nAlice\n40\n?\n\n\nMuhammad\n30\n?\n\n\n\n. . .\n\nWhat is a natural estimate of the average effect of taking BART (rather than driving) if we only observed this data?\n\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "5-causation/01-defining-causality/slides.html#section-4",
    "href": "5-causation/01-defining-causality/slides.html#section-4",
    "title": "Defining Causality",
    "section": "",
    "text": "All of the four people in the table below need to travel from Berkeley to San Francisco today. The table below shows the actual travel times for each individual using the method they chose to use.\n\n\nName\nTravel time by car (min)\nTravel time by BART (min)\n\n\n\nMaria\n?\n28\n\n\nFan\n?\n32\n\n\nAlice\n40\n?\n\n\nMuhammad\n30\n?\n\n\n\n. . .\n\nWhat are two different reasons why this estimate of the average treatment effect might not be very reliable?\n\n\n\n\n\n−&plus;\n\n03:00\n\n\n\n\nThis last question is designed to be a “pair and share” exercise and does not have an associated PollEV item. After they discuss you can ask a couple of them to share.\nThis is a good chance to talk about why it’s hard to measure causal effects. In the example the people who choose to ride BART are those for whom getting to San Francisco is easiest anyway. Asking “How would you try to collect data about this causal claim differently to get a better. measure of the causal effect?” provides a nice lead-in to the lab."
  },
  {
    "objectID": "5-causation/02-experiments/slides.html",
    "href": "5-causation/02-experiments/slides.html",
    "title": "Experiments",
    "section": "",
    "text": "Announcements\nConcept Questions\nLab 7.2"
  },
  {
    "objectID": "5-causation/02-experiments/slides.html#agenda",
    "href": "5-causation/02-experiments/slides.html#agenda",
    "title": "Experiments",
    "section": "",
    "text": "Announcements\nConcept Questions\nLab 7.2"
  },
  {
    "objectID": "5-causation/02-experiments/slides.html#announcements",
    "href": "5-causation/02-experiments/slides.html#announcements",
    "title": "Experiments",
    "section": "Announcements",
    "text": "Announcements\n\nLab 7.1 due Friday 4/19 at 11:59 PM. Aim to finish during class today.\nLab 7.2 due Tuesday 4/23 at 9:00 AM.\nExperiment groups must be of size 3 or 4.\n2 problem sets will be assigned next week (both due Tuesday 4/30)."
  },
  {
    "objectID": "5-causation/02-experiments/slides.html#section",
    "href": "5-causation/02-experiments/slides.html#section",
    "title": "Experiments",
    "section": "",
    "text": "We run an experiment to see if informative phone calls encourage people to vote. We obtain phone numbers for 1000 registered voters and randomly assign half of them to receive calls from volunteers. The volunteers tell them the location of their polling place and the date of the election. Our outcome is whether or not the subjects vote in the next election.\n\nWhich of the following are possible covariates for our study?\n\nThe party affiliation of the voter (Republican or Democrat).\nWhether the next election is a presidential election or not.\nWhether the voter voted in the last election.\nWhether the voter votes primarily for Republicans or Democrats in the next election.\nWhether the treated voters hung up on the volunteer.\n\n\n\nThe point of this question is to get the students thinking about whether variables are measured before or after treatment assignment.A and C are correct answers, D and E are not (although E is measured before the primary outcome, it is still after treatment). B is technically a covariate but probably not a useful one; unless our experiments spans multiple election cycles, it will have the same value for all subjects.\n\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "5-causation/02-experiments/slides.html#section-1",
    "href": "5-causation/02-experiments/slides.html#section-1",
    "title": "Experiments",
    "section": "",
    "text": "Which of the following claims about covariate balance are true?\n\nRandomized treatment assignment tends to produce covariate balance.\nA standardized mean difference that is not equal to zero likely means that randomization was not conducted correctly.\nThe Love plot shows the difference in means or proportions for each covariate across treatment groups.\nIn a hypothesis test for balance, the null hypothesis is that the treatment assignment and the covariate are independent.\n\n\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "5-causation/02-experiments/slides.html#the-essential-question-of-our-time",
    "href": "5-causation/02-experiments/slides.html#the-essential-question-of-our-time",
    "title": "Experiments",
    "section": "The Essential Question of Our Time",
    "text": "The Essential Question of Our Time\n\n\n\n\n\n\n\n\n\nWhich is the best flavor?\n\n\nThis purely for fun. Some people have strong opinions about this."
  },
  {
    "objectID": "5-causation/02-experiments/slides.html#section-2",
    "href": "5-causation/02-experiments/slides.html#section-2",
    "title": "Experiments",
    "section": "",
    "text": "Could your taster distinguish between flavors?\n\nYes, and it was statistically significant (p &lt; .05).\n\nYes, but it was not statistically significant (p &gt; .05).\n\nYes, but we haven’t conducted the test yet.\nNo.\n\nNot sure, we haven’t computed the difference in proportions yet.\nWe tested a different claim.\n\n\n\n\nThe goal of this question is to allow all groups to concisely share their results with the class. It also opens up the conversation about how there were many different ways to carry out this experiment. Two groups might have come to different conclusions because their tasters really had different abilities, or because their claims and experiments were slightly different, or just due to random chance.\nYou may want to project 1 or 2 protocols and ask the groups to talk through how they uses the principles of experimental design."
  },
  {
    "objectID": "5-causation/04-time/ps.html",
    "href": "5-causation/04-time/ps.html",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "We continue our analysis of minimum wage and employment using Card and Krueger’s data about the 1992 New Jersey minimum wage increase. We now use a different version of the data that contains covariate and outcome values for each store both before and after New Jersey’s policy change. This dataframe can be loaded as fastfood2 from the stat20data package.\n\nDescribe in words how to construct a pre/post estimate for this dataset, and how to construct a difference in difference estimate. What are the differences in the assumptions we rely on between these two designs? Which do you think is more plausible?\n\n\n\n\n\n\n\n\n\n\n\nWhat additional data would we need to have available to conduct an interrupted time series design? How would the associated assumptions differ from those in a pre/post design?\n\n\n\n\n\n\n\n\n\nSketch a line plot showing how you anticipate employment levels will change between the two timepoints, with one line averaging over all New Jersey restaurants and one line averaging over all Pennsylvania restaurants.\n\n\n\n\n\n\n\n\n\n\n\n\nWrite code to create a line plot like the one in the previous question and construct it. Describe similarities and differences between the hypothesized and actual plots.\n\n\n\n\n\n\n\n\n\n\nWrite code to compute the pre/post estimate and difference in difference estimators. Run the code and compare the values obtained to each other and to the effects you obtained from the fastfood1 dataset in the previous problem set.\n\n\n\n\n\n\n\n\n\n\nWrite code to construct a bootstrap confidence interval or a hypothesis test for one of the estimates in the previous question, making sure to permute or bootstrap restaurants rather than individual observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecently a new minimum wage of $20 per hour went into effect in California. Raising the wage to this level was a politically controversial decision. Give an example of both predictive and causal claims that proponents or detractors of the law might make and describe how they are different.\n\n\n\n\n\n\n\n\n\n\n\n\nTo what extent is your analysis of Card and Krueger’s fast food data relevant for evaluating the claims from the previous question? To the degree it is relevant, what does it tell us about the claims?"
  },
  {
    "objectID": "5-causation/04-time/ps.html#case-study-minimum-wage-and-the-njpa-fast-food-industry-cont.",
    "href": "5-causation/04-time/ps.html#case-study-minimum-wage-and-the-njpa-fast-food-industry-cont.",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "We continue our analysis of minimum wage and employment using Card and Krueger’s data about the 1992 New Jersey minimum wage increase. We now use a different version of the data that contains covariate and outcome values for each store both before and after New Jersey’s policy change. This dataframe can be loaded as fastfood2 from the stat20data package.\n\nDescribe in words how to construct a pre/post estimate for this dataset, and how to construct a difference in difference estimate. What are the differences in the assumptions we rely on between these two designs? Which do you think is more plausible?\n\n\n\n\n\n\n\n\n\n\n\nWhat additional data would we need to have available to conduct an interrupted time series design? How would the associated assumptions differ from those in a pre/post design?\n\n\n\n\n\n\n\n\n\nSketch a line plot showing how you anticipate employment levels will change between the two timepoints, with one line averaging over all New Jersey restaurants and one line averaging over all Pennsylvania restaurants.\n\n\n\n\n\n\n\n\n\n\n\n\nWrite code to create a line plot like the one in the previous question and construct it. Describe similarities and differences between the hypothesized and actual plots.\n\n\n\n\n\n\n\n\n\n\nWrite code to compute the pre/post estimate and difference in difference estimators. Run the code and compare the values obtained to each other and to the effects you obtained from the fastfood1 dataset in the previous problem set.\n\n\n\n\n\n\n\n\n\n\nWrite code to construct a bootstrap confidence interval or a hypothesis test for one of the estimates in the previous question, making sure to permute or bootstrap restaurants rather than individual observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecently a new minimum wage of $20 per hour went into effect in California. Raising the wage to this level was a politically controversial decision. Give an example of both predictive and causal claims that proponents or detractors of the law might make and describe how they are different.\n\n\n\n\n\n\n\n\n\n\n\n\nTo what extent is your analysis of Card and Krueger’s fast food data relevant for evaluating the claims from the previous question? To the degree it is relevant, what does it tell us about the claims?"
  },
  {
    "objectID": "5-causation/04-time/notes.html",
    "href": "5-causation/04-time/notes.html",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "When we don’t have access to a randomized experiment or a natural experiment, creative methods are required to extract evidence about causal claims from data. In the last set of notes, we looked at matching methods, which approximate unobserved counterfactual outcomes by identifying different subjects that appear very similar based on their covariate values. Today we think about a very different way to approximate missing counterfactuals: repeated observations for the same subject at different times with different treatments.\n\nLet’s say that we collected data from Evelyn Fix before and after she graduated from Cal.\n\n\n\n\nStudent\ndate\nGPA\nYears exp\nRec\nCal Grad\nGood Job\n\n\n\nEvelyn Fix\nJune 2022\n3.9\n3\nstrong\nyes\nyes\n\n\nEvelyn Fix\nApril 2022\n3.9\n3\nstrong\nno\nno\n\n\n\n\n\nHere, the unit of observation is a student at a particular time, which allows us to compare Evelyn before and after graduating from Cal. How close a counterfactual is pre-graduation Evelyn? Keep in mind the true counterfactual would be the alternate universe where everything about the world is the same but for one thing: Evelyn hadn’t graduated from Cal. Is everything the same in this pre-graduation world?\nThe answer may be yes. Here we see her GPA is unchanged, she hasn’t gained any more work experience, and she’s sending out the same strong letter of recommendation. If these are the only other variables involved in getting a good job, then this would be good evidence that the graduation is the cause of the good job.\nBut what if her GPA had inched up that final semester? Or what if the jobs that she applied for pre-graduation were simply not as good as the jobs that she applied for after graduation? In this setting, there could be multiple reasons that she got the good job because we haven’t isolated the variable of interest: graduating from Cal.\nData with repeated measures for individual subjects is called longitudinal or panel data. There are several study designs that rely upon this structure to interrogate causal claims. Pre/post designs use two measurements for each subject, one with the treatment and one without, to measure a causal effect. Interrupted time series designs are similar but use multiple observations before and after each unit enters treatment to estimate counterfactual trends. Finally, difference in difference designs unite some of the benefits of matching and pre/post designs."
  },
  {
    "objectID": "5-causation/04-time/notes.html#causality-and-variation-across-time",
    "href": "5-causation/04-time/notes.html#causality-and-variation-across-time",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "When we don’t have access to a randomized experiment or a natural experiment, creative methods are required to extract evidence about causal claims from data. In the last set of notes, we looked at matching methods, which approximate unobserved counterfactual outcomes by identifying different subjects that appear very similar based on their covariate values. Today we think about a very different way to approximate missing counterfactuals: repeated observations for the same subject at different times with different treatments.\n\nLet’s say that we collected data from Evelyn Fix before and after she graduated from Cal.\n\n\n\n\nStudent\ndate\nGPA\nYears exp\nRec\nCal Grad\nGood Job\n\n\n\nEvelyn Fix\nJune 2022\n3.9\n3\nstrong\nyes\nyes\n\n\nEvelyn Fix\nApril 2022\n3.9\n3\nstrong\nno\nno\n\n\n\n\n\nHere, the unit of observation is a student at a particular time, which allows us to compare Evelyn before and after graduating from Cal. How close a counterfactual is pre-graduation Evelyn? Keep in mind the true counterfactual would be the alternate universe where everything about the world is the same but for one thing: Evelyn hadn’t graduated from Cal. Is everything the same in this pre-graduation world?\nThe answer may be yes. Here we see her GPA is unchanged, she hasn’t gained any more work experience, and she’s sending out the same strong letter of recommendation. If these are the only other variables involved in getting a good job, then this would be good evidence that the graduation is the cause of the good job.\nBut what if her GPA had inched up that final semester? Or what if the jobs that she applied for pre-graduation were simply not as good as the jobs that she applied for after graduation? In this setting, there could be multiple reasons that she got the good job because we haven’t isolated the variable of interest: graduating from Cal.\nData with repeated measures for individual subjects is called longitudinal or panel data. There are several study designs that rely upon this structure to interrogate causal claims. Pre/post designs use two measurements for each subject, one with the treatment and one without, to measure a causal effect. Interrupted time series designs are similar but use multiple observations before and after each unit enters treatment to estimate counterfactual trends. Finally, difference in difference designs unite some of the benefits of matching and pre/post designs."
  },
  {
    "objectID": "5-causation/04-time/notes.html#do-supermarket-checkout-displays-cause-unhealthy-eating",
    "href": "5-causation/04-time/notes.html#do-supermarket-checkout-displays-cause-unhealthy-eating",
    "title": "Using Time to Measure Causal Effects",
    "section": "Do supermarket checkout displays cause unhealthy eating?",
    "text": "Do supermarket checkout displays cause unhealthy eating?\n1\nYou (as well as any small children you know) have probably noticed that supermarket checkout lines often feature a display of candy, soda, chips, or other “less-healthy” snacks. Government agencies and consumer advocacy groups have called for supermarket chains to remove such displays, claiming that they lead to unhealthy eating choices and poorer health for consumers. In the United Kingdom, some supermarkets have responded by instituting ‘checkout line’ policies under which candy is removed and replaced with healthier options. We will look at data2 on the number of purchases of unhealthy foods at four stores that instituted such a policy in the mid-2010s. Purchases are measured both before and after the policy is instituted.\n\n\n  store_id sales checkout_policy\n1      243  4585           FALSE\n2      243  2455            TRUE\n3      245  6221           FALSE\n4      245  4344            TRUE\n5      246  1991           FALSE\n6      246  3341            TRUE\n7      247  2347           FALSE\n8      247  2271            TRUE\n\n\nThe causal claim here is that consumers would have purchased fewer unhealthy snacks if the candy display had been removed from the checkout line (the broader claims about the impact of this change on public health are not things we can test with data from the supermarkets alone). A great way to test this theory might be to randomly assign some supermarkets to remove their checkout displays and others to keep theirs, and compare purchases. Unfortunately that study was not conducted, and it’s not clear that these stores decided to institute their policies in a quasi-random way (e.g. what if only the stores selling very low levels of candy agreed to participate?). We could also consider matching stores with the policy to those without the policy and comparing their sales. But we our dataset doesn’t include many store covariates, and in general datasets like this one may only include stores that instituted the policy."
  },
  {
    "objectID": "5-causation/04-time/notes.html#prepost-design",
    "href": "5-causation/04-time/notes.html#prepost-design",
    "title": "Using Time to Measure Causal Effects",
    "section": "Pre/post design",
    "text": "Pre/post design\nInstead, we will focus on comparing the same stores before and after the policy change, an idea called a pre/post design.\n\nPre/post design\n\nA study in which a group of units receiving a treatment is compared to earlier versions of themselves before treatment occurred.\n\n\nA pre/post design takes the simple approach of subtracting the pre-treatment value from the post-treatment value for each store and taking the average. The following plot shows the change in sales for each of four stores in the data, and the pre/post estimate is computed below. \n\n\n\n\n\n\n\n\n\n\nResponse: sales (numeric)\nExplanatory: checkout_policy (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 -683.\n\n\nPros and cons of pre/post\nTo see the benefits of a pre/post design, think about what covariate balance looks like on store covariates such as size, geographic location (urban/suburban/rural), demographics of surrounding neighborhood (senior citizens vs. young professionals vs. families with children), and staffing. Even though we don’t have these covariates measured in our dataset, we can be pretty confident that most of them are perfectly or near-perfectly balanced.\nCovariate balance also helps identify the biggest weakness of a pre/post design. What would the SMD look like for time (if we measured as a numeric variable counting the number of months starting from January 2016)? It would probably be quite large since the treated observations are systematically later than the control observations. So although most covariates will be well-controlled, pre/post designs are highly susceptible to time trends in the outcome (or other outcome-related covariates that shift over time). For example, what if one of the stores rolled out its policy on January 1st, so that the four weeks prior to the policy included the winter holidays, when British consumers traditionally buy and consume a lot of sweets, and the four weeks immediately following the policy included the beginning of the year when many people are starting diet and exercise plans? A matched study could avoid this particular issue by comparing different stores, one with the policy and one without, during the same time period. So while matched studies require no unmeasured confounding, pre/post designs require counterfactual outcomes and unmeasured confounding to be stable across time."
  },
  {
    "objectID": "5-causation/04-time/notes.html#dealing-with-effects-of-time",
    "href": "5-causation/04-time/notes.html#dealing-with-effects-of-time",
    "title": "Using Time to Measure Causal Effects",
    "section": "Dealing with effects of time",
    "text": "Dealing with effects of time\nSuppose there is a gradual downward trend in purchase of unhealthy snacks over time, as British consumers become better informed about health risks and gradually switch to healthier options. This is a problem for our pre/post design, which can’t distinguish between reductions in sales due to the nationwide trend and reductions in sales due to the policy change that we’re interested in. Fortunately if we have the right kind of additional data available we can attempt to identify the time trend and distinguish it from our treatment effect. We’ll focus on two approaches, each of which relies on a different kind of additional data.\nMore timepoints: interrupted time series\nFirst note that in addition to measuring sales in the four-week periods immediately before and after the policy change, we have data on several more four-week periods going further back in time before the change, and going further forward in time after the change. We can now extend the earlier plot for the pre/post design to include the additional timepoints.\n\n\n\n\n\n\n\n\nBy looking at the changes over time before and after the time, we are able to get a sense for the time trend; in addition, by looking at the sudden shift in the line’s height that occurs at the time of the policy change, we can get a separate sense for the average treatment effect. Because the treatment effect “interrupts” the pattern of the smooth time trend, this design is known as an interrupted time series.\nTo estimate the treatment effect after separating out the trend, we can fit a regression model for sales with two independent variables, a numeric variable for time period and a binary indicator for whether the policy change has been instituted:\n\n\n\nCall:\nlm(formula = sales ~ checkout_policy + time, data = checkout_its)\n\nCoefficients:\n        (Intercept)  checkout_policyTRUE                 time  \n            3109.49              -570.74                62.79  \n\n\nFitting this model to all our observations will give us two parallel lines, one for the before-treatment periods (drawn in red) and one for the after-treatment periods (drawn in blue).\n\n\n\n\n\n\n\n\nThe distance from the red line to the blue line is given by the coefficient of checkout_policy and is our estimate of the average treatment effect. You can think of the dotted red line as the (counterfactual) version of sales had the policy not been instituted, to which we are comparing the actual post-treatment sales (in solid blue). \n\n\nInterrupted time series do not require unobserved confounders to be stable over time, but they do require changes in unobserved confounders to affect outcomes only linearly (i.e. with a constant slope). To see why this assumption is important, consider the following hypothetical analysis:\n\n\n\n\n\n\n\n\nHere the gray band gives the shape of the actual pattern in the data, which appears to be a smooth and gradually decreasing curve. Although there is no apparent treatment effect here (the pattern doesn’t experience any jumps or dips at the time of treatment) fitting straight lines to the curve incorrectly suggests that a large negative treatment effect (the large gap between the red and blue lines) is present.\n\nPure controls: difference-in-difference design\nThe interrupted time series design relies on additional observations in pre and/or post periods to separate a time trend from a treatment effect. Another kind of data that can help us is data from additional stores during the same time periods that never institute a checkout policy. Clearly these stores do not experience a treatment effect, but if they are subject to the same time trend as the treated stores, we can use them to estimate the time trend. Here is a plot illustrating this idea:\n3\nThe dashed line represents a counterfactual average outcome for the treated units if they had not received treatment, and we are interested in learning the difference between this dashed line and the actual outcome for the intervention group. Although we don’t have multiple observations prior to treatment to estimate the slope of the dashed line anymore, we can learn it by looking at the pre/post differences in the control group below. We can then get an effect estimate by subtracting the pure control pre/post difference in the outcome \\(Y\\) from the treated group’s pre/post difference in outcome \\(Y\\):\n\\[\n\\text{Estimated effect} = (\\overline{Y}^{treat}_{post} -  \\overline{Y}^{treat}_{pre}) -\n(\\overline{Y}^{control}_{post} -  \\overline{Y}^{ control}_{pre})\n\\]\nThis is called a difference-in-difference design. You can think of it as a combination of the matching approach from the last set of notes and the panel data approaches from these set of notes, since it takes advantages of comparisons across both units and time. It doesn’t require that time trends be linear as the interrupted time series does (since we don’t have to rely on a constant slope across previous time points), but it does require that the time trend is the same in the pure control stores as in the treated stores, an assumption known as parallel trends.\nWe now test out difference-in-difference analysis on the supermarket checkout data. We start by making a pre/post plot with our four treated stores and two additional stores that never instituted a checkout policy 4.\n\n\n\n\n\n\n\n\nOne thing that immediately jumps out about this plot is that both pure control stores have much larger sales volume in general than the treated stores. This probably means that the pure control stores are much bigger than their treated counterparts. This would be very bad news if we were trying to match treated stores in the post period to control stores in the post period, since we wouldn’t succeed in finding close pairs. Fortunately we don’t need to construct close matched pairs for this design. We do need to believe parallel counterfactual time trends (on average) between the treated stores and the pure control stores if neither were to be treated, but this seems more plausible based on the plot5.\nWe now compute pre/post differences for the treated stores and for the pure control stores, and take their difference to get our effect estimate.\n\n\n# A tibble: 2 × 2\n  ever_treated `Average Pre/Post Difference in Sales`\n  &lt;lgl&gt;                                         &lt;dbl&gt;\n1 FALSE                                         -655 \n2 TRUE                                          -683.\n\n\nResponse: sales_diff (numeric)\nExplanatory: ever_treated (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 -28.2"
  },
  {
    "objectID": "5-causation/04-time/notes.html#dependence-in-panel-data-and-matched-designs",
    "href": "5-causation/04-time/notes.html#dependence-in-panel-data-and-matched-designs",
    "title": "Using Time to Measure Causal Effects",
    "section": "Dependence in panel data and matched designs",
    "text": "Dependence in panel data and matched designs\nSo far we have focused on estimating effects using panel data. We may also be interested in creating confidence intervals for our effect estimates and conducting hypothesis tests to determine whether they differ significantly from zero. At a high level, we can use the same processes as in the generalization unit.\nHowever, we have to be careful because in panel data it is usually not reasonable to think of our individual observations as a simple random sample or as independent and identically distributed draws from a population. Instead, our data come in groups of repeated observations for an individual subject, which will tend to be more similar to each other than to observations from other subjects. This has implications for how we bootstrap or permute our data.\nFor example, if we simply permuted all sales values in our checkout policy dataset, the values we observed for a single store might be scattered across several different stores instead of staying together. Similarly, if we took a bootstrap sample from all observations, we might end up with three or more observations from some stores and only one for others. This would make it difficult to compute a pre/post estimate!\nThe simplest solution is to aggregate our data to form a new data frame with one row per unique subject, then permute or bootstrap the rows of this new data frame. For pre/post and difference in difference designs, the natural way to aggregate is to take each subject’s pre/post difference, since the effect estimate is simply an average (or a difference in means) of these quantities across unique subjects. 6\nA similar strategy is recommended for matching estimators. Although matched subjects are not actually different copies of the same person, they have been chosen to be more similar to each other than to other subjects; because of this we should permute or bootstrap matched differences rather than individual outcomes."
  },
  {
    "objectID": "5-causation/04-time/notes.html#the-ideas-in-code",
    "href": "5-causation/04-time/notes.html#the-ideas-in-code",
    "title": "Using Time to Measure Causal Effects",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nWe use several different filterings of the checkout data frame to do the analyses above. Here we define two of them, checkout_its (which excludes pure controls) and checkout_w_control (which excludes timepoints besides those immediately before and after treatment and relabels these timepoints as ‘pre’ and ‘post’).\n\nlibrary(stat20data)\ncheckout_its &lt;- checkout |&gt;\n  filter(!store_id %in% c(244,248)) \ncheckout_w_control &lt;- checkout |&gt;\n  filter(time &gt; 13 & time &lt; 16) |&gt;\n  mutate('time' = factor(time, labels = c('pre','post'))) |&gt;\n  select(store_id, sales, time, checkout_policy)\n\nTo make plots for panel data designs, it is often helpful to use the geom_line() geometry, mapping the unique subject ID to the group argument to get a separate line for each unit.\n\ncheckout_its |&gt;\n  ggplot(mapping = aes(x = time, y = sales, group = store_id)) + \n  geom_line() + labs(title = 'Sales of unhealthy foods did not show a dramatic time trend',\n                     x = 'Time (4-week periods)',\n                     y = 'Sales (number of packages sold)') + \n  geom_vline(xintercept = 15, linetype = 2) +\n  annotate('text', label = 'Checkout policy enacted', x = 15, y = 50)\n\n\n\n\n\n\n\nTo do a hypothesis test or create a confidence interval, we must usually begin by transforming the data frame so each row represents a different unique subject. The pivot_wider command does this, creating new pre- and post- versions of our other variables for ecah store. We call the new data frame checkout_by_store. This code chunk shows how we can calculate the effect estimates from store-specific sales differences using infer commands.\n\n#create new data frame with one row per store and pre/post sales difference\ncheckout_by_store &lt;- checkout_w_control |&gt;\n  pivot_wider(names_from = time, values_from = c(sales,checkout_policy)) |&gt;\n  mutate('ever_treated' = checkout_policy_post, \n         'sales_diff' = sales_post - sales_pre)\n\n\ncheckout_by_store |&gt;\n  group_by(ever_treated) |&gt;\n  summarize('Average Pre/Post Difference in Sales' = mean(sales_diff))\n\n# A tibble: 2 × 2\n  ever_treated `Average Pre/Post Difference in Sales`\n  &lt;lgl&gt;                                         &lt;dbl&gt;\n1 FALSE                                         -655 \n2 TRUE                                          -683.\n\ncheckout_by_store |&gt;\n  specify(response = sales_diff, \n          explanatory = ever_treated) |&gt;\n  calculate(stat = 'diff in means', order = c('TRUE', 'FALSE'))\n\nResponse: sales_diff (numeric)\nExplanatory: ever_treated (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 -28.2\n\n\nThis code chunk shows how we can create a bootstrap confidence interval for our difference in difference estimate by resampling store differences.\n\nset.seed(2024-3-28)\nbootstrap_did &lt;- checkout_by_store |&gt;\n  specify(response = sales_diff, \n          explanatory = ever_treated) |&gt;\n  generate(reps = 500, type = 'bootstrap') |&gt;\n  calculate(stat = 'diff in means', order = c('TRUE', 'FALSE'))\n\nbootstrap_confint &lt;- bootstrap_did |&gt;\n  get_confidence_interval(level = 0.95)\n\nbootstrap_did |&gt;\n  visualize() +\n  shade_confidence_interval(bootstrap_confint)\n\nIf we want a hypothesis test instead, we could instead specify an independence null hypothesis for checkout_by_store with the same response and explanatory variables, and generate null draws with generate() using the type = permute argument."
  },
  {
    "objectID": "5-causation/04-time/notes.html#summary",
    "href": "5-causation/04-time/notes.html#summary",
    "title": "Using Time to Measure Causal Effects",
    "section": "Summary",
    "text": "Summary\nRepeated observations for the same subject across time can be a powerful tool for developing arguments about causal claims. When all units are measured once before and once after treatment, a pre/post design helps address concerns about unobserved confounders as long as the counterfactual outcomes and any unobserved confounders remain the same across time. Even when there is some effect of time on the outcome, we can attempt to estimate that time effect and separate it from our effect estimate, either by using additional measurements for each subject (interrupted time series) or by using subjects who never receive treatment (difference-in-difference). Like any non-randomized study, these approaches require assumptions about the true process that generated the data that you should think carefully about. It is also important to be careful about confidence interval construction for designs with repeated measures to make sure bootstrapped datasets are similar to the original data."
  },
  {
    "objectID": "5-causation/04-time/notes.html#footnotes",
    "href": "5-causation/04-time/notes.html#footnotes",
    "title": "Using Time to Measure Causal Effects",
    "section": "Footnotes",
    "text": "Footnotes\n\nImage from Shutterstock/SpeedKingz, obtained from Pawlowski, A. (Dec 2015). Kids can’t resist candy? Stores try junk food-free ‘healthy checkout lanes.’ Today.com. https://www.today.com/parents/kids-cant-resist-candy-stores-try-junk-food-free-healthy-t60621.↩︎\nData is a modified version of data used in Ejlerskov KT, Sharp SJ, Stead M, Adamson AJ, White M, Adams J. Supermarket policies on less-healthy food at checkouts: Natural experimental evaluation using interrupted time series analyses of purchases. PLoS Med. 2018 Dec 18;15(12). Accessed via a repository described in Turner, S. L., Korevaar, E., Karahalios, A., Forbes, A. B., & McKenzie, J. E. (2023). Interrupted time series datasets from studies investigating the impact of interventions or exposures in public health and social science: A data note. Available at ResearchSquare, https://doi.org/10.21203/rs.3.rs-3669411/v1.↩︎\nImage from Haber, N. A., Clarke-Deelder, E., Salomon, J. A., Feller, A., & Stuart, E. A. (2021). Impact evaluation of coronavirus disease 2019 policy: A guide to common design issues. , 190(11), 2474-2486.↩︎\nThe original version of the supermarket checkout data does not actually contain pure controls, so observations for two control units have been synthetically generated for illustrative purposes.↩︎\nWe still might feel better about believing the parallel trends assumptions if pure control stores looked generally similar to treated stores in size and in other attributes. If we had a larger set of pure controls available and more store covariates measured, we could check covariate balance and conduct matching first, then run the difference-in-difference design on the pre/post data from the matched subjects only.↩︎\nFor interrupted time series, the regression model we estimate cannot easily be written in terms of unit differences. Hypothesis testing and confidence intervals require instead that groups of observations be permuted or resampled in a specific pattern, using tools such as clustered standard errors or the block bootstrap. These more complex procedures cannot be carried out using the infer package and we won’t cover them in this class.↩︎"
  },
  {
    "objectID": "5-causation/labs/06-taste-test/lab.html",
    "href": "5-causation/labs/06-taste-test/lab.html",
    "title": "Lab 7: A Matter of Taste",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "5-causation/labs/06-taste-test/lab.html#part-i-understanding-the-context-of-the-data",
    "href": "5-causation/labs/06-taste-test/lab.html#part-i-understanding-the-context-of-the-data",
    "title": "Lab 7: A Matter of Taste",
    "section": "Part I: Understanding the Context of the Data",
    "text": "Part I: Understanding the Context of the Data\n\nLab 6.1: A Matter of Taste"
  },
  {
    "objectID": "5-causation/labs/06-taste-test/lab.html#part-ii-computing-on-the-data",
    "href": "5-causation/labs/06-taste-test/lab.html#part-ii-computing-on-the-data",
    "title": "Lab 7: A Matter of Taste",
    "section": "Part II: Computing on the Data",
    "text": "Part II: Computing on the Data\nBe sure to tag your groupmates when you submit this to Gradescope!\n\nList any changes that you made to your experimental protocol from when you formulated it on Tuesday/Wednesday and when you executed it on Thursday/Friday.\nCreate a data frame based of the data you collected, listed in the order in which it was collected, and print it out into your lab report. You can print all rows your data frame using slice(my_df, 1:100). Consult the notes “A Tool for Computing with Data” for a refresher of how to make a data frame. Does the data frame differ at all from the one that you sketched into your experimental protocol? If so, how?\nCreate a visualization of the data you collected (not the null distribution) similar to the one you sketched in the handout. Does it look clearly in support of your claim or contrary to your claim or somewhere in between?\nConsidering the order in which the data was collected as a numeric covariate, (i.e. 1 for the first observation, 2 for the second, etc.), compute and report its standardized mean difference. Conduct a hypothesis test to check whether run order differs significantly across your experimental groups and interpret the results.\n\nConduct a hypothesis test to determine whether your data is consistent with the null hypothesis. Be sure to provide.\n\nThe null and alternative hypotheses.\nThe value of the observed test statistic.\nA visualization of the null distribution and observed test statistic with the p-value shaded in.\nThe p-value and your conclusion (based on the \\(\\alpha\\)-value you selected in Lab 7.1) regarding the null hypothesis and the original claim.\n\n\nA thought experiment: if you did not find a significant effect, speculate as to what you could change about your protocol to increase the chance that you find an effect. If you did find a significant effect, speculate as to what you would change about your protocol if you wanted to decrease the chance that you’d find an effect if you were to repeat the experiment."
  },
  {
    "objectID": "staff-guide/pre-semester-checklist.html",
    "href": "staff-guide/pre-semester-checklist.html",
    "title": "Pre-semester Checklist",
    "section": "",
    "text": "Staffing\n\n\nCommunication and web infrastructure\nGoogle Drive is used primarily to author and collect data via Forms, record meeting notes using Docs, and store and share logistical information using the Stat 20 Logistics Sheet. Slack is used as the staff communication platform. Ed is used as the student communication platform. Gradescope is used to distribute, collect, and grade assignments. bCourses is used only for it’s administrative role: it pulls the enrollment information from the registrar and syncs it with Ed and Gradescope. Otherwise, it is not used.\n\nCopy the previous semester’s Google Drive folder and rename it to the current semester. Thin out any unneeded files.\nCreate staff Slack.\nCreate Stat 20 bCourses site (one site for all sections).\nCreate Gradescope site from bCourses site.\n\nCreate new course on Gradescope.\nIn course settings, link to canvas course and select the bCourses site.\nUnder roster, click Sync Canvas Roster to pull in instructors and students.\n\nCreate a class forum. To use Ed, see this FAQ for how to set it up from bCourses.\n\n\n\nCourse Website\n\nUpdate _course-settings.yml (see Course Settings for details).\n\nUpdate assignment dates for new semester.\nMove notes and assignments in and out of an archive as needed.\nUpdate instructor info.\nCheck that assignment release dates are as desired.\n\nUpdate external links. Go into _quarto.yml and update the href fields with the new urls.\n right:\n   - text: \"\"\n     href: https://edstem.org/us/courses/31657\n     aria-label: Ed Discussion Forum\n   - text: \"\"\n     href: https://www.gradescope.com/courses/416233\n     aria-label: Gradescope\n   - text: \"\"\n     href: https://stat20.datahub.berkeley.edu/\n     aria-label: RStudio\n\n\n\nWeek one of classes\n\nHold (full) staff meeting\nSend out greeting via Ed.\n\n\n\nComputing Platform"
  },
  {
    "objectID": "staff-guide/grading.html",
    "href": "staff-guide/grading.html",
    "title": "Grading",
    "section": "",
    "text": "Readers meet with the head reader once each week for 1 hour. Those meeting generally involve:\n\nCheck-ins on the previous week’s grading\nGroup sprints to finish up almost-done assignments\nIntroduction to new labs: the context, major questions, type of R code in use.\n20-30 min segments where you grade through assignments longitudinally to test out the rubric and discuss."
  },
  {
    "objectID": "staff-guide/grading.html#reader-meeting",
    "href": "staff-guide/grading.html#reader-meeting",
    "title": "Grading",
    "section": "",
    "text": "Readers meet with the head reader once each week for 1 hour. Those meeting generally involve:\n\nCheck-ins on the previous week’s grading\nGroup sprints to finish up almost-done assignments\nIntroduction to new labs: the context, major questions, type of R code in use.\n20-30 min segments where you grade through assignments longitudinally to test out the rubric and discuss."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Lab assignments are an opportunity to put the concepts from the notes into practice to answer questions about a real data set. Your lab report should be a pdf file generated from a fully reproducible qmd file. For a helpful R reference, see base R, data visualization (ggplot2), and data wrangling (dplyr).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "assignments.html#labs",
    "href": "assignments.html#labs",
    "title": "Assignments",
    "section": "",
    "text": "Lab assignments are an opportunity to put the concepts from the notes into practice to answer questions about a real data set. Your lab report should be a pdf file generated from a fully reproducible qmd file. For a helpful R reference, see base R, data visualization (ggplot2), and data wrangling (dplyr).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "assignments.html#problem-sets",
    "href": "assignments.html#problem-sets",
    "title": "Assignments",
    "section": "Problem Sets",
    "text": "Problem Sets\nProblem sets are than the worksheets given to you in class. The goal of the problems sets is simply practice: they help you drill the techniques needed to complete the labs and learn the material.\n\n\n\nPS 1: Problem Set 1\n\n\n\nNo matching items"
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/slides.html",
    "href": "4-generalization/06-wrong-by-design/slides.html",
    "title": "Wrong By Design",
    "section": "",
    "text": "Concept Questions\nProblem Set 15"
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/slides.html#agenda",
    "href": "4-generalization/06-wrong-by-design/slides.html#agenda",
    "title": "Wrong By Design",
    "section": "",
    "text": "Concept Questions\nProblem Set 15"
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/slides.html#section",
    "href": "4-generalization/06-wrong-by-design/slides.html#section",
    "title": "Wrong By Design",
    "section": "",
    "text": "Instead of constructing a confidence interval to learn about the parameter, we could assert the value of a parameter and see whether it is consistent with the data using a hypothesis test. Say you are interested in testing whether there is a clear majority opinion of support or opposition to the project.\n\nWhat are the null and alternative hypotheses?\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nThe null is the p = .5 and the alternative is that p != .5.\nThis brings up a good discussion of one- and two-tailed tests. When students share their answers, you can draw a picture of a null distribution on the board with the observed statistic as a vertical line and consider how the p-value calculation would change depending on the alternative hypothesis.\nThis also brings up a discussion of the link between hypothesis tests and confidence intervals. Below the picture of the null distribution on the board, you could draw a bootstrap distribution centered on the vertical line of the observed statistic. In the HT setting, you consider the location of the statistic relative to the null distribution. In making a decision with a CI, you consider the location of the parameter relative to the bootstrap distribution (or more generally, the sampling distribution of the statistic)."
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/slides.html#section-1",
    "href": "4-generalization/06-wrong-by-design/slides.html#section-1",
    "title": "Wrong By Design",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(infer)\nlibrary(stat20data)\n\nppk &lt;- ppk |&gt;\n  mutate(support_before = Q18_words %in% c(\"Somewhat support\", \n                                          \"Strongly support\",\n                                          \"Very strongly support\"))"
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/slides.html#section-2",
    "href": "4-generalization/06-wrong-by-design/slides.html#section-2",
    "title": "Wrong By Design",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(infer)\nlibrary(stat20data)\n\nppk &lt;- ppk |&gt;\n  mutate(support_before = Q18_words %in% c(\"Somewhat support\", \n                                          \"Strongly support\",\n                                          \"Very strongly support\"))\nobs_stat &lt;- ppk |&gt;\n  specify(response = support_before,\n          success = \"TRUE\") |&gt;\n  calculate(stat = \"prop\")"
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/slides.html#section-3",
    "href": "4-generalization/06-wrong-by-design/slides.html#section-3",
    "title": "Wrong By Design",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(infer)\nlibrary(stat20data)\n\nppk &lt;- ppk |&gt;\n  mutate(support_before = Q18_words %in% c(\"Somewhat support\", \n                                          \"Strongly support\",\n                                          \"Very strongly support\"))\nobs_stat &lt;- ppk |&gt;\n  specify(response = support_before,\n          success = \"TRUE\") |&gt;\n  calculate(stat = \"prop\")\nobs_stat\n\nResponse: support_before (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 0.339"
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/slides.html#section-4",
    "href": "4-generalization/06-wrong-by-design/slides.html#section-4",
    "title": "Wrong By Design",
    "section": "",
    "text": "Codenull &lt;- ppk |&gt;\n  specify(response = support_before,\n          success = \"TRUE\") |&gt;\n  hypothesize(null = \"point\", p = .5) |&gt;\n  generate(reps = 500, type = \"draw\") |&gt;\n  calculate(stat = \"prop\")"
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/slides.html#section-5",
    "href": "4-generalization/06-wrong-by-design/slides.html#section-5",
    "title": "Wrong By Design",
    "section": "",
    "text": "Codenull &lt;- ppk |&gt;\n  specify(response = support_before,\n          success = \"TRUE\") |&gt;\n  hypothesize(null = \"point\", p = .5) |&gt;\n  generate(reps = 500, type = \"draw\") |&gt;\n  calculate(stat = \"prop\")\nnull\n\nResponse: support_before (factor)\nNull Hypothesis: point\n# A tibble: 500 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1 0.516\n 2         2 0.495\n 3         3 0.497\n 4         4 0.517\n 5         5 0.493\n 6         6 0.508\n 7         7 0.496\n 8         8 0.502\n 9         9 0.496\n10        10 0.485\n# ℹ 490 more rows"
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/slides.html#section-6",
    "href": "4-generalization/06-wrong-by-design/slides.html#section-6",
    "title": "Wrong By Design",
    "section": "",
    "text": "Codenull &lt;- ppk |&gt;\n  specify(response = support_before,\n          success = \"TRUE\") |&gt;\n  hypothesize(null = \"point\", p = .5) |&gt;\n  generate(reps = 500, type = \"draw\") |&gt;\n  calculate(stat = \"prop\")\nvisualize(null) +\n  shade_p_value(obs_stat, direction = \"both\")"
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/slides.html#section-7",
    "href": "4-generalization/06-wrong-by-design/slides.html#section-7",
    "title": "Wrong By Design",
    "section": "",
    "text": "What would a Type I error be in this context?\n\n\nAnswer: Concluding that there is a clear majority when in fact there is an even split.\n\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/slides.html#section-8",
    "href": "4-generalization/06-wrong-by-design/slides.html#section-8",
    "title": "Wrong By Design",
    "section": "",
    "text": "What would a Type II error be in this context?\n\n\nAnswer: Concluding that there is an even split when in fact there is a clear majority.\nAfter the previous question, students should get this quite easily."
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/ps.html",
    "href": "4-generalization/05-hypothesis-tests-2/ps.html",
    "title": "Hypothesis Tests II",
    "section": "",
    "text": "Case Study 1\nIn order to study gender bias in promotion decisions, researchers took 48 resumes and randomly assigned them to 48 bank managers who were asked whether they would promote this candidate or not. The resumes were identical except the name: 24 of them had names generally associated with women, 24 of them had names generally associated with men.\nThe data from this study can be found in the promote data frame in the stat20data library. Did gender played a role in promotion decisions?\n\nWrite down an expression for the null and alternative hypothesis. Is \\(H_0\\) an “independence” null hypothesis or a point null hypothesis?\n\n\n\n\nWrite an infer pipeline to compute the observed test statistic.\n\n\n\n\n\n\nWhat type of plot can be used to visualize the observed data? Sketch a possibility for what the observed data might look like if it were consistent with the null hypothesis.\n\n\n\n\n\n\n\n\n\n\nWrite an infer pipeline to construct and save the null distribution of statistics.\n\n\n\n\n\n\n\n\nCompute the p-value using an infer pipeline with the null distribution from the previous part.\n\n\n\n\n\n\nInterpret the p-value. What does it say about the consistency between the null hypothesis and the observed data?\n\n\n\nCase Study 2\nIn the 2016 NBA season, it was noted that professional basketball player Steph Curry had a remarkable basket-shooting performance beyond 30 feet. The curry data frame within the stat20data library contains his long range shooting performance across 27 attempts. By comparison, the long range shooting percentage of NBA players that season was 7.5%.\nAssess whether this data is consistent with the notion that Steph Curry has a long range shooting percentage that is no different from the average NBA player. Said another way, assess just how remarkable Curry’s shooting performance was in 2016.\n\nWrite down an expression for the null and alternative hypothesis. Is \\(H_0\\) an “independence” null hypothesis or a point null hypothesis?\n\n\n\n\nWrite an infer pipeline to compute the observed test statistic.\n\n\n\n\n\n\nWrite an infer and ggplot2 pipeline to construct a plot featuring 9 subplots, each one featuring a visualization of a data set generated under the null hypothesis.\n\n\n\n\n\n\nDoes your visualization of the observed data from the previous part look like it could be one of these plots?\n\n\n\n\nVisualize the null distribution using an infer pipeline.\n\n\n\n\n\n\nSketch the null distribution that you found below, labeling axes and shading the area corresponding to areas considered “extreme”.\n\n\n\n\n\n\n\n\n\n\nWhat does your visualization say about the consistency between the null hypothesis and the observed data?"
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/notes.html",
    "href": "4-generalization/05-hypothesis-tests-2/notes.html",
    "title": "Hypothesis Tests II",
    "section": "",
    "text": "Hypothesis tests are a tool for assessing the consistency between data and a proposed model for how the data was generated. There are myriad hypothesis tests, but they all follow the same basic structure.\nIn the last set of notes, we learned about one tool for step 3: shuffling. When the null hypothesis asserts that two variables have nothing to do with one another, then we can simulate other possible data sets by shuffling one of the columns. This approach, called a permutation test, is useful when one of the variables defines two groups. In the case of Kristen Gilbert, shuffling allowed us to simulate worlds where deaths in the ward were just as likely as not to fall into the group of shifts when Gilbert was working.\nIn these notes, you’ll learn a method for simulating data that corresponds to a different class of null hypotheses. The method will look familiar: it can be thought of as another version of the box model we used for random variables. Also familiar is our first example: Benford’s Law and voting data from Iran."
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/notes.html#test-of-many-proportions",
    "href": "4-generalization/05-hypothesis-tests-2/notes.html#test-of-many-proportions",
    "title": "Hypothesis Tests II",
    "section": "Test of Many Proportions",
    "text": "Test of Many Proportions\n2009 Presidential Election in Iran\nIn 2009, Iran held a presidential election where the incumbent, Mahmoud Ahmadinejad, faced three challengers, Mohsen Rezai and two allied members of the reformist opposition, Mehdi Karroubi and Mir-Hussein Mousavi1. Leading up to the elections, polling for Mousavi and Karroubi was strong; they were considered to present the first serious challenge to Ahmadinejad’s governance. When the results of the election came in, any hopes for an upset were dashed by an decisive victory for the incumbent: Ahmadinejad received 62.6% of votes cast, Mousavi 33.75%, and Karroubi a bit above 1%.\n\n\n\n\nFrom left to right: Rezai, Ahmadinejad, Karroubi, and Mousavi.\n\nProtests broke out among the supporters of Mousavi and Karroubi, alleging that the results were fraudulent and manipulated by the incumbent to remain in power. The protests grew into the largest ever seen in the Republic of Iran and drew the attention of governments, journalists, and election watchers from across the world. One of these watchers, working from Poland, conducted an analysis that purported to find irregularities in the voting data2. They applied to the data a controversial analytical approach: Benford’s Law.\nBenford’s Law\nBenford’s Law is a mathematical law that describes a particular distribution of the digits 1 through 9. It is often a good approximation for the distribution of the first digits in sets of naturally occurring numbers that span many orders of magnitude, for example in the population counts of cities. According to Benford’s Law, the most common first digit should be a 1, at a frequency of 0.3, followed by 2 at 0.18, and decreasing until 9 at 0.05.\nThis pattern has been thought to apply to the first digit counts of voting data as well. The theory is that if the voting process is fair, then the distribution of the first digit of vote counts across different municipalities should follow Benford’s Law. Fraudulent activity such as ballot-stuffing, would materialize as deviations from Benford’s Law.\n\n\n\n\n\n\n\n\nThe election watchers studying the voting data from Iran noticed an anomaly in the distribution of first digits of vote counts for Karroubi (above right). The digit 7 was unexpectedly common, far more common than would be expected based on Benford’s Law. Is this evidence of voter fraud? Could this anomaly be just be due to chance?\nTaking Draws from the Null\nOne way to frame this analysis is as a hypothesis test. Under the null hypothesis, the first digit distribution of Karroubi’s vote counts was generated according to Benford’s Law. We can state this in the language of parameters, where \\(p_i\\) is the probability that a vote count has \\(i\\) as the first digit.\n\\[H_0: p_1 = .301, p_2 = .176, p_3 = .125, p_4 = .097, p_5 = .079, p_6 = .067, p_7 = .058, p_8 = .051, p_9 = .046\\]\nThe alternative hypothesis is that the first digits were drawn according to a different distribution (at least one of the \\(p_1\\) is different).\nThis null hypothesis describes everything we need to simulate the sort of data that we would observe in a world where first digits are drawn accordng to Benford’s Law. For this simulation, we can use the metaphor of a box with tickets. In our box we place 1000 tickets. 301 of them have the number 1 on them, 176 of them have the number 2, and so on until the 46 tickets that have the number 9. We can simulate one first digit from the vote count of one municipality by drawing a single ticket with replacement. To simulate a process akin to the voting data from Iran, we would draw 366 tickets with replacement, one for each municipality.\nWhile we could indeed do this process by hand (sacrificing both index cards and time), we will opt instead to use a computer. Below are nine different first digit distributions that we might see in a world where the first digits follow Benford’s Law.\n\nlibrary(infer)\n\np_benfords &lt;- c(\"1\" = log10(1 + 1/1),\n                \"2\" = log10(1 + 1/2),\n                \"3\" = log10(1 + 1/3),\n                \"4\" = log10(1 + 1/4),\n                \"5\" = log10(1 + 1/5),\n                \"6\" = log10(1 + 1/6),\n                \"7\" = log10(1 + 1/7),\n                \"8\" = log10(1 + 1/8),\n                \"9\" = log10(1 + 1/9))\n\nset.seed(30)\ndraw_9 &lt;- iran |&gt;\n  specify(response = first_digit) |&gt;\n  hypothesize(null = \"point\", p = p_benfords) |&gt;\n  generate(reps = 9, type = \"draw\")\n\ndraw_9 |&gt;\n  ggplot(aes(x = first_digit)) +\n  geom_bar() +\n  facet_wrap(vars(replicate)) +\n  labs(x = \"First Digit\")\n\n\n\n\n\n\n\nWe can see that there is some natural variation in the distribution just due to chance. While the first sample of 366 digits follows the decaying shape of Benford’s Law quite closely, the sixth has an unusually large number of 3s. The fifth sample, like the observed sample of Karroubi’s, has an unusually large number of 7s.\nAt this point, we could conduct a very informal version of a hypothesis test. Does the empirical distribution of Karroubi’s first digits look like one of these nine distributions generated according to Benford’s Law? It’s hard to say; the observed distribution is different from Benford’s Law. But how different is different enough?\nA Distance Statistic: \\(\\chi^2\\)\n\nWhen we discern a particular structure in a plot, we can quantify it precisely by calculating an appropriate summary statistic. If we’re looking at two box plots that are shifted from one another, we can calculate a difference in medians. If we’re looking at a scatter plot with a linear association, we can calculate a correlation coefficient.\nHere, the structure that we’re considering is the difference between two bar charts: the distribution according to Benford’s Law (above left) and the empirical distribution of Karroubi’s first digts (above right). There are several different statistics that could collapse this visual information into a single number. The most commonly used is the chi-squared statistic.\n\nChi-squared Statistic\n\nA statistic used to measure the distance between two categorical distributions, one observed and one expected. For a distribution with \\(k\\) categories, \\(O_i\\) observed counts in each category, and \\(E_i\\) expected counts, \\[\\chi^2 = \\sum_{i=1}^k \\frac{\\left(O_i - E_i \\right)^2}{E_i}\\]\n\n\nIn our setting, index \\(i\\) refers to each of the 9 digits. We could find the first term in the sum by taking the difference between the observed count of 1s, \\(O_1 = 125\\), and the count we would expect if those 366 digits followed Benford’s Law, \\(E_1 = 366 \\times .301 = 110.166\\). That difference \\(O_1 - E_1 = 125 - 110.166 = 14.834\\) captures the difference in the heights of the bars corresponding to 1 in the two bar charts. To complete that first term in the sum, we square it (to treat negative differences the same as positive differences) and then we divide that by \\(E_1\\) (so that the squared differences are relative to the height of that particular bar).\nThis process is repeated for each of the 9 digits, then the result added up to a single statistic. We’ll save the tedious arithmetic and let R calculate the chi-squared statistic that corresponds to Karroubi’s first digit distribution.\n\nobs_stat &lt;- iran |&gt;\n  specify(response = first_digit) |&gt;\n  hypothesize(null = \"point\", p = p_benfords) |&gt;\n  calculate(stat = \"Chisq\")\n\n\n\n[1] 28.0832\n\n\n28! That tells us that the distance from Karroubi’s distribution to Benford’s Law was 28! But . . . is 28 a lot? Or or a little? We don’t have any natural scale on which to make meaning out of this statistic.\nWe can however, compare this statistic to the statistics we would see in a world where the null hypothesis is true. Here are the chi-squared statistics for each of the 9 distributions above. Look through them one-by-one: what sort of chi-squared statistic do you get for distributions that are very similar to Benford’s? What about for ones that look different?\n\n\n\n\nreplicate\nstat\n\n\n\n1\n9.517228\n\n\n2\n8.122086\n\n\n3\n3.235929\n\n\n4\n3.647709\n\n\n5\n10.673920\n\n\n6\n17.632670\n\n\n7\n9.761574\n\n\n8\n6.901445\n\n\n9\n8.396731\n\n\n\n\n\nThis mode of thinking allows us to put our observed statistic of 28.1 into context. It is quite a bit larger than any of the nine statistics that we generated in a world where the first digits followed Benford’s Law. To be more thorough in our reasoning, though, we need to look at more than just nine statistics. We need to look at a full distribution of them.\nApproximating the null distribution\nWe can repeat the simulation process used above 5000 times and, for each of the 5000 simulated data sets, calculate a chi-squared statistic. Those 5000 statistics form a null distribution.\n\nnull &lt;- iran |&gt;\n  specify(response = first_digit) |&gt;\n  hypothesize(null = \"point\", p = p_benfords) |&gt;\n  generate(reps = 5000, type = \"draw\") |&gt;\n  calculate(stat = \"Chisq\")\n\nnull |&gt;\n  visualize() +\n  shade_p_value(obs_stat, direction = \"right\")\n\n\n\n\n\n\n\nWe see that, in a world where first digits follow Benford’s Law, while we would expect statistics around 8 or 9, it is possible to observe very small chi-squared statistics near zero and ones as high as about 20. The statistic that we actually observed was 28.1, indicated by the red line. On the scale of the null distribution, this is off the charts.\nWe can quantify the consistency between the observed statistic and the null hypothesis by calculating a p-value. Formally, it is estimated as the proportion of simulated null statistics that are as extreme or more so than the one that you observed. In this case, there are no statistics that matched or exceeded 28.1, so the p-value is essentially zero.\nEvidence of what?\nA low p-value indicates that our data - Karroubi’s official vote data out of Iran - is inconsistent with our null hypothesis - that Karroubi’s first digit counts follow Benford’s Law. But what does that say about election fraud?\nThe controversy around the application of Benford’s Law in situations like this centers on whether or not a fair election would actually be expected to generate first digits that look like Benford’s Law. Benford’s Law is a simple mathematical abstraction and elections are very particular things, with each one unfolding according to different policies and procedure, and each one aggregated at different levels (precincts, cities, counties, etc.).\n\nIf you repeat this analysis on first digit distributions from US elections, you’d find that some of them follow Benford’s Law very well. Others found deviations even more extreme than seen in Karroubi’s data. US election, for the most part are free of the sort of fraud that would show up in these analyses, so the appropriate conclusion is not that we detected evidence of fraud but rather that Benford’s Law simply isn’t a good fit for many of the processes that generate vote counts in elections.\nThis is an important lesson moving beyond descriptive statistics and into generalizations. When you lay out the null hypothesis, you are describing a complicated real world process with a simplified explanation. The success of that generalization depends in large part on the degree to which this simplification preserves the most important features of reality3."
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/notes.html#one-tail-or-two",
    "href": "4-generalization/05-hypothesis-tests-2/notes.html#one-tail-or-two",
    "title": "Hypothesis Tests II",
    "section": "One Tail or Two?",
    "text": "One Tail or Two?\nIn the United States vs. Kristen Gilbert, the null hypothesis that the statisticians operated under was that Gilbert was innocent. More specifically, it asserted that the occurrence of a death on a shift at the hospital was independent of whether or not Gilbert was working. This hypothesis can be stated in terms of parameters; that the probability of a death on a shift with Gilbert present is equal to the probability of a death on a shift without her present.\n\n\n\\(H_0\\): \\(p_{gilbert} - p_{no\\,gilbert} = 0\\)\n\n\nThere are actually three different ways that the statisticians could have set up their alternative hypotheses.\n\n\n\\(H_A\\): \\(p_{gilbert} - p_{no\\,gilbert} &lt; 0\\)\n\n\n\\(H_A\\): \\(p_{gilbert} - p_{no\\,gilbert} \\ne 0\\)\n\n\n\\(H_A\\): \\(p_{gilbert} - p_{no\\,gilbert} &gt; 0\\)\n\n\nThe first version seems surprising: we’re entertaining an alternate explanation where in fact fewer deaths would have occurred on Gilbert’s shifts. The second version is the one that we used in the previous notes: that it’s possible either more or fewer deaths occurred on Gilbert’s shifts.\nThe third version is very tempting. It disregards the extra alternative (that Gilbert could be associated with fewer deaths) and that seems natural since the data pointed in the opposite direction. However, there are two dangers if we ignore possibilities that disagree with our data or that conflict with our world view:\n\nFraming an alternative hypothesis simply to match the direction that the data point will inflate the type I error rate. After all the work we have done (and will continue to do) to rigorously control the error rates in hypothesis tests, careless construction of the alternative hypotheses can disrupt that hard work.\nIf we only use alternative hypotheses that agree with our worldview, then we are going to be subjecting ourselves to confirmation bias, which means we are looking for data that supports our ideas. That’s not very scientific, and we can do better!\n\nThe alternative hypotheses found in 1) and 3) define what are called one-sided hypothesis tests (also called “one-tailed”) because they only explore one direction of possibilities. Such hypotheses are appropriate when we are exclusively interested in the single direction, but usually we want to consider all possibilities. A better approach is to use two-sided hypothesis tests (also called “two-tailed”), defined by hypotheses in the form of 2).\nThe chi-squared test used for the Iran elections is an unusual case where the one-sided test is appropriate. The left tail of that distribution contains statistics that indicate strong consistency with the null hypothesis, so only the right tail is used to calculate the p-value."
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/notes.html#the-significance-level",
    "href": "4-generalization/05-hypothesis-tests-2/notes.html#the-significance-level",
    "title": "Hypothesis Tests II",
    "section": "The Significance Level",
    "text": "The Significance Level\nIf it is necessary to make a binary decision based on a p-value, it must be decided beforehand what level of evidence is needed before you would rule out the null hypothesis.\n\nSignificance level, \\(\\alpha\\)\n\nA number between 0 and 1 that serves as the threshold for the p-value. The null hypothesis is rejected when the p-value \\(&lt; \\alpha\\), and the finding is found “statistically significant”.\n\n\nBy convention, \\(\\alpha = 0.05\\), however you should adjust the significance level based on the application. Certain scientific fields might tend to use a slightly higher or lower threshold for what constitutes statistical significance. In a setting where the decisions have very different real-world consequences, those, too, can factor into the choice of \\(\\alpha\\)."
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/notes.html#summary",
    "href": "4-generalization/05-hypothesis-tests-2/notes.html#summary",
    "title": "Hypothesis Tests II",
    "section": "Summary",
    "text": "Summary\nIn these notes you learned about a second form of null hypothesis, one where you explicitly define the proportions of a categorical variable. This type of null hypothesis allows you to generate data through simulating the process of drawing tickets from a box that is designed to match your null hypothesis. The most common test statistic used in this scenario is the chi-squared statistic, which measures the distance between two categorical distributions. Just as in the permutation tests from last time, you can assess the consistency between the null hypothesis and your data by calculating a p-value: the probability of drawing a statistic as or more extreme than the one that you observed in a world where the null hypothesis is true."
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/notes.html#footnotes",
    "href": "4-generalization/05-hypothesis-tests-2/notes.html#footnotes",
    "title": "Hypothesis Tests II",
    "section": "Footnotes",
    "text": "Footnotes\n\nImage from Pyvand Iran News archives http://www.payvand.com/news/09/jun/1085.html.↩︎\nRoukema, Boudewijn F. (2014). “A first-digit anomaly in the 2009 Iranian presidential election”. Journal of Applied Statistics. 41: 164–199.↩︎\nFor more conversation around the application of Benford’s Law to the election in Iran (and elections in general), see a blog post by Andrew Gelman, a professor of statistics at Columbia University: Unconvincing (to me) use of Benford’s law to demonstrate election fraud in Iran.↩︎"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html",
    "href": "4-generalization/03-bootstrapping/slides.html",
    "title": "Bootstrapping",
    "section": "",
    "text": "If you’ve been given an index card, please write on it:\n\nYour first name\nYour year at Cal (1 is first year, 2 is second year, etc)\nWhether or not you are interested in majoring in a business- or econ-related field. 1 = yes, 0 = no\n\n\nAs students arrive, randomly give ~8 of them an index card."
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#while-youre-waiting",
    "href": "4-generalization/03-bootstrapping/slides.html#while-youre-waiting",
    "title": "Bootstrapping",
    "section": "",
    "text": "If you’ve been given an index card, please write on it:\n\nYour first name\nYour year at Cal (1 is first year, 2 is second year, etc)\nWhether or not you are interested in majoring in a business- or econ-related field. 1 = yes, 0 = no\n\n\nAs students arrive, randomly give ~8 of them an index card."
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#agenda",
    "href": "4-generalization/03-bootstrapping/slides.html#agenda",
    "title": "Bootstrapping",
    "section": "Agenda",
    "text": "Agenda\n\nConcept Question\nActivity: The Bootstrap\nPS: The Bootstrap\nBootstrapping with infer"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#section",
    "href": "4-generalization/03-bootstrapping/slides.html#section",
    "title": "Bootstrapping",
    "section": "",
    "text": "Which of these is a valid bootstrap sample?\n\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nCodelibrary(tidyverse)\nlibrary(palmerpenguins)\ndata(penguins)\n\nset.seed(30)\na &lt;- penguins |&gt;\n  sample_n(5) |&gt;\n  mutate(name = c(\"Gus\", \"Luz\", \"Ida\", \"Ola\", \"Abe\")) |&gt;\n  rename(length = bill_length_mm) |&gt;\n  select(name, species, length)\n\n\n\n\n\n\nCodelibrary(kableExtra)\nkable(a, caption = \"Original Sample\")\n\n\nOriginal Sample\n\nname\nspecies\nlength\n\n\n\nGus\nChinstrap\n50.7\n\n\nLuz\nGentoo\n48.5\n\n\nIda\nChinstrap\n52.8\n\n\nOla\nGentoo\n44.5\n\n\nAbe\nAdelie\n42.0\n\n\n\n\n\n\n\nCodea |&gt;\n  slice_sample(prop = 1, replace = T) |&gt;\n  kable(caption = \"BS A\")\n\n\nBS A\n\nname\nspecies\nlength\n\n\n\nIda\nChinstrap\n52.8\n\n\nLuz\nGentoo\n48.5\n\n\nAbe\nAdelie\n42.0\n\n\nOla\nGentoo\n44.5\n\n\nIda\nChinstrap\n52.8\n\n\n\n\nCodea |&gt;\n  slice_sample(n = 6, replace = T) |&gt;\n  kable(caption = \"BS B\")\n\n\nBS B\n\nname\nspecies\nlength\n\n\n\nOla\nGentoo\n44.5\n\n\nGus\nChinstrap\n50.7\n\n\nIda\nChinstrap\n52.8\n\n\nLuz\nGentoo\n48.5\n\n\nGus\nChinstrap\n50.7\n\n\nGus\nChinstrap\n50.7\n\n\n\n\n\n\n\nCodenames_b &lt;- a |&gt;\n  slice_sample(prop = 1, replace = T) |&gt;\n  pull(name)\na |&gt;\n  mutate(name = names_b) |&gt;\n  kable(caption = \"BS C\")\n\n\nBS C\n\nname\nspecies\nlength\n\n\n\nGus\nChinstrap\n50.7\n\n\nOla\nGentoo\n48.5\n\n\nOla\nChinstrap\n52.8\n\n\nIda\nGentoo\n44.5\n\n\nIda\nAdelie\n42.0\n\n\n\n\nCodea |&gt;\n  slice_sample(prop = 1, replace = T) |&gt;\n  kable(caption = \"BS D\")\n\n\nBS D\n\nname\nspecies\nlength\n\n\n\nGus\nChinstrap\n50.7\n\n\nAbe\nAdelie\n42.0\n\n\nGus\nChinstrap\n50.7\n\n\nGus\nChinstrap\n50.7\n\n\nGus\nChinstrap\n50.7"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#parameters-and-statistics",
    "href": "4-generalization/03-bootstrapping/slides.html#parameters-and-statistics",
    "title": "Bootstrapping",
    "section": "Parameters and Statistics",
    "text": "Parameters and Statistics\n\nOur Goal: Assess the sampling error / variability in our estimate of the median year at Cal and the proportion of students in an econ-related field.\n. . .\n\nOur Tool: The Bootstrap"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#collecting-a-sample-of-data",
    "href": "4-generalization/03-bootstrapping/slides.html#collecting-a-sample-of-data",
    "title": "Bootstrapping",
    "section": "Collecting a sample of data",
    "text": "Collecting a sample of data\nIf you’ve been given an index card, please write on it:\n\nYour first name\nYour year at Cal (1 is first year, 2 is second year, etc)\nWhether or not you are interested in majoring in a business- or econ-related field. 1 = yes, 0 = no"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#section-3",
    "href": "4-generalization/03-bootstrapping/slides.html#section-3",
    "title": "Bootstrapping",
    "section": "",
    "text": "boardwork\n\nCollect index cards from students and record the data into a data frame on the board labelled “Observed sample”. Calculate the sample median and sample proportion of econ-related majors.\nAsk for a volunteer to generate the first bootstrap sample. Hand them the stack of cards and have them randomly choose a single card and read off the data to you. As they do so, write out the first row of a “Bootstrap Sample 1” data frame on the board. Be sure to label the row with the student name - that helps emphasis when there are repeats.Have them return the card to the deck, shuffle, and randomly choose a card and read off the data. Repeat until you have filled out the same number of rows as in the original data set. Calculate the median and proportion (you may want to write dplyr code to do this using summarize()).\nAsk for a second volunteer to generate the second bootstrap sample. Repeat the procedure as before, drawing a third data frame on the board and computing a second set of statistics (median and proportion).\nCollect the bootstrapped medians and proportions and sketch them as the first few points in a broader density plot that we’ll be able to see when we take more and more bootstrap samples. Label this as the “bootstrap distribution” and speak of it as an approximation to the true sampling distribution. You can explain the 1 - alpha bootstrap interval as the interval that captures the middle 95% of bootstrapped statistics."
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#example-penguins",
    "href": "4-generalization/03-bootstrapping/slides.html#example-penguins",
    "title": "Bootstrapping",
    "section": "Example: Penguins",
    "text": "Example: Penguins\nLet’s consider our 344 penguins to be a SRS from the broader population of Antarctic penguins. What is a point and interval estimate for the population proportion of penguins that are Adelie?\n\n\nCodelibrary(tidyverse)\nlibrary(palmerpenguins)\ndata(penguins)\n\n\n. . .\n\n\n\nCodepenguins &lt;- penguins |&gt;\n  mutate(is_adelie = species == \"Adelie\")\n\npenguins |&gt;\n  ggplot(aes(x = is_adelie)) +\n  geom_bar()"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#point-estimate",
    "href": "4-generalization/03-bootstrapping/slides.html#point-estimate",
    "title": "Bootstrapping",
    "section": "Point estimate",
    "text": "Point estimate\n\nCodeobs_stat &lt;- penguins |&gt;\n  summarize(p_adelie = mean(is_adelie))\nobs_stat\n\n# A tibble: 1 × 1\n  p_adelie\n     &lt;dbl&gt;\n1    0.442"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#generating-one-bootstrap-sample",
    "href": "4-generalization/03-bootstrapping/slides.html#generating-one-bootstrap-sample",
    "title": "Bootstrapping",
    "section": "Generating one bootstrap sample",
    "text": "Generating one bootstrap sample\n\nCodelibrary(infer)\npenguins |&gt;\n  specify(response = is_adelie,\n          success = \"TRUE\") |&gt;\n  generate(reps = 1, \n           type = \"bootstrap\")\n\n\n. . .\n\n\nResponse: is_adelie (factor)\n# A tibble: 344 × 2\n# Groups:   replicate [1]\n   replicate is_adelie\n       &lt;int&gt; &lt;fct&gt;    \n 1         1 FALSE    \n 2         1 FALSE    \n 3         1 TRUE     \n 4         1 FALSE    \n 5         1 TRUE     \n 6         1 TRUE     \n 7         1 FALSE    \n 8         1 TRUE     \n 9         1 TRUE     \n10         1 TRUE     \n# ℹ 334 more rows"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#two-more-bootstrap-samples",
    "href": "4-generalization/03-bootstrapping/slides.html#two-more-bootstrap-samples",
    "title": "Bootstrapping",
    "section": "Two more bootstrap samples",
    "text": "Two more bootstrap samples\n\n\n\nCodepenguins |&gt;\n  specify(response = is_adelie,\n          success = \"TRUE\") |&gt;\n  generate(reps = 1, \n           type = \"bootstrap\")\n\nResponse: is_adelie (factor)\n# A tibble: 344 × 2\n# Groups:   replicate [1]\n   replicate is_adelie\n       &lt;int&gt; &lt;fct&gt;    \n 1         1 FALSE    \n 2         1 TRUE     \n 3         1 FALSE    \n 4         1 FALSE    \n 5         1 FALSE    \n 6         1 TRUE     \n 7         1 TRUE     \n 8         1 FALSE    \n 9         1 FALSE    \n10         1 FALSE    \n# ℹ 334 more rows\n\n\n\n\n\nCodepenguins |&gt;\n  specify(response = is_adelie,\n          success = \"TRUE\") |&gt;\n  generate(reps = 1, \n           type = \"bootstrap\")\n\nResponse: is_adelie (factor)\n# A tibble: 344 × 2\n# Groups:   replicate [1]\n   replicate is_adelie\n       &lt;int&gt; &lt;fct&gt;    \n 1         1 FALSE    \n 2         1 TRUE     \n 3         1 TRUE     \n 4         1 FALSE    \n 5         1 FALSE    \n 6         1 TRUE     \n 7         1 TRUE     \n 8         1 FALSE    \n 9         1 FALSE    \n10         1 FALSE    \n# ℹ 334 more rows"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#visualizing-9-bs-samples",
    "href": "4-generalization/03-bootstrapping/slides.html#visualizing-9-bs-samples",
    "title": "Bootstrapping",
    "section": "Visualizing 9 bs samples",
    "text": "Visualizing 9 bs samples\n\n\n\nCodepenguins |&gt;\n  specify(response = is_adelie,\n          success = \"TRUE\") |&gt;\n  generate(reps = 9, \n           type = \"bootstrap\") |&gt;\n  ggplot(aes(x = is_adelie)) +\n  geom_bar() +\n  facet_wrap(vars(replicate),\n             nrow = 3)"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#calculating-9-hatp",
    "href": "4-generalization/03-bootstrapping/slides.html#calculating-9-hatp",
    "title": "Bootstrapping",
    "section": "Calculating 9 \\(\\hat{p}\\)\n",
    "text": "Calculating 9 \\(\\hat{p}\\)\n\n\n\n\nCodepenguins |&gt;\n  specify(response = is_adelie,\n          success = \"TRUE\") |&gt;\n  generate(reps = 9, \n           type = \"bootstrap\") |&gt;\n  calculate(stat = \"prop\")\n\n\n\n\n\n\nResponse: is_adelie (factor)\n# A tibble: 9 × 2\n  replicate  stat\n      &lt;int&gt; &lt;dbl&gt;\n1         1 0.404\n2         2 0.430\n3         3 0.404\n4         4 0.433\n5         5 0.468\n6         6 0.448\n7         7 0.427\n8         8 0.413\n9         9 0.474\n\n\n\n\n. . .\n\nNote the change in data frame size."
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#the-bootstrap-dist-reps-500",
    "href": "4-generalization/03-bootstrapping/slides.html#the-bootstrap-dist-reps-500",
    "title": "Bootstrapping",
    "section": "The bootstrap dist (reps = 500)",
    "text": "The bootstrap dist (reps = 500)\n. . .\n\n\n\nCodepenguins |&gt;\n  specify(response = is_adelie,\n          success = \"TRUE\") |&gt;\n  generate(reps = 500, \n           type = \"bootstrap\") |&gt;\n  calculate(stat = \"prop\") |&gt;\n  ggplot(aes(x = stat)) +\n  geom_histogram()"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#interval-estimate",
    "href": "4-generalization/03-bootstrapping/slides.html#interval-estimate",
    "title": "Bootstrapping",
    "section": "Interval Estimate",
    "text": "Interval Estimate\n. . .\nWe can extract the middle 95% by identifying the .025 quantile and the .975 quantile of the bootstrap distribution with get_ci().\n\nCodepenguins |&gt;\n  specify(response = is_adelie,\n          success = \"TRUE\") |&gt;\n  generate(reps = 500, \n           type = \"bootstrap\") |&gt;\n  calculate(stat = \"prop\") |&gt;\n  get_ci(level = .95)\n\n\n. . .\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.392    0.494"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#documentation-infer.tidymodels.org",
    "href": "4-generalization/03-bootstrapping/slides.html#documentation-infer.tidymodels.org",
    "title": "Bootstrapping",
    "section": "Documentation: infer.tidymodels.org\n",
    "text": "Documentation: infer.tidymodels.org"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/slides.html#your-turn",
    "href": "4-generalization/03-bootstrapping/slides.html#your-turn",
    "title": "Bootstrapping",
    "section": "Your Turn",
    "text": "Your Turn\n\nCreate a 95% confidence interval for the median bill length of penguins.\n\n\nCodecountdown::countdown(5, font_size = \"2em\")\n\n\n\n−&plus;\n\n05:00"
  },
  {
    "objectID": "4-generalization/02-confidence-intervals/slides.html",
    "href": "4-generalization/02-confidence-intervals/slides.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "Concept Questions\nLab 5"
  },
  {
    "objectID": "4-generalization/02-confidence-intervals/slides.html#agenda",
    "href": "4-generalization/02-confidence-intervals/slides.html#agenda",
    "title": "Confidence Intervals",
    "section": "",
    "text": "Concept Questions\nLab 5"
  },
  {
    "objectID": "4-generalization/02-confidence-intervals/slides.html#section",
    "href": "4-generalization/02-confidence-intervals/slides.html#section",
    "title": "Confidence Intervals",
    "section": "",
    "text": "You embark on a mission to estimate a population mean using a simple random sample of \\(n\\) observations.\n\nWhat sample size would you need to increase the precision of your estimate by approximately 3x compared to the original sample?\n\n\nA. n + 3 B. 3n D. n + 6 C. 6n D. 9n\nThis assesses the 1/sqrt(n) relationship between the SE of a mean and the SD of the population. This relationship is worth reviewing. It’s also a good time to underline the definition of SE from the notes, and its link to our notion of “precision”.\n\n\nCodecountdown::countdown(1)\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "4-generalization/02-confidence-intervals/slides.html#section-1",
    "href": "4-generalization/02-confidence-intervals/slides.html#section-1",
    "title": "Confidence Intervals",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(stat20data)\n\nset.seed(5)\nflights %&gt;%\n  slice_sample(n = 30) %&gt;%\n  summarize(xbar = mean(air_time),\n            sx = sd(air_time),\n            n = n())\n\n# A tibble: 1 × 3\n   xbar    sx     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1  118.  81.5    30\n\n\n\nWhat is an approximate 95% confidence interval for the mean air time in flights using the normal curve?\n\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nA. 30 +/- 1.96 * 81.5 * sqrt(118) B. 118 +/- 1.68 * 81.5 / sqrt(30) C. 118 +/- 1.96 * 81.5 / sqrt(30) D. 30 +/- 1.68 * 118 / sqrt(81.5)\nAnswer: C. This is a simple application of the formula for a CI based on the normal approximation.\nThis is a good question to kick off a discussion of the interpretation of confidence intervals by recreating the figure from the notes of many CIs from many samples, 5% of which cover the parameter. You can sketch the empty plot on the board, add the CI from this question, then rerun the code in R (with no set seed), to get another point estimate and another interval… and so on until you demonstrate that ~95% of such intervals should contain the parameter.\nNote that you can find the parameter by just emitting the slice_sample() line of code."
  },
  {
    "objectID": "4-generalization/02-confidence-intervals/slides.html#section-2",
    "href": "4-generalization/02-confidence-intervals/slides.html#section-2",
    "title": "Confidence Intervals",
    "section": "",
    "text": "An economist aims to estimate the average weekly cost of groceries per household in two cites: Oakland, CA (population ~400,000) and Fremont, CA (population ~200,000). Both of these populations of households are presumed to have a similar standard deviation of weekly grocery costs. The economist takes a simple random sample (without replacement) of 100 households from each city, records their costs, and computes a 95% confidence interval for the average weekly cost.\n\nApproximately how much wider would Oakland’s confidence interval be than Fremont’s?\n\n\nA. 1.5 times as wide B. 2 times as wide C. 3 times as wide D. 4 times as wide E. The same width\nIt’s more or less E here since N is so large.\n\n\nCodecountdown::countdown(1)\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "4-generalization/02-confidence-intervals/slides.html#section-3",
    "href": "4-generalization/02-confidence-intervals/slides.html#section-3",
    "title": "Confidence Intervals",
    "section": "",
    "text": "An economist aims to estimate the average weekly cost of groceries per household in two cites: Grimes, CA (population ~400) and Tranquility, CA (population ~800). Both of these populations of households are presumed to have a similar standard deviation of weekly grocery costs. The demographer takes a simple random sample (without replacement) of 100 households from each city, records their costs, and computes a 95% confidence interval for the average weekly cost.\n\nApproximately how much wider would Tranquility’s confidence interval be than Grimes’s?\n\n\nA. Half as wide B. Roughly 10% less than Grimes C. The same width D. Roughly 10% more than Grimes E. Twice as wide\nSampling without replacement in a setting where n is not significantly smaller than N will end up showing less variability in statistics than when N is huge. The effect is slight, so you can eyeball that the answer should be D.\nYou can get the precise answer by calculating the finite population correction factor for each one: ((N - n)/(N-1)). This is in a footnotes to the notes but is good for them to be aware of - that we can’t always just use SD(pop)/sqrt(n).\n\n\nCodecountdown::countdown(2)\n\n\n\n−&plus;\n\n02:00"
  },
  {
    "objectID": "4-generalization/02-confidence-intervals/slides.html#section-4",
    "href": "4-generalization/02-confidence-intervals/slides.html#section-4",
    "title": "Confidence Intervals",
    "section": "",
    "text": "What will happen to the shape of the empirical distribution as we increase \\(n\\)?\n\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nA. The standard deviation will decrease. B. The standard deviation will increase. C. The mean will increase D. It will more closely resemble the population dist. E. It will more closely resemble the sampling dist.\nThe answer is D.\nThis plot and example are from the notes. It is probably good to remind them than in any practical setting you only get to see the empirical distribution. Here we have the full population and we’re simulating drawing many samples to get a sense of what that sampling distribution should look like."
  },
  {
    "objectID": "4-generalization/02-confidence-intervals/slides.html#section-5",
    "href": "4-generalization/02-confidence-intervals/slides.html#section-5",
    "title": "Confidence Intervals",
    "section": "",
    "text": "What will happen to the shape of the sampling distribution as we increase \\(n\\)?\n\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nA. The standard deviation will decrease B. The standard deviation will increase C. The mean will increase D. It will more closely resemble the population dist. E. It will more closely resemble the sampling dist.\nThe answer is A but note here the SD of the sampling dist is called the SE."
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/ps.html",
    "href": "4-generalization/04-hypothesis-tests/ps.html",
    "title": "Hypothesis Tests",
    "section": "",
    "text": "Is Yawning Contagious?\nAn experiment conducted by MythBusters tested if a person can be subconsciously influenced into yawning if another person near them yawns.\n\n\n\n\n\nIn this study 50 people were randomly assigned to two groups: 34 to a group where a person near them yawned (group: stimulus) and 16 to a control group where there wasn’t a yawn seed (group: no stimulus). They then recorded the whether each subject yawned (response: yawn) or not (response: no yawn).\n\nWhat is the explanatory variable? Response variable?\n\n\n\nWhat was the proportion of yawners in the stimulus group, \\(\\hat{p}_s\\)?\n\n\n\nWhat was the proportion of yawners in the no stimulus group, \\(\\hat{p}_n\\)?\n\n\n\nIf there were no association between yawning and the proximity of another yawner, what would you expect the difference to be between these two proportions?\n\n\n\nWhat are the possible values that \\(\\hat{p}_s - \\hat{p}_{n}\\) can take? (i.e., what is the maximum value? the minimum value?)\n\n\n\nIn terms of \\(\\hat{p}_s - \\hat{p}_{n}\\), what is an example of a result that would demonstrate a strong association between yawning and being exposed to a yawn?\n\n\nSimulating Yawners\nWhat kind of data would be observed if there was no association between these variables and if the only variation was caused by the process of randomly assigning subjects to the two conditions? Find out by simulating the process.\n\nCreate a deck of cards, 36 of which represent subjects who did not yawn, 14 of which represent subjects who yawned.\nShuffle the deck of cards to simulate the process of randomly assignment to the two conditions: being exposed to a yawn (stimulus) and not being exposed (no stimulus).\nDeal them into two decks of size 16 and 34, representing the 1/3 of the subjects that were assigned to the no stimulus group and the 2/3 assigned to the stimulus group.\nCalculate the difference in the proportion of yawners in the two group, \\(\\hat{p}_s - \\hat{p}_{n}\\), and record it below.\nRepeat process 5 more times and sketch the distribution of simulated statistics from the class in the blank plot.\n\n\n\n\n\n\n\n\n\nSim\n\\(\\hat{p}_s - \\hat{p}_{n}\\)\n\n\n\n1\n\n\n\n2\n\n\n\n3\n\n\n\n4\n\n\n\n5\n\n\n\n6\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nIf there was in fact no relationship between yawning and the stimulus, what values of the statistic would you expect to see?\n\n\n\nWhat value of \\(\\hat{p}_s - \\hat{p}_{n}\\) did the MythBusters actually observe?\n\n\n\nIs this data convincing evidence that yawning is contagious? Why or why not?"
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/slides.html",
    "href": "4-generalization/04-hypothesis-tests/slides.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set 13: Hypothesis Tests? (first page)\nBreak\nProblem Set 13: Hypothesis Tests? (second page)"
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/slides.html#agenda",
    "href": "4-generalization/04-hypothesis-tests/slides.html#agenda",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set 13: Hypothesis Tests? (first page)\nBreak\nProblem Set 13: Hypothesis Tests? (second page)"
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/slides.html#section",
    "href": "4-generalization/04-hypothesis-tests/slides.html#section",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Which of the following statements below represents claims that correspond to a null hypothesis (as opposed to an alternative hypothesis)?\nHint: try to write them using parameters (statements about means / proportions / etc)\nA. King cheetahs on average run the same speed as standard spotted cheetahs.\nB. For a particular student, the probability of correctly answering a 5-option multiple choice test is larger than 0.2 (i.e., better than guessing).\nC. The mean length of African elephant tusks has changed over the last 100 years.\nD. The risk of facial clefts is equal for babies born to mothers who take folic acid supplements compared with those from mothers who do not.\nE. Mean birth weight of newborns is dependent on caffeine intake during pregnancy.\nF. The probability of getting in a car accident is the same if using a cell phone than if not using a cell phone.\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nCorrect answers: A, D, F.\nMost of these they haven’t seen examples of, so it will be worth laying out the null hypotheses."
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/slides.html#section-1",
    "href": "4-generalization/04-hypothesis-tests/slides.html#section-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "A pharmaceutical company developed a new treatment for eczema and performed a hypothesis test to see if it worked better than the company’s old treatment. The P-value for the test was \\(10\\%\\). Which one of the following statements are true?\n\nA. The probability that the null hypothesis is false is \\(10\\%\\).\nB. The P-value of about \\(10\\%\\) was computed assuming that the null hypothesis was true.\nC. The new drug is significantly better than the old.\nD. The alternative hypothesis is 10 times more likely the null.\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/slides.html#what-do-you-think",
    "href": "4-generalization/04-hypothesis-tests/slides.html#what-do-you-think",
    "title": "Hypothesis Testing",
    "section": "What do you think?",
    "text": "What do you think?\nAnswer at pollev.com."
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/slides.html#what-do-you-think-about-the-of-the-experiment-in-the-video",
    "href": "4-generalization/04-hypothesis-tests/slides.html#what-do-you-think-about-the-of-the-experiment-in-the-video",
    "title": "Hypothesis Testing",
    "section": "What do you think about the of the experiment in the video?",
    "text": "What do you think about the of the experiment in the video?\n\nDiscuss with your group, identifying anything that seems amiss.\n\n\n\n\n\n−&plus;\n\n01:30"
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/slides.html#do-you-still-think-yawning-is-contagious",
    "href": "4-generalization/04-hypothesis-tests/slides.html#do-you-still-think-yawning-is-contagious",
    "title": "Hypothesis Testing",
    "section": "Do you still think yawning is contagious?",
    "text": "Do you still think yawning is contagious?\nAnswer at pollev.com."
  },
  {
    "objectID": "4-generalization/01-sampling-distributions/ps.html",
    "href": "4-generalization/01-sampling-distributions/ps.html",
    "title": "Sampling Distributions",
    "section": "",
    "text": "The National Youth Tobacco Survey (NYTS) is a complex survey carried out by the CDC to better understand the rate of e-cigarette usage among teens. It is a school-based, pencil-and-paper questionnaire, self-administered to a nationally representative sample of students in grades 6-12 in the U.S. In 2019, 5,675 students completed the NYTS; the overall survey response rate was 71.6%. Among other questions, students were asked if they had used an e-cigarette in the past 30 days.\n\nDescribe the sample. How large is it (\\(n\\)) and what is the unit of observation?\n\n\n\n\nDescribe the population. Approximately how large is it (\\(N\\))?\n\n\n\n\nSelect a population parameter that researchers would be interested in estimating and the corresponding statistic that will serve as the estimate.\n\n\n\n\nIdentify possible sources of statistical bias and whether they are selection, measurement, and non-response bias. How do you expect this will effect the estimate of the parameter?\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify possible sources of variation and whether they are sampling or measurement variability. How do you expect this will effect the estimate of the parameter?\n\n\n\n\n\n\n\n\n\n\n\n\nOn the back of this page make two sketches: 1. an empirical distribution of the data and 2. what you think the sampling distribution of the statistic might look like given what you know about the data collection. Be sure to label the axes of both plots. Illustrate any anticipation of bias you have by adding a vertical line on the sampling distribution that hits the x-axis where you expect the population parameter to be."
  },
  {
    "objectID": "4-generalization/01-sampling-distributions/ps.html#case-study-1-national-youth-tobacco-study",
    "href": "4-generalization/01-sampling-distributions/ps.html#case-study-1-national-youth-tobacco-study",
    "title": "Sampling Distributions",
    "section": "",
    "text": "The National Youth Tobacco Survey (NYTS) is a complex survey carried out by the CDC to better understand the rate of e-cigarette usage among teens. It is a school-based, pencil-and-paper questionnaire, self-administered to a nationally representative sample of students in grades 6-12 in the U.S. In 2019, 5,675 students completed the NYTS; the overall survey response rate was 71.6%. Among other questions, students were asked if they had used an e-cigarette in the past 30 days.\n\nDescribe the sample. How large is it (\\(n\\)) and what is the unit of observation?\n\n\n\n\nDescribe the population. Approximately how large is it (\\(N\\))?\n\n\n\n\nSelect a population parameter that researchers would be interested in estimating and the corresponding statistic that will serve as the estimate.\n\n\n\n\nIdentify possible sources of statistical bias and whether they are selection, measurement, and non-response bias. How do you expect this will effect the estimate of the parameter?\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify possible sources of variation and whether they are sampling or measurement variability. How do you expect this will effect the estimate of the parameter?\n\n\n\n\n\n\n\n\n\n\n\n\nOn the back of this page make two sketches: 1. an empirical distribution of the data and 2. what you think the sampling distribution of the statistic might look like given what you know about the data collection. Be sure to label the axes of both plots. Illustrate any anticipation of bias you have by adding a vertical line on the sampling distribution that hits the x-axis where you expect the population parameter to be."
  },
  {
    "objectID": "4-generalization/01-sampling-distributions/notes.html",
    "href": "4-generalization/01-sampling-distributions/notes.html",
    "title": "From Samples to Populations",
    "section": "",
    "text": "Generalization is the process of using a subset of information to draw conclusions about some broader set or phenomenon. It is powerful - it allows us to draw conclusions about things we have not observed - but it is tricky to do well. In these notes, you will learn the sources of error that can creep in when making a generalization.\nThere are four terms that you will see come up again and again as we discuss generalization. They are familiar terms that have tightly coupled meanings, so we present them together.\nTo see how these terms interrelate and to introduce the sources of error that can creep in while generalizing from a sample to a population, let’s look a scenario."
  },
  {
    "objectID": "4-generalization/01-sampling-distributions/notes.html#sources-of-error",
    "href": "4-generalization/01-sampling-distributions/notes.html#sources-of-error",
    "title": "From Samples to Populations",
    "section": "Sources of Error",
    "text": "Sources of Error\nTo understand the forms that estimation error can take, consider the analogy of darts thrown at a dart board. The center of the dart board represents the parameter that we are trying to hit and each dart that we throw represents a statistic calculated from a sample. There are two ways in which a dart throw can miss the bullseye. One way is that we could systematically tend to throw above and to the left of the bullseye. This form of error is called bias. Another way that our dart-throwing could miss the bullseye is if we are an erratic thrower and one throw tends to be very different from this next. This form of error is called variation or variability.\n\n\n\n\nIf a sample is representative of the population, there is no bias present. That is represented by the top row of bullseyes.\nTypes of Statistical Bias\nStatistical bias comes in many forms. Here we describe two of the most important types.\n\nSelection bias\n\nWhen the mechanism used to choose units for the sample tends to select certain units with a higher probability than other units. That is, not all units in the population are equally likely to be selected for the sample.\n\n\nAs an example, a convenience sample chooses the units that are most easily available. Problems can arise when those who are easy to reach differ in important ways from those harder to reach. Another example of selection bias can happen with observational studies and experiments. These studies often rely on volunteers (people who choose to participate), and this self-selection has the potential for bias if the volunteers differ from the target population in important ways.\n\nMeasurement bias\n\nWhen your process of measuring a variable systematically misses the target in one direction.\n\n\nFor example, low humidity can systematically give us incorrectly high measurements for air pollution. In addition, measurement devices can become unstable and drift over time and so produce systematic errors. In surveys, measurement bias can arise when questions are confusingly worded or leading, or when respondents may not be comfortable answering honestly.\n\nNon-response bias\n\nWhen certain units originally selected for the sample fail to provide data and those non-responders different in meaningful ways from the responders. When non-response is present, the final sample size for which there is full data is less than the initial sample size.\n\n\nAll of these types of bias can lead to situations where the data are not centered on the unknown targeted value. A common method to address selection bias is to draw a simple random sample, where each unit from the population is equally likely to be drawn. A pilot survey can improve question wording and so reduce measurement bias. In the lab sciences, procedures to calibrate instruments and protocols to take measurements in random order can reduce measurement bias. Non-response bias can be addressed by providing incentives for participation.\nBias does not need to be avoided under all circumstances. If an instrument is highly precise (low variance) and has a small bias, then that instrument might be preferable to another that has high variance and little to no bias. Biased studies can be useful to pilot a survey instrument or to capture useful information for the design of a larger study.\nStatistical Bias in Pimentel\nThe method used by the professor likely suffers from statistical bias. The units that were drawn into the sample (the 18 students in the front row who were called on) constitute a convenience sample: they were sampled because they were easy to sample. That is not necessarily a problem, but there is good reason to think that students of all years are not equally likely to sit in the front row. First year students, bright-eyed and bushy-tailed with enthusiasm, will be more likely to sit in the front row. If that is true, the professor incurred selection bias that will lead to an estimate of the population mean that is too low.\nWhat about measurement bias? In this case, the process of measuring a student’s year involves the act of asking them a question, hearing their answer, and writing it on the board. How could this be systematically in error? One way would be if first year students occasionally lie about their year when asked, for fear of being thought of as an over-eager over-achiever. If this were true, then lots of the 2s that we recorded were in fact 1s, and the estimate of 1.77 would be too high.\nWhat about non-response bias? This is likely not a problem in this setting. While it can be easy to ignore a question that appears in a survey that you get via email, it is much more difficult to dodge a question asked of you directly during class.\nTypes of Variation\nWhether or not bias is present, data typically also exhibit variation.\n\nSampling variability\n\nIf the sample is drawn from the population with some amount of randomness, the sampling variability describes the variability from one sample to the next.\n\n\n\nMeasurement variability\n\nWhen we take multiple measurements on the same object and we get variations in measurements from one sample to the next.\n\n\nVariability in Pimentel\nThere is some amount of randomness that plays into which units made their way into the professor’s sample of size 18. At the start of a semester, students tend to sit in a different seat each day they come to class. Even in a world were all students are equally likely to sit in the first row (therefore there is no bias), our sample of 1.77 might be too low because that day an unusally high number of first year students happened to sit in the front row, purely due to chance. The next day, perhaps an unually high number of juniors would sit in the front row and the estimate would leap up to, say, 2.7. This is sampling variability.\nVariability in measurement here would refer to a process by which the recorded year for a student would differ from one measurement to the next. Imagine a student who sits in the front row on the first day and is recorded as a two That same student sits in the front row on the 10th day and is recorded as a one. It’s hard to imagine there being much measurement variability here but you can imagine a very absent-minded professor who, upon hearing the year, will sometimes immediately forget what had been said and will record a random year."
  },
  {
    "objectID": "4-generalization/01-sampling-distributions/notes.html#the-sampling-distribution",
    "href": "4-generalization/01-sampling-distributions/notes.html#the-sampling-distribution",
    "title": "From Samples to Populations",
    "section": "The Sampling Distribution",
    "text": "The Sampling Distribution\nOne of the key concepts in understanding estimation errors when using statistics is to understand the shape of the sampling distribution of that statistic.\n\nSampling Distribution\n\nThe distribution of a statistic upon repeated sampling.\n\n\nEven though this distribution has an innocuous name, something big is happening with this definition. No longer are we considering the (usually unknowable) distribution of the population or the observed distribution of the data (an empirical distribution). The sampling distribution is a distribution of a statistic, illustrating the different values that it could take, along with the probability of getting each of those values in a given sample.\nUsually the sampling distribution is a hypothetical thing: what would our statistic have looked like if we had taken a different sample of data? We can make it concrete by working in a setting where we can actually do just that."
  },
  {
    "objectID": "4-generalization/01-sampling-distributions/notes.html#drawing-samples-from-population",
    "href": "4-generalization/01-sampling-distributions/notes.html#drawing-samples-from-population",
    "title": "From Samples to Populations",
    "section": "Drawing Samples from Population",
    "text": "Drawing Samples from Population\nTo understand the role that bias and variation play in estimates, we will for the moment assume that we have access to the entire population. This is almost never the case: if we had the population, we wouldn’t need to bother with estimating it from a sample! It is a very useful thought experiment, however. It allows us to see the different ways that samples and statistics can be drawn under different scenarios.\nFor this thought-experiment, we will will be drawing from a population that has 527 observations / rows, one for every student in Pimentel, with a population distribution of year that looks like this.\n\nCodepop_eager &lt;- data.frame(year = factor(rep(c(1, 2, 3, 4), \n                                   times = c(245, 210, 47, 25))),\n                        eagerness = rep(c(10, 6, 3, 1), \n                                        times = c(245, 210, 47, 25)))\nset.seed(34)\npop_eager &lt;- pop_eager %&gt;%\n  slice_sample(n = nrow(pop_eager)) # shuffle the rows to make them look natural\npop_eager %&gt;%\n  ggplot(aes(x = year)) +\n  geom_bar(fill = \"purple\") +\n  labs(title = \"Population Distribution\")\n\n\n\n\n\n\n\nScenario 1: Calling on the front row\nIn the data collection scheme used by the professor, the students who happened to be sitting in the 18 seats at the front of the class are the ones who will make it into the sample. As discussed above, it is likely that this process will result in selection bias since first year students tend to be more eager and eager students tend to sit at the font of the class.\nWe can do our best to envision what this selection bias would look like by selecting each student out of the population with a probability that is proportional to their eagerness. Let’s say first year students have an eagerness score of 10 out of 10, sophomores have a 6, juniors a 3, and seniors a 1. Here are the first 5 rows of the population data frame with the eagerness scores right next to the year of the student.\n\nCodeslice(pop_eager, 1, 2, 3, 4, 5)\n\n  year eagerness\n1    3         3\n2    2         6\n3    2         6\n4    1        10\n5    2         6\n\n\nWhen simulating the process that the professor used to draw 18 students, we can select the first student, a junior, with probability of 3 divided by the total eagerness of all of the students (the sum of the eagerness column), which is 4900. The probability of that student (or any junior) is \\(3/4900 = 0.0006\\). That is small, but that’s not surprising: there are 527 students in the class, so the probability of selecting just one of them should be small.\nThe probability of selecting the fourth student from the population data frame above, a first year, can be calculated as \\(10/4900 = .002\\). That’s also small, but it is 3.33 (10/3) times the chance of selecting a junior. In this setting, we’re using eagerness as a sampling weight to determine the relative probability of selecting more eager students into the sample.\nLet’s now simulate the process of drawing 18 rows out of the data frame of 527 rows, where each row is being selected with a probability proportional to it’s eagerness. The three plots along the top row of the plot below illustrate what the empirical distribution of three samples might look like. They help us envision what the professor’s plot on the board would look like on three different days where the 18 students in the front row were called on, and in each of those days, it is the first year students who are most likely to end up the front row.\n\nCodelibrary(infer)\nsamp_1 &lt;- pop_eager %&gt;%\n  slice_sample(n = 18,\n                   replace = FALSE,\n                   weight_by = eagerness) \n\nmany_samps &lt;- samp_1 %&gt;%\n  mutate(replicate = 1)\n\nset.seed(40211)\n\nfor (i in 2:500) {\n  many_samps &lt;- pop_eager %&gt;%\n    slice_sample(n = 18,\n                 replace = FALSE,\n                 weight_by = eagerness) %&gt;%\n    mutate(replicate = i ) %&gt;%\n    bind_rows(many_samps)\n}\n\np1 &lt;- many_samps %&gt;%\n  filter(replicate == 1) %&gt;%\n  ggplot(aes(x = year)) + \n  geom_bar() +\n  labs(title = \"Sample 1\")\n\np2 &lt;- many_samps %&gt;%\n  filter(replicate == 2) %&gt;%\n  ggplot(aes(x = year)) + \n  geom_bar() +\n  labs(title = \"Sample 2\")\n\np3 &lt;- many_samps %&gt;%\n  filter(replicate == 3) %&gt;%\n  ggplot(aes(x = year)) + \n  geom_bar() +\n  labs(title = \"Sample 3\")\n\nmany_xbars &lt;- many_samps %&gt;%\n  group_by(replicate) %&gt;%\n  summarize(xbar = mean(as.numeric(year)))\n\np4 &lt;- many_xbars %&gt;%\n  ggplot(aes(x = xbar)) +\n  geom_bar(fill = \"purple\") +\n  lims(x = c(0, 4)) +\n  labs(title = \"Sampling Distribution\")\n\nlibrary(patchwork)\n\n(p1 + p2 + p3) / p4\n\n\n\n\n\n\n\nAs you look from Sample 1 to Sample 3, you notice that the distribution of year varies from sample to sample. This is sampling variability. If you compare these plots to the population distribution above, you’ll find these three samples on balance seem to systematically have more first year students that you might expect. This is an effect of selection bias.\nBoth of these notions are captured in the sampling distribution. To understand the notion of a sampling distribution, imagine that you:\n\nCalculate the sample mean for sample 1 and store it away as \\(x_1 =\\) 1.3.\nCalculate the sample mean for sample 2 and store it away as \\(x_2 =\\) 1.2.\nCalculate the sample mean for sample 3 and store it away as \\(x_3 =\\) 1.5.\nRepeat this process 500 times then\nPlot the distribution of those 500 \\(\\bar{x}\\)s.\n\nThis distribution is shown in the bottom row. We can see that in this scenario, it’s possible to get \\(\\bar{x}\\)s that are as low as around 1 and as high as around 1.8. The \\(\\bar{x}\\) that we would expect from this process is around 1.4."
  },
  {
    "objectID": "4-generalization/01-sampling-distributions/notes.html#summary",
    "href": "4-generalization/01-sampling-distributions/notes.html#summary",
    "title": "From Samples to Populations",
    "section": "Summary",
    "text": "Summary\nIn these notes we laid out a common goal of making generalizations: estimating the value of population parameters using statistics calculated from samples. The process of generalization is subject to several sources of error that are lumped into statistical bias and variation. Three of the central forms of statistical bias are selection bias, measurement bias, and non-response bias. Two common forms of variation are sampling variability and measurement variability. With these notions of error in mind, we learned about the sampling distribution, the distribution of statistics that we would observe if we were to sample from the sample population many times and compute many statistics.\nThe sampling distribution is the central concept in making generalizations so we will revisit it throughout the coming weeks."
  },
  {
    "objectID": "4-generalization/labs/05-peoples-park/lab.html",
    "href": "4-generalization/labs/05-peoples-park/lab.html",
    "title": "Lab 4: People’s Park",
    "section": "",
    "text": "Slides\nIn answering the following questions, it will be helpful to consult"
  },
  {
    "objectID": "4-generalization/labs/05-peoples-park/lab.html#part-i-understanding-the-context-of-the-data",
    "href": "4-generalization/labs/05-peoples-park/lab.html#part-i-understanding-the-context-of-the-data",
    "title": "Lab 4: People’s Park",
    "section": "Part I: Understanding the Context of the Data",
    "text": "Part I: Understanding the Context of the Data\n\nLab Part I: People’s Park"
  },
  {
    "objectID": "4-generalization/labs/05-peoples-park/lab.html#part-ii-computing-on-the-data",
    "href": "4-generalization/labs/05-peoples-park/lab.html#part-ii-computing-on-the-data",
    "title": "Lab 4: People’s Park",
    "section": "Part II: Computing on the Data",
    "text": "Part II: Computing on the Data\nThe data collected by the Chancellor’s Office on Cal students can be found as ppk in the stat20data package.\nThe ppk data set represents a subset of questions that were asked in the questionnaire and have had random noise added to them. The results, in aggregate, share similar statistical properties to the raw data, but a given row no longer reflects an individual student’s response completely.\nQuestion 1\nPrint the first few rows with the columns that correspond to the responses to survey questions 1, 7, and 8 (Note: we have changed the data back from all numerical data, as suggested by part 1 question 8, to a mix of numerical and categorical data). Please comment on whether your encoding of the data from Q7 on the questionnaire matches the encoding in ppk.\nQuestion 2\nReturn to your sketches from lab part 1, question 9. Create those visualizations (or more appropriate analogues) using the questionnaire data. For each, add a title and axis labels to make it clear what they are showing, and describe the distribution in words. If your visualization is of ordinal data, the bars should be ordered accordingly. For part a here, you’re welcome to select just one of the priorities to visualize.\n\nQuestion 9\nQuestion 10\nQuestion 18 and 21 (showing the change from before and after the information in one plot)\nQuestion 3\nCreate a new column called support_before that takes the response data from question 18 and returns TRUE for answers of “Very strongly support”, “Strongly support”, and “Somewhat support” and FALSE otherwise. What proportion of the survey participants in each class (freshman, sophomore, etc) supported the People’s Park Project before being presented with the information on the bottom of page 14?\nQuestion 4\nRepeat the previous question but this time drop any rows with NA (missing data) before starting your calculation. This can be done by using the drop_na() function, which scans any columns that you provide and drop any rows with NA in that column.\n\nppk |&gt;\n  drop_na(Q18_words)\n\nDoes the proportion change? If it does, which proportion do you think most consumers of this data analysis will assume they’re looking at?\n\nFor the remaining questions in this lab, drop rows with NAs in the columns that you’re using for that question.\n\nQuestion 5\nWhat is the mean and median rating of the condition of People’s Park (question 15 on the survey)?\nQuestion 6\nCreate a new column called change_in_support that measures the change in support from question 18 to 21 of the survey. What is the mean change in support of the survey participants in each class (freshman, sophomore, etc) for the People’s Park Project after reading the information? What assumption must you make about the values of the Likert scale in order for these statistics to be informative?\nQuestion 7\nConstruct one addition visualization that captures a variable or relationship between two variables that you are interested in. Describe the structure that you see in the plot.\nQuestion 8\nCreate a 95% confidence intervals for the mean rating of the condition of People’s Park using the normal curve. Interpret the interval in the context of the problem in a clear sentence.\nQuestion 9\nCreate a 95% confidence interval using the normal curve for the overall proportion of students who support the People’s Park Project without having been exposed to the information on page 14. Interpret the interval in the context of the problem in a clear sentence. Does your point estimate approximately match that reported in the Chancellor’s email?\nQuestion 10\nSelect a confidence level lower than 95% and recalculate the interval from the previous question to express this confidence level. In general, are intervals with higher confidence wider or narrower than those with lower confidence?\nQuestion 11\nUsing the normal curve, create a 95% confidence interval for the mean change in support for the Project across the entire population after being exposed to the information on page 14.\nQuestion 12\nDoes your interval from the previous question contain 0? What are the implications of that for those working in the Chancellor’s Office on the People’s Park Project?"
  },
  {
    "objectID": "glossary-defs.html",
    "href": "glossary-defs.html",
    "title": "Definitions",
    "section": "",
    "text": "Questions and Data\n\n\nData\n\nAn item of (chiefly numerical) information, especially one obtained by scientific work, a number of which are typically collected together for reference, analysis, or calculation. From Latin datum: that which is given. Facts.\n\n\n\n\n\nVariable\n\nA characteristic of an object or observational unit that can be measured and recorded.\n\n\n\n\n\nNumerical Variable\n\nA variable that take numbers as values and where the magnitude of the number has a quantitative meaning.\n\n\n\n\n\nCategorical Variable\n\nA variable that take categories as values. Each unique category is called a level.\n\n\n\n\n\nContinuous Numerical Variable\n\nA numerical variable that takes values on an interval of the real number line.\n\n\n\n\n\nDiscrete Numerical Variable\n\nA numerical variable that takes values that have jumps between them.\n\n\n\n\n\nOrdinal Categorical Variable\n\nA categorical variable with levels that have a natural ordering.\n\n\n\n\n\nNominal Categorical Variable\n\nA categorical variable with levels with no ordering.\n\n\n\n\n\nData Frame\n\nAn array that associates the observations (downs the rows) with the variables measured on each observation (across the columns). Each cell stores a value observed for a variable on an observation.\n\n\n\n\n\nUnit of Observation\n\nThe class of object on which the variables are observed.\n\n\n\n\n\nSummary\n\nA numerical, graphical, or verbal description of an aspect of data that is on hand.\n\n\n\n\n\nGeneralization\n\nA numerical, graphical, or verbal description of a broader set of units than those on which data was been recorded.\n\n\n\n\n\nCausal Claim\n\nA claim that changing the value of one variable will influence the value of another variable.\n\n\n\n\n\nPrediction\n\nA guess about the value of an unknown variable, based on other known variables."
  },
  {
    "objectID": "4-generalization/labs/05-peoples-park/slides.html#section",
    "href": "4-generalization/labs/05-peoples-park/slides.html#section",
    "title": "Lab: People’s Park",
    "section": "",
    "text": "This is a promo video produced by the University to make an argument for the project. This was released at roughly the same time as the survey results."
  },
  {
    "objectID": "4-generalization/labs/05-peoples-park/slides.html#peoples-park-project",
    "href": "4-generalization/labs/05-peoples-park/slides.html#peoples-park-project",
    "title": "Lab: People’s Park",
    "section": "People’s Park Project",
    "text": "People’s Park Project\nThe University wishes to turn a university-owned plot of land called People’s Park into housing.\n\n\nOpponents\n\nAdvocates for the homeless residents living on site\nNeighbors who don’t want a tall tower built\nHistorical preservationists who want to preserve it for its role in countercultural movement of the ’60s.\n\n\n\nProponents\n\nAdvocates for homeless more generally\nNeighbors who want the blight and crime gone\nStudents who want easier access to housing\nCity of Berkeley who wants more housing built"
  },
  {
    "objectID": "4-generalization/labs/05-peoples-park/slides.html#the-chancellors-survey",
    "href": "4-generalization/labs/05-peoples-park/slides.html#the-chancellors-survey",
    "title": "Lab: People’s Park",
    "section": "The Chancellor’s Survey",
    "text": "The Chancellor’s Survey\n\n\n\n\nChancellor Carol Christ\n\n\nIn fall of 2021, the Chancellor’s Office commissioned a survey of the Berkeley community to gauge opinions on people’s park.\n\nIn this lab you will gain access to this survey data."
  },
  {
    "objectID": "4-generalization/labs/05-peoples-park/slides.html#part-i-understanding-the-context-of-the-data",
    "href": "4-generalization/labs/05-peoples-park/slides.html#part-i-understanding-the-context-of-the-data",
    "title": "Lab: People’s Park",
    "section": "Part I: Understanding the Context of the Data",
    "text": "Part I: Understanding the Context of the Data\n\n\n\n\n−&plus;\n\n25:00"
  },
  {
    "objectID": "4-generalization/labs/05-peoples-park/lab-context.html",
    "href": "4-generalization/labs/05-peoples-park/lab-context.html",
    "title": "People’s Park",
    "section": "",
    "text": "In answering the following questions, it will be helpful to consult the documents linked on the lab page on the course website.\n\nQuestion 1\nWhat do you think was the goal(s) of the Chancellor’s office in commissioning this survey?\n\n\n\n\n\nQuestion 2\nWhat is the population from which the Chancellor’s office has drawn their sample? What is \\(N\\)?\n\n\n\n\n\nQuestion 3\nDescribe two population parameters that the Chancellor’s office is trying to estimate using the survey data.\n\n\n\n\n\n\n\nQuestion 4\nWhat was initial number of students selected to take the survey? What was the final sample size, \\(n\\), that actually did? What was the response rate?\n\n\n\n\n\n\nQuestion 5\nIdentify possible sources of bias and whether they are selection, measurement, or nonresponse bias. How do you expect this will effect the estimate of the parameter?\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\nIdentify possible sources of variation and whether they are sampling or measurement variability. How do you expect this will effect the estimate of the parameter?\n\n\n\n\n\n\n\n\nQuestion 7\nConsider the type of data collected in question 8 of the survey, which is measured using the Likert Scale. Review the Wikipedia article on the Likert Scale (particularly the Scoring and Analysis section) to determine. What two considerations go into whether such categorical data can be analyzed as if it were numerical data (what they call “interval data”)? Do you think the assumption used for second consideration is reasonable for this question?\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\nSketch a data frame of what the first 5 rows of the data frame might look like that contains the responses from the first 5 students. Include columns showing what the data might look like that comes out of questions 1, 7, and 8. Note that in the real data set, the data values are all translated from words into numbers. Speculate as to how this translation is done.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\nSketch a plot of what the data might look like that is generated by each of the following survey questions. Note that this should be done without looking at the actual data frame.\nA. Q. 9\nB. Q. 10\nC. Q. 18 and 21 (showing the individual change from before and after the information in one plot)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\nSelect one parameter that you identified in Question 3 and and sketch what you think the sampling distribution of the statistic might look like given what you know about the data collection process. Be sure to label the axes. Illustrate any anticipation of bias by adding a vertical line on the sampling distribution that hits the x-axis where you expect the population parameter to be."
  },
  {
    "objectID": "4-generalization/01-sampling-distributions/slides.html",
    "href": "4-generalization/01-sampling-distributions/slides.html",
    "title": "Sampling Distributions",
    "section": "",
    "text": "Concept Questions\nPS: Sampling Distributions\nLab"
  },
  {
    "objectID": "4-generalization/01-sampling-distributions/slides.html#agenda",
    "href": "4-generalization/01-sampling-distributions/slides.html#agenda",
    "title": "Sampling Distributions",
    "section": "",
    "text": "Concept Questions\nPS: Sampling Distributions\nLab"
  },
  {
    "objectID": "4-generalization/01-sampling-distributions/slides.html#section",
    "href": "4-generalization/01-sampling-distributions/slides.html#section",
    "title": "Sampling Distributions",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\n\nThe top plots are population distributions; the bottom two are sampling distributions of the means from many samples of size 200. Match numbers to letters.\n\n\nCorrect answer: It depends on the sampling process.\nThese sampling distributions were actually made from taking samples of size 200 with replacement from populations of size 100. Most students will assume this when they first answer and choose 1-B, 2-A. This is a good opportunity to cue them to think very carefully about exactly how it is worded, then give them time to discuss and revote.\n\n\nCodelibrary(tidyverse)\nset.seed(123123)\nfamily_pop1 &lt;- rep(c(1,2,3,4), times = c(20,40,25,15)) \nfamily_pop2 &lt;- rep(c(1,2,3,4), times = c(15,20,35,30))\n\npop_plot1 &lt;- ggplot(data.frame(family_pop1), aes(x=family_pop1)) +\n  geom_bar(fill = \"gray55\", width = 0.98) + \n  xlab(\"\") + \n  annotate(\"text\", x=3.8, y = 35, label=\"(1)\", color = \"black\", size=5) +\n  theme_gray(base_size = 16)\n\npop_plot2 &lt;- ggplot(data.frame(family_pop2), aes(x=family_pop2)) +\n  geom_bar(fill = \"gray55\", width = 0.98) + \n  xlab(\"\") + \n  annotate(\"text\", x=1, y = 30, label=\"(2)\", color = \"black\", size=5) +\n  theme_gray(base_size = 16)\n\nfamily_sample1 &lt;- replicate(500, \n                        mean(sample(family_pop1, 200, replace = TRUE)))\n\nsample_plot1 &lt;- ggplot(data.frame(family_sample1), aes(x=family_sample1)) +\n  geom_histogram(fill = \"gray\", color = \"black\",\n                 bins = 25) + xlab(\"\") +\n  annotate(\"text\", x=2.2, y = 42, label=\"(B)\", color = \"black\", size=5) +\n  theme_gray(base_size = 16)\n\nfamily_sample2 &lt;- replicate(500, \n                            mean(sample(family_pop2, 200, replace = TRUE)))\n\nsample_plot2 &lt;- ggplot(data.frame(family_sample2), aes(x=family_sample2)) +\n  geom_histogram(fill = \"gray\", color = \"black\",\n                 bins = 25) + xlab(\"\") +\n  annotate(\"text\", x=3, y = 60, label=\"(A)\", color = \"black\", size=5) +\n  theme_gray(base_size = 16)\n\nlibrary(patchwork)\n\n(pop_plot1 + pop_plot2) / (sample_plot2 + sample_plot1)"
  },
  {
    "objectID": "4-generalization/01-sampling-distributions/slides.html#section-1",
    "href": "4-generalization/01-sampling-distributions/slides.html#section-1",
    "title": "Sampling Distributions",
    "section": "",
    "text": "Scenario 1: Calling on the front row\n\nCodelibrary(tidyverse)\npop_eager &lt;- data.frame(year = factor(rep(c(1, 2, 3, 4), \n                                   times = c(245, 210, 47, 25))),\n                        eagerness = rep(c(10, 6, 3, 1), \n                                        times = c(245, 210, 47, 25)))\nset.seed(34)\npop_eager &lt;- pop_eager %&gt;%\n  slice_sample(n = nrow(pop_eager)) \n\nlibrary(infer)\nsamp_1 &lt;- pop_eager %&gt;%\n  slice_sample(n = 18,\n                   replace = FALSE,\n                   weight_by = eagerness) \n\nmany_samps &lt;- samp_1 %&gt;%\n  mutate(replicate = 1)\n\nset.seed(40211)\n\nfor (i in 2:500) {\n  many_samps &lt;- pop_eager %&gt;%\n    slice_sample(n = 18,\n                 replace = FALSE,\n                 weight_by = eagerness) %&gt;%\n    mutate(replicate = i ) %&gt;%\n    bind_rows(many_samps)\n}\n\np1 &lt;- many_samps %&gt;%\n  filter(replicate == 1) %&gt;%\n  ggplot(aes(x = year)) + \n  geom_bar() +\n  labs(title = \"Sample 1\")\n\np2 &lt;- many_samps %&gt;%\n  filter(replicate == 2) %&gt;%\n  ggplot(aes(x = year)) + \n  geom_bar() +\n  labs(title = \"Sample 2\")\n\np3 &lt;- many_samps %&gt;%\n  filter(replicate == 3) %&gt;%\n  ggplot(aes(x = year)) + \n  geom_bar() +\n  labs(title = \"Sample 3\")\n\nmany_xbars &lt;- many_samps %&gt;%\n  group_by(replicate) %&gt;%\n  summarize(xbar = mean(as.numeric(year)))\n\np4 &lt;- many_xbars %&gt;%\n  ggplot(aes(x = xbar)) +\n  geom_bar(fill = \"purple\") +\n  geom_vline(xintercept = mean(as.numeric(pop_eager$year)),\n             col = \"blue\", lwd = 1.5) +\n  lims(x = c(0, 4)) +\n  labs(title = \"Sampling Distribution\")\n\nlibrary(patchwork)\n\np_patched &lt;- (p1 + p2 + p3) / p4\np_patched\n\n\n\n\n\n\n\n\nThis is the example from the notes, where the prof just takes a sample of size n = 18 from the first row. 1st year students are 10 times more likely to be drawn than 4th year students. The result is that most of the sample means are below the true population parameter (the blue vertical line)."
  },
  {
    "objectID": "4-generalization/01-sampling-distributions/slides.html#section-2",
    "href": "4-generalization/01-sampling-distributions/slides.html#section-2",
    "title": "Sampling Distributions",
    "section": "",
    "text": "Codep4 +\n  labs(title = \"Sampling Distribution from calling on the first row\")\n\n\n\n\n\n\n\n\nHow would the sampling distribution change if instead of calling on the front row, the Prof. put all 527 names on tickets in a box, mixed them up, then drew 18 names without replacement? Select the most dramatic change.\n\n\nThis describes a simple random sample, the sampling distribution will move up to be centered around the parameter.\nA few follow up questions if you’d like:\n\nWhat would happen if in the SRS we called on 8 instead of 18? Answer: the sampling distribution would still be centered about the parameter, but the standard deviation would increase.\n\nTakehome: sampling variability for an average increases or decreases with changing sample size according to 1/sqrt(n).\n\nReturning to the “calling on the front row” sampling method and plot: what would happen if, by the end of the semester, 90% of students sit in the same seat every class period? Answer: in that case there would be very little variability from one day / sample to the next and the SD of the sampling distribution would decrease dramatically.\n\nTakehome: if there is little / no variability in the sampling process, there really isn’t much of a sampling distribution. It’s more like a constant and your only concern is bias.\n\n\nCodecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "4-generalization/01-sampling-distributions/slides.html#section-3",
    "href": "4-generalization/01-sampling-distributions/slides.html#section-3",
    "title": "Sampling Distributions",
    "section": "",
    "text": "Scenario 2: Drawing names from a box\n\nCodepop_equal &lt;- pop_eager %&gt;%\n  mutate(eagerness = 1)\n\nsamp_1 &lt;- pop_equal %&gt;%\n  slice_sample(n = 18,\n                   replace = FALSE,\n                   weight_by = eagerness)\n\nmany_samps &lt;- samp_1 %&gt;%\n  mutate(replicate = 1)\n\nfor (i in 2:500) {\n  many_samps &lt;- pop_equal %&gt;%\n    slice_sample(n = 18,\n                 replace = FALSE,\n                 weight_by = eagerness) %&gt;%\n    mutate(replicate = i ) %&gt;%\n    bind_rows(many_samps)\n}\n\np1 &lt;- many_samps %&gt;%\n  filter(replicate == 1) %&gt;%\n  ggplot(aes(x = year)) +\n  geom_bar() +\n  labs(title = \"Sample 1\")\n\np2 &lt;- many_samps %&gt;%\n  filter(replicate == 2) %&gt;%\n  ggplot(aes(x = year)) +\n  geom_bar() +\n  labs(title = \"Sample 2\")\n\np3 &lt;- many_samps %&gt;%\n  filter(replicate == 3) %&gt;%\n  ggplot(aes(x = year)) +\n  geom_bar() +\n  labs(title = \"Sample 3\")\n\nmany_xbars &lt;- many_samps %&gt;%\n  group_by(replicate) %&gt;%\n  summarize(xbar = mean(as.numeric(year)))\n\np4 &lt;- many_xbars %&gt;%\n  ggplot(aes(x = xbar)) +\n  geom_bar(fill = \"purple\") +\n  geom_vline(xintercept = mean(as.numeric(pop_eager$year)),\n             col = \"blue\", lwd = 1.5) +\n  lims(x = c(0, 4)) +\n  labs(title = \"Sampling Distribution\")\n\n(p1 + p2 + p3) / p4"
  },
  {
    "objectID": "4-generalization/01-sampling-distributions/slides.html#section-4",
    "href": "4-generalization/01-sampling-distributions/slides.html#section-4",
    "title": "Sampling Distributions",
    "section": "",
    "text": "Say we want to estimate the size of an average class at Berkeley.\n\nShould we survey the students, and ask them how large their classes are? Should we ask the administration? Does it matter?\n\n\nCodecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nThis one you may want to send straight to “discuss with your group”.\nThis is a good question to debrief with careful boardwork. You can go quite deep on it, so decide beforehand how far you’d like to go.\nOne way to start the debrief is to make a table of Pros and Cons for both the Administration and the Student Survey, then ask students for help filling it in. Things that might come up include sources of variability (small sample variability in the survey), non-response bias in the survey, and possibly selection bias in the survey (which could take several forms).\nYou can also debrief by sketching data frames. For the administrative data, draw a sketch of the data frame they have access to, where the unit is a class, and one of the columns records the class size. Then draw a sketch of the student survey data frame, where the unit is a student in a particular class, with a column that records the class size. If you summarize the class size column with the mean in the first dataframe and n = N, then you’ve just calculated the parameter - it’s a summary task, no generalization necessary.\nIf you take the mean of class size in the student survey data set, you’ll get a much bigger number, because you’ll be over-sampling students from the very large classes. This can be thought of as selection bias, leading to an overestimate of the parameter.\nOne wrinkle that you could end with: which of those two averages is actually better for understanding the effect of large class sizes? The first one is a property of classes. That might be useful for thinking through things like the size of classrooms on campus. The second average is a property of students. In a way, it better captures the student experience of being in large vs small classes.\nThis is going very far afield, but there’s a similar phenomenon when measuring population density of cities. If it’s measured as people / sq mile, that gives us a sense of how each sq mile of land feels. You could also measure it using the average number of people that you run into in a day per person. This gives a sense of how crowded things are for people.\nThese two measures diverge if you imagine a city on 2 square miles with a population of 100,000 people split evenly between those two squares. Then imagine a second city also on 2 sq miles, but 99,999 live in one sq mile and just 1 person lives in the other square mile. By the first metric, they have the same density, but the second city will feel much more dense to its residents."
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/notes.html",
    "href": "4-generalization/04-hypothesis-tests/notes.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Classical statistics features two primary methods for using a sample of data to make an inference about a more general process. The first is the confidence interval, which expresses the uncertainty in an estimate of a population parameter. The second classical method of generalization is the hypothesis test.\nThe hypothesis test takes a more active approach to reasoning: it posits a specific explanation for how the data could be generated, then evaluates whether or not the observed data is consistent with that model. The hypothesis test is one of the most common statistical tools in the social and natural sciences, but the reasoning involved can be counter-intuitive. Let’s introduce the logic of a hypothesis test by looking at another criminal case that drew statisticians into the mix."
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/notes.html#example-the-united-states-vs-kristen-gilbert",
    "href": "4-generalization/04-hypothesis-tests/notes.html#example-the-united-states-vs-kristen-gilbert",
    "title": "Hypothesis Testing",
    "section": "Example: The United States vs Kristen Gilbert",
    "text": "Example: The United States vs Kristen Gilbert\n\n\n\n\nIn 1989, fresh out of nursing school, Kristen Gilbert got a job at the VA Medical Center in Northampton, Massachusetts, not far from where she grew up1. Within a few years, she became admired for her skill and competence.\nGilbert’s skill was on display whenever a “code blue” alarm was sounded. This alarm indicates that a patient has gone into cardiac arrest and must be addressed quickly by administering a shot of epinephrine to restart the heart. Gilbert developed for a reputation for her steady hand in these crises.\nBy the mid-1990s, however, the other nurses started to grow suspicious. There seemed to be a few too many code blues, and a few too many deaths, during Gilbert’s shifts. The staff brought their concerns to the VA administration, who brought in a statistician to evaluate the data.\nThe Data\nThe data that the VA provided to the statistician contained the number of deaths at the medical center over the previous 10 years, broken out by the three shifts of the days: night, daytime, and evening. As part of the process of exploratory data analysis, the statistician constructed a plot.\n\n\n\n\nThis visualization reveals several striking trends. Between 1990 and 1995, there were dramatically more deaths than the years before and after that interval. Within that time span, it was the evening shift that had most of the deaths. The exception is 1990, when the night and daytime shifts had the most deaths.\nSo when was Gilbert working? She began working in this part of the hospital in March 1990 and stopped working in February 1996. Her shifts throughout that time span? The evening shifts. The one exception was 1990, when she was assigned to work the night shift.\nThis evidence is compelling in establishing an association between Gilbert and the increase in deaths. When the district attorney brought a case against Gilbert in court, this was the first line of evidence they provided. In a trial, however, there is a high burden of proof.\nCould there be an alternative explanation for the trend found in this data?\nThe role of random chance\nSuppose for a moment that the occurrence of deaths at the hospital had nothing to do with Gilbert being on shift. In that case we would expect that the proportion of shifts with a death would be fairly similar when comparing shifts where Gilbert was working and shifts where she was not. But we wouldn’t expect those proportions to be exactly equal. It’s reasonable to think that a slightly higher proportion of Gilbert’s shifts could have had a death just due to random chance, not due to anything malicious on her part.\nSo just how different were these proportions in the data? The plot above shows data from 1,641 individual shifts, on which three different variables were recorded: the shift number, whether or not there was a death on the shift, and whether or not Gilbert was working that shift.\nHere are the first 10 observations.\n\n\n# A tibble: 1,641 × 3\n   shift death staff     \n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     \n 1   626 no    no_gilbert\n 2   590 no    no_gilbert\n 3  1209 no    no_gilbert\n 4  1122 no    no_gilbert\n 5   622 no    no_gilbert\n 6  1536 no    no_gilbert\n 7  1472 no    no_gilbert\n 8   214 no    gilbert   \n 9   277 yes   no_gilbert\n10  1332 no    no_gilbert\n# ℹ 1,631 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing this data frame, we can calculate the sample proportion of shifts where Gilbert was working (257) that had a death (40) and compare them to the sample proportion of shifts where Gilbert was not working (1384) that had a death (34).\n\\[\n\\hat{p}_{gilbert} - \\hat{p}_{no\\_gilbert} = \\frac{40}{257} - \\frac{34}{1384} = .155 - .024 = .131\n\\]\n\n\nA note on notation: it’s common to use \\(\\hat{p}\\) (“p hat”) to indicate that a proportion has been computed from a sample of data.\nA difference of .131 seems dramatic, but is that within the bounds of what we might expect just due to chance? One way to address this question is to phrase it as: if in fact the probability of a death on a given shift is independent of whether or not Gilbert is on the shift, what values would we expect for the difference in observed proportions?\nWe can answer this question by using simulation. To a simulate a world in which deaths are independent of Gilbert, we can\n\nShuffle (or permute) the values in the death variable in the data frame to break the link between that variable and the staff variable.\nCalculate the resulting difference in proportion of deaths in each group.\n\nThe rationale for shuffling values in one of the columns is that if in fact those two columns are independent of one another, then it was just random chance that led to a value of one variable landing in the same row as the value of the other variable. It could just as well have been a different pairing. Shuffling captures another example of the arbitrary pairings that we could have observed if the two variables were independent of one another2.\nBy repeating steps 1 and 2 many many times, we can build up the full distribution of the values that this difference in proportions could take.\n\n\n\n\n\n\n\n\nAs expected, in a world where these two variables are independent of one another, we would expect a difference in proportions around zero. Sometimes, however, that statistic might reach values of +/- .01 or .02 or rarely .03. In the 500 simulated statistics shown above, however, none of them reached beyond +/- .06.\nSo if that’s the range of statistics we would expect in a world where random chance is the only mechanism driving the difference in proportions, how does it compare to the world that we actually observed? The statistic that we observed in the data was .131, more than twice the value of the most extreme statistic observed above.\nTo put that into perspective, we can plot the observed statistic as a vertical line on the same plot.\n\n\n\n\n\n\n\n\nThe method used above shows that the chance of observing a difference of .131 is incredibly unlikely if in fact deaths were independent of Gilbert being on shift. On this point, the statisticians on the case agreed that they could rule out random chance as an explanation for this difference. Something else must have been happening."
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/notes.html#elements-of-a-hypothesis-test",
    "href": "4-generalization/04-hypothesis-tests/notes.html#elements-of-a-hypothesis-test",
    "title": "Hypothesis Testing",
    "section": "Elements of a Hypothesis Test",
    "text": "Elements of a Hypothesis Test\nThe logic used by the statisticians in the Gilbert case is an example of a hypothesis test. There are a few key components common to every hypothesis test, so we’ll lay them out one-by-one.\nA hypothesis test begins with the assertion of a null hypothesis.\n\nNull Hypothesis\n\nA description of the chance process for generating data. Sometimes referred to as \\(H_0\\) (“H naught”).\n\n\nIt is common for the null hypothesis to be that nothing interesting is happening or that it is business as usual, a hypothesis that statisticians try to refute with data. In Gilbert case, this could be described as “The occurrence of a death is independence of the presence of Gilbert” or “The probability of death is the same whether or not Gilbert is on shift” or “The difference in the probability of death is zero, when comparing shifts where Gilbert is present to shifts where Gilbert is not present”. Importantly, the null model describes a possible state of the world, therefore the latter two versions are framed in terms of parameters (\\(p\\) for proportions) instead of observed statistics (\\(\\hat{p}\\)).\nThe hypothesis that something indeed is going on is usually framed as the alternative hypothesis.\n\nAlternative Hypothesis\n\nThe assertion that a mechanism other than the null hypothesis generated the data. Sometimes referred to as \\(H_A\\) (“H A).\n\n\nIn the Gilbert case, the corresponding alternative hypothesis is that there is “The occurrence of a death is dependent on the presence of Gilbert” or “The probability of death is different whether or not Gilbert is on shift” or “The difference in the probability of death is non-zero, when comparing shifts where Gilbert is present to shifts where Gilbert is not present”\nIn order to determine whether the observed data is consistent with the null hypothesis, it is necessary to compress the data down into a single statistic.\n\nTest Statistic\n\nA numerical summary of the observed data that bears on the null hypothesis. Under the null hypothesis it has a sampling distribution (also called a “Null Distribution”).\n\n\nIn Gilbert’s case, a difference in two proportions, \\(\\hat{p}_1 - \\hat{p}_2\\) is a natural test statistic and the observed test statistic was .131.\nIt’s not enough, though, to just compute the observed statistic. We need to know how likely this statistic would be in a world where the null hypothesis is true. This probability is captured in the notion of a p-value.\n\np-value\n\nThe probability of a test statistic as rare or even more rare than the one observed under the assumptions of the null hypothesis.\n\n\nIf the p-value is high, then the data is consistent with the null hypothesis. If the p-value is very low, however, there the statistic that was observed would be very unlikely in a world where the null hypothesis was true. As a consequence, the null hypothesis can be rejected as reasonable model for the data.\nThe p-value can be estimated using the proportion of statistics from the simulated null distribution that are as or more extreme than the observed statistic. In the simulation for the Gilbert case, there were 0 statistics greater than .131, so the estimated p-value is zero."
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/notes.html#what-a-p-value-is-not",
    "href": "4-generalization/04-hypothesis-tests/notes.html#what-a-p-value-is-not",
    "title": "Hypothesis Testing",
    "section": "What a p-value is not",
    "text": "What a p-value is not\nThe p-value has been called the most used as well as the most abused tool in statistics. Here are three common misinterpretations to be wary of.\n\n\nThe p-value is the probability that the null hypothesis is true (FALSE!)\nThis is one of the most common confusions about p-values. Graphically, a p-value corresponds to the area in the tail of the null distribution that is more extreme than the observed test statistic. That null distribution can only be created if you assume that the null hypothesis is true. The p-value is fundamentally a conditional probability of observing the statistic (or more extreme) given the null hypothesis is true. It is flawed reasoning to start with an assumption that the null hypothesis is true and arrive at a probability of that same assumption.\n\n\nA very high p-value suggests that the null hypothesis is true (FALSE!)\nThis interpretation is related to the first one but can lead to particularly wrongheaded decisions. One way to keep your interpretation of a p-value straight is to recall the distinction made in the US court system. A trial proceeds under the assumption that the defendant is innocent. The prosecution presents evidence of guilt. If the evidence is convincing the jury will render a verdict of “guilty”. If the evidence is not-convincing (that is, the p-value is high) then the jury will render a verdict of “not guilty” - not a verdict of “innocent”.\nImagine a setting where the prosecution has presented no evidence at all. That by no means indicates that the defendant is innocent, just that there was insufficient evidence to establish guilt.\n\n\nThe p-value is the probability of the data (FALSE!)\nThis statement has a semblance of truth to it but is missing an important qualifier. The probability is calculated based on the null distribution, which requires the assumption that the null hypothesis is true. It’s also not quite specific enough. Most often p-values are calculated as probabilities of test statistics, not probabilities of the full data sets.\n\n\nAnother more basic check on your understanding of a p-value: a p-value is a (conditional) probability, therefore it must between a number between 0 and 1. If you ever find yourself computing a p-value of -6 or 3.2, be sure to pause and revisit your calculations!"
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/notes.html#one-test-many-variations",
    "href": "4-generalization/04-hypothesis-tests/notes.html#one-test-many-variations",
    "title": "Hypothesis Testing",
    "section": "One test, many variations",
    "text": "One test, many variations\nThe hypothesis testing framework laid out above is far more general than just this particular example from the case of Kristen Gilbert where we computed a difference in proportions and used shuffling (aka permutation) to build the null distribution. Below are just a few different research questions that could be addressed using a hypothesis test.\n\nPollsters have surveyed a sample of 200 voters ahead of an election to assess their relative support for the Republican and Democratic candidate. The observed difference in those proportions is .02. Is this consistent with the notion of evenly split support for the two candidates, or is one decidedly in the lead?\nBrewers have tapped 7 barrels of beer and measured the average level of a compound related to the acidity of the beer as 610 parts per million. The acceptable level for this compound is 500 parts per million. Is this average of 610 consistent with the notion that the average of the whole batch of beer (many hundreds of barrels) is at the acceptable level of this compound?\nA random sample of 40 users of a food delivery app were randomly assigned two different versions of a menu where they entered the amount of their tip: one with the tip amount in ascending order, the other in descending order. The average tip amount of those with the menu in ascending order was found to be $3.87 while the average tip of the users in the descending order group was $3.96. Could this difference in averages be explained by chance?\n\nAlthough the contexts of these problems are very different, as are the types of statistics they’ve calculated, they can still be characterized as a hypothesis test by asking the following questions:\n\nWhat is the null hypothesis used by the researchers?\nWhat is the value of the observed test statistic?\nHow did researchers approximate the null distribution?\nWhat was the p-value, what does it tell us and what does it not tell us?"
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/notes.html#summary",
    "href": "4-generalization/04-hypothesis-tests/notes.html#summary",
    "title": "Hypothesis Testing",
    "section": "Summary",
    "text": "Summary\nIn classical statistics there are two primary tools for assessing the role that random variability plays in the data that you have observed. The first is the confidence interval, which quantifies the amount of uncertainty in a point estimate due to the variability inherent in drawing a small random sample from a population. The second is the hypothesis test, which postings a specific model by which the data could be generated, then assesses the degree to which the observed data is consistent with that model.\nThe hypothesis test begins with the assertion of a null hypothesis that describes a chance mechanism for generating data. A test statistic is then selected that corresponds to that null hypothesis. From there, the sampling distribution of that statistic under the null hypothesis is approximated through a computational method (such as using permutation, as shown here) or one rooted in probability theory (such as the Central Limit Theorem). The final result of the hypothesis test procedure is the p-value, which is approximated as the proportion of the null distribution that is as or more extreme than the observed test statistic. The p-value measures the consistency between the null hypothesis and the observed test statistic and should be interpreted carefully.\nA postscript on the case of Kristen Gilbert. Although the hypothesis test ruled out random chance as the reason for the spike in deaths under her watch, it didn’t rule out other potential causes for that spike. It’s possible, after all, that the nightshifts that Gilbert was working happen to be the time of day when cardiac arrests are more common. For this reason, the statistical evidence was never presented to the jury, but the jury nonetheless found her guilty based on other evidence presented in the trial."
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/notes.html#the-ideas-in-code",
    "href": "4-generalization/04-hypothesis-tests/notes.html#the-ideas-in-code",
    "title": "Hypothesis Testing",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nA hypothesis test using permutation can be implemented by introducing one new step into the process used for calculating a bootstrap interval. The key distinction is that in a hypothesis test the researchers puts forth a model for how the data could be generated. That is the role of hypothesize().\n\n\n\n\nhypothesize()\nA function to place before generate() in an infer pipeline where you can specify a null model under which to generate data. The one necessary argument is\n\n\nnull: the null hypothesis. Options include \"independence\" and \"point\".\n\nThe following example implements a permutation test under the null hypothesis that there is no relationship between the body mass of penguins and their\n\nlibrary(tidyverse)\nlibrary(stat20data)\nlibrary(infer)\n\npenguins |&gt;\n  specify(response = body_mass_g,\n          explanatory = sex) |&gt;\n  hypothesize(null = \"independence\")\n\nResponse: body_mass_g (numeric)\nExplanatory: sex (factor)\nNull Hypothesis: independence\n# A tibble: 333 × 2\n   body_mass_g sex   \n         &lt;dbl&gt; &lt;fct&gt; \n 1        3750 male  \n 2        3800 female\n 3        3250 female\n 4        3450 female\n 5        3650 male  \n 6        3625 female\n 7        4675 male  \n 8        3200 female\n 9        3800 male  \n10        4400 male  \n# ℹ 323 more rows\n\n\nObserve:\n\nThe output is the original data frame with new information appended to describe what the null hypothesis is for this data set.\nThere are other forms of hypothesis tests that you will see involving a \"point\" null hypothesis. Those require adding additional arguments to hypothesize().\nCalculating an observed statistic\nLet’s say for this example you select as your test statistic a difference in means, \\(\\bar{x}_{female} - \\bar{x}_{male}\\). While you can use tools you know - group_by() and summarize() to calculate this statistic, you can also recycle much of the code that you’ll use to build the null distribution with infer.\n\nobs_stat &lt;- penguins |&gt;\n  specify(response = body_mass_g,\n          explanatory = sex) |&gt;\n  calculate(stat = \"diff in means\")\n\nobs_stat\n\nResponse: body_mass_g (numeric)\nExplanatory: sex (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 -683.\n\n\nCalculating the null distribution\nTo generate a null distribution of the kind of differences in means that you’d observe in a world where body mass had nothing to do with sex, just add the hypothesis with hypothesize() and the generation mechanism with generate().\n\nnull &lt;- penguins |&gt;\n  specify(response = body_mass_g,\n          explanatory = sex) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 500, type = \"permute\") |&gt;\n  calculate(stat = \"diff in means\")\n\nnull\n\nResponse: body_mass_g (numeric)\nExplanatory: sex (factor)\nNull Hypothesis: independence\n# A tibble: 500 × 2\n   replicate    stat\n       &lt;int&gt;   &lt;dbl&gt;\n 1         1  -59.9 \n 2         2 -125.  \n 3         3   68.9 \n 4         4   37.1 \n 5         5  129.  \n 6         6   36.5 \n 7         7   -7.08\n 8         8   60.8 \n 9         9  -16.7 \n10        10  -63.8 \n# ℹ 490 more rows\n\n\nObserve:\n\nThe output data frame has reps rows and 2 columns: one indicating the replicate and the other with the statistic (a difference in means)."
  },
  {
    "objectID": "4-generalization/04-hypothesis-tests/notes.html#footnotes",
    "href": "4-generalization/04-hypothesis-tests/notes.html#footnotes",
    "title": "Hypothesis Testing",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis case study appears in Statistics in the Courtroom: United States v. Kristen Gilbert by Cobb and Gelbach, published in Statistics: A Guide to the Unknown by Peck et. al.↩︎\nThe technical notion that motivates the use of shuffling is a slightly more general notion than independence called exchangability. The distinction between these two related concepts is a topic in a course in probability.↩︎"
  },
  {
    "objectID": "4-generalization/02-confidence-intervals/notes.html",
    "href": "4-generalization/02-confidence-intervals/notes.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "The process of generalizing from a statistic of a sample to a parameter of a population is known as statistical inference. The parameter of interest could be a mean, median, proportion, correlation coefficient, the coefficient of a linear model . . . the list goes on. In the scenario that unfolded in Pimentel Hall, the parameter was the mean year of the 527 students in the class. The process of estimating that parameter by calculating the sample mean of the 18 students who decided to sit in the front row that day induces a sampling distribution.\nThis sampling distribution captures the two sources of error that creep in while generalizing. The horizontal offset from the true population parameter (the red line) to the mean of the sampling distribution (the gold triangle) represents the bias. The spread of the sampling distribution represents the variation. In these lecture notes you’ll learn how to quantify sampling variability using two common tools.\nTo focus on the variation, let’s introduce a second example, one in which we will not need to worry about bias."
  },
  {
    "objectID": "4-generalization/02-confidence-intervals/notes.html#a-simple-random-sample",
    "href": "4-generalization/02-confidence-intervals/notes.html#a-simple-random-sample",
    "title": "Confidence Intervals",
    "section": "A Simple Random Sample",
    "text": "A Simple Random Sample\nRestaurants in San Francisco\nEvery year, the city of San Francisco’s health department visits all the restaurants in the city and inspects them for food safety. Each restaurant is given an inspection score; these range from 100 (perfectly clean) to 48 (serious potential for contamination). We have these scores from 2016. Let’s build up to the sampling distribution bit by bit.\n\nThe Population Distribution\nOur population consists of the restaurants in San Francisco. Since the data are published online for all restaurants, we have a census1 of scores for every restaurant in the city.\n\n\n\n\n\n\n\n\nThe population distribution is skewed left with a long left tail. The highest possible score is 100. It appears that even scores are more popular than odd scores for scores in the 90s; in fact there are no scores of 99, 97, and 95.\nWe can calculate two parameters of this population:\nPopulation parameters, like the parameters of probability distributions, are usually given a Greek letter. The population mean is \\(\\mu\\), said “myoo”, and the population standard deviation is \\(\\sigma\\), said “sigma”.\n\nThe population mean, \\(\\mu\\), is 87.6.\nThe population SD, \\(\\sigma\\), is 8.9.\nThe Empirical Distribution\nAlthough we have data on all of the restaurants in the city, imagine that you’re an inspector who has visited a simple random sample of 100 restaurants. That is, you draw 100 times without replacement from the population, with each unit equally likely to be selected. This leads to a representative sample that will have no selection bias.\nThe distribution of this sample (an empirical distribution) looks like:\n\n\n\n\n\n\n\n\nThe sample statistics here are:\nWhile parameters are symbolized with Greek letters, statistics are usually symbolized with Latin letters.\n\nThe sample mean, \\(\\bar{x}\\), is 86.27.\nThe sample SD, \\(s\\), is 9.9.\n\nObserve that the empirical distribution resembles the population distribution because we are using a sampling method without with selection bias. It’s not a perfect match but the shape is similar. The sample average (\\(\\bar{x}\\)) and the sample SD (\\(s\\)) are also close to but not the same as the population average (\\(\\mu\\)) and SD (\\(\\sigma\\)).\nThe Sampling Distribution\nIf you compared your sample to that of another inspector who visited 100 restaurants, their sample would not be identical to yours, but it would still resemble the population distribution, and its \\(\\bar{x}\\) and \\(s\\) would be close to those of all the restaurants in the city.\nThe distribution of the possible values of the \\(\\bar{x}\\) of a simple random sample of 100 restaurants is the sampling distribution of the mean (of the sample). We can use it to, for example, find the chance that the sample mean will be over 88, or the chance that the sample mean will be between 85 and 95.\nOrdinarily this distribution takes some work to create, but in this thought-experiment have have access to the full population, so we can simply use the computer to simulate the process. We repeat 100,000 times the process of drawing a simple random sample of 100 restaurants. The full distribution looks like:\n\n\n\n\n\n\n\n\nWe can consider numerical summaries of this distribution:\n\nThe mean of the sampling distribution is 87.6.\nThe SD of the sampling distribution, which is called the Standard Error (SE), is 0.9. This convention of using a different name for the SD for the distribution of a statistic helps keep straight which kind of standard deviation we’re talking about.\n\nObserve that the sampling distribution of \\(\\bar{x}\\) doesn’t look anything like the population or sample. Instead, it’s roughly symmetric in shape with a center that matches \\(\\mu\\), and a small SE. The small size of the SE reflects the fact that the \\(\\bar{x}\\) tends to be quite close to \\(\\mu\\).\nAgain, the sampling distribution provides the distribution for the possible values of \\(\\bar{x}\\). From this distribution, we find that the chance \\(\\bar{x}\\) is over 88 is about 0.33, and the chance \\(\\bar{x}\\) is between 85 and 95 is roughly, 1.\nPutting the Three Panels Together\nLet’s look at these three aspects of this process side-by-side.\n\n\n\n\n\n\n\n\n\n\n\nPopulation\nEmpirical\nSampling\n\n\n\nShape\nleft skew\nleft skew\nbell-shaped / normal\n\n\nMean\n\n\\(\\mu\\) = 87.6\n\n\\(\\bar{x}\\) = 86.27\n87.6\n\n\nSD\n\n\\(\\sigma\\) = 8.9\n\n\\(s\\) = 9.9\n0.89\n\n\n\nObserve that:\n\n\\(\\mu\\) and the mean of the sampling distribution are roughly the same.\n\n\\(\\sigma\\) and the SE of the sample averages are related in the following way2:\n\\[SE(\\bar{x}) \\approx \\frac{\\sigma}{\\sqrt{n}}\\]\n\nThe histogram of the sample averages is not skewed like the histogram of the population, on the contrary, it is symmetric and bell-shaped, like the normal curve.\nThe histogram of our sample of 100 resembles the population histogram.\nSince 100 is a pretty large sample,\n\n\\[\n\\begin{aligned}\n\\mu &\\approx \\bar{x} \\\\\n\\sigma &\\approx s \\\\\n\\end{aligned}\n\\]\nUp until this point, we’ve worked through this thought experiment with the unrealistic assumption that we know the population. Now we’re ready to make inferences in a setting where we don’t know the population."
  },
  {
    "objectID": "4-generalization/02-confidence-intervals/notes.html#inference-for-a-population-average",
    "href": "4-generalization/02-confidence-intervals/notes.html#inference-for-a-population-average",
    "title": "Confidence Intervals",
    "section": "Inference for a Population Average",
    "text": "Inference for a Population Average\nDrawing on our understanding of the thought-experiment, we ask:\nWhat happens when you don’t see the population, you just have your sample, and you want to make an inference about the population?\nWe have serious gaps in our procedure for learning about the sampling distribution!\n\n\n\n\nTo start, we know we can use the sample average, \\(\\bar{x}\\), to infer the population average, \\(\\mu\\). This is called a point estimate for the population parameter.\nBut can we do better than that? Can we bring in more of the information that we have learned from the thought-experiment? For example, can we accompany our point estimate with a sense of its accuracy? Ideally, this would be the SE of the sample mean. Unfortunately, we don’t know the SE because it depends on \\(\\sigma\\). So now what do we do?\nStandard Error\nThe thought-experiment tells us that \\(s\\) is close to the \\(\\sigma\\) (when you have a SRS). So we can substitute the \\(s\\) into the formula for the SE.\n\\[ SE(\\bar{x}) \\approx \\frac{s}{\\sqrt{n}}\\]\nWhen presenting our findings, you might say, that based on a SRS of 100 restaurants in San Francisco, the average food safety score is estimated to be 86 with a standard error of about 1.\nSuppose someone took a sample of 25 restaurants and provided an estimate of the average food safety score. Is that only 1/4 as accurate because the sample is 1/4 the size of ours?\nSuppose someone took a sample of 100 restaurants in New York City where there are 50,000 restaurants (this is a made up number). Is their estimate only 1/10 as accurate because the number of units in the population is 10 times yours?\nWe can use the formula for the SE to answer these questions. In the table below, we have calculated SEs for a generic value of \\(\\sigma\\) and various choices of the population size and sample size.\n\n\nPopulation Size (\\(N\\))\n\nSample Size (\\(n\\))\n\n\n\n\n\n25\n100\n400\n\n\n500\n\\(SE = \\sigma/5\\)\n\\(SE = \\sigma/10\\)\n\\(SE = \\sigma/20\\)\n\n\n5,000\n\\(SE = \\sigma/5\\)\n\\(SE = \\sigma/10\\)\n\\(SE = \\sigma/20\\)\n\n\n50,000\n\\(SE = \\sigma/5\\)\n\\(SE = \\sigma/ 10\\)\n\\(SE = \\sigma/ 20\\)\n\n\n\nWhat do you notice about the relationship between sample size and population size and SE?\n\nThe absolute size of the population doesn’t enter into the accuracy of the estimate, as long as the sample size is small relative to the population.\nA sample of 400 is twice as accurate as a sample of 100, which in turn is twice as accurate as a sample of 25 (assuming the population is relatively much larger than the sample). The precision of estimating the population mean improves according to the square root of the sample size.\nConfidence Intervals\nConfidence intervals bring in more information from the thought-experiment. The confidence interval provides an interval estimate, instead of a point estimate, that is based on the spread of the sampling distribution of the statistic.\nWe have seen that the sampling distribution takes a familiar shape: that of the normal curve (also called the bell curve)3. Therefore we can fill in some of the holes in the thought-experiment with approximations.\n\n\n\n\nThis is the Central Limit Theorem in action. The CLT states that sums of random variables become normally distributed as \\(n\\) increases. Conveniently enough, most useful statistics are some version of a sum: \\(\\bar{x}\\) is a sum divided by \\(n\\) and \\(\\hat{p}\\) is a sum of variables that take values 0 or 1, divided by \\(n\\). This powerful mathematical result enables one of the most popular methods of constructing confidence intervals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormal Confidence Intervals\nWhen the sampling distribution is roughly normal in shape, then we can construct an interval that expresses exactly how much sampling variability there is. Using our single sample of data and the properties of the normal distribution, we can be 95% confident that the population parameter is within the following interval.\n\n\nThe number 1.96 doesn’t come out of thin air. Refer to the notes on the Normal Distribution to understand the origins.\n\\[[\\bar{x} - 1.96 SE, \\bar{x} + 1.96SE]\\]\nSo for a sample where the sample mean is 86 and the 95% confidence interval is [84.3, 88.2 ], you would say,\n\nI am 95% confident that the population mean is between 84.3 and 88.2.\n\nFor the particular interval that you have created, you don’t know if it contains the population mean or not. This is why we use the term confidence to describe it instead of probability. Probability comes into play when taking the sample, after that our confidence interval is a known observed value with nothing left to chance.\nConfidence not Probability\nTo be more precise about what is meant by “confidence”, let’s take 100 samples of size 25 from the restaurant scores, and calculate a 95% confidence interval for each of our 100 samples. How many of these 100 confidence intervals do you think will include the population mean?\nLet’s simulate it! At the bottom of the plot below, the horizontal line at the \\(y = 1\\) indicates the coverage of the confidence interval from the first sample. It stretches from roughly 84 to 91. The line above it at \\(y = 2\\) indicates the coverage of the confidence interval that resulted from the second sample, from roughly 85 to 92.5. Both of these confidence intervals happened to cover the true population parameter, indicated by the black vertical line.\n\n\n\n\n\n\n\n\nAs we look up the graph through the remaining intervals, we see that 95 of the 100 confidence intervals cover the population parameter. This is by design. If we simulate another 100 times, we may get a different number, but it is likely to be close to 95."
  },
  {
    "objectID": "4-generalization/02-confidence-intervals/notes.html#inference-for-a-population-proportion",
    "href": "4-generalization/02-confidence-intervals/notes.html#inference-for-a-population-proportion",
    "title": "Confidence Intervals",
    "section": "Inference for a Population Proportion",
    "text": "Inference for a Population Proportion\nTo gain practice with making confidence intervals, we turn to another example. This time we sample from a population where the values are 0s and 1s. You will see that the process is very much the same, although there are a few simplifications that arise due to the nature of the population.\nSuppose we only want to eat at restaurants with food safety scores above 95. Let’s make a confidence interval for the proportion of restaurants in San Francisco with scores that are “excellent” (scores over 95). To tackle this problem, we can modify our population. Since we need only to keep track of whether a score is excellent, we can replace the scores on the tickets with 0s and 1s, where 1 indicates an excellent score. Of the 5766 restaurants in San Francisco, 1240 are excellent. We can think of our population as a box with 5766 tickets in it, and 1240 are marked 1, and 4526 are marked 0. This time let’s take a SRS of 25.\nThe thought-experiment appears as\n\n\n\n\n\n\n\n\n\n\n\nPopulation\nEmpirical\nSampling\n\n\n\nShape\nleft skew\nleft skew\nbell-shaped / normal\n\n\nMean\n\n\\(p\\) = 0.22\n\n\\(\\hat{p}\\) = 0.36\n0.22\n\n\nSD\n\n\\(\\sigma = \\sqrt{p(1-p)}\\) = 0.41\n\n\\(s\\) = 0.49\n0.08\n\n\n\nIn the special case of a 0-1 box:\n\nThe population average is the proportion of 1s in the box, let’s call this parameter \\(p\\).\nThe taking a draw from the population distribution taking a draw from a Bernoulli random variable, so \\(\\sigma = \\sqrt{p(1-p)}\\).\nThe sampling distribution has mean \\(p\\).\nThe sampling proportion, \\(\\hat{p}\\), is similar to \\(p\\).\nThe SE of the sample proportion4 is approximately \\(SE(\\hat{p}) = \\frac{\\sqrt{\\hat{p}(1-\\hat{p})}}{\\sqrt{n}}\\).\n\nWith an equation to estimate \\(SE\\) from our data in hand, we can form a 95% confidence interval.\n\\[\\left[\\hat{p} - 1.96 \\frac{\\sqrt{\\hat{p}(1-\\hat{p})}}{\\sqrt{n}}, \\hat{p} + 1.96 \\frac{\\sqrt{\\hat{p}(1-\\hat{p})}}{\\sqrt{n}}\\right]\\]"
  },
  {
    "objectID": "4-generalization/02-confidence-intervals/notes.html#summary",
    "href": "4-generalization/02-confidence-intervals/notes.html#summary",
    "title": "Confidence Intervals",
    "section": "Summary",
    "text": "Summary\nIn these notes, we have restricted ourselves to the simple random sample, where the only source of error that we’re concerned with is sampling variability. We outlined two tools for estimating that variability: the standard error (SE) and the confidence interval.\nWe saw how the size of the sample impacts the standard error of the estimate. The larger the sample, the more accurate our estimates are and in particular the accuracy improves according to \\(1/\\sqrt{n}\\). We also found that the size of the population doesn’t impact the accuracy, as long as the sample is small compared to the population.\nWe made confidence intervals for population averages and proportions using the normal distribution. This approach can be extended to other properties of a population, such as the median of a population, or the coefficient in a regression equation.\nThe confidence intervals that we have made are approximate in the following sense:\n\nWe’re approximating the shape of the unknown sampling distribution with the normal curve.\nThe SD of the sample is used in place of the SD of the population in calculating the SE of the statistic.\n\nThere are times when we are unwilling to make the assumption of normality. This is the topic of the next set of notes."
  },
  {
    "objectID": "4-generalization/02-confidence-intervals/notes.html#footnotes",
    "href": "4-generalization/02-confidence-intervals/notes.html#footnotes",
    "title": "Confidence Intervals",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe terms census refers to a setting where you have access to the entire population.↩︎\n\nThis approximation becomes equality for a random sample with replacement. When we have a SRS, the exact formula is \\(SE(\\bar{x}) = \\sqrt{\\frac{N-n}{N-1}} \\sigma/\\sqrt{n}\\).\nThis additional term, called the finite population correction factor, adjusts for the fact that we are drawing without replacement. Here \\(N\\) is the number of tickets in the box (the size of the population) and \\(n\\) is the number of tickets drawn from the box (the size of the sample).\nTo help make sense of this correction factor, think about the following two cases:\n\nDraw \\(N\\) tickets from the box (that is, \\(n = N\\)).\nDraw only one ticket from the box.\n\nWhat happens to the SE in these two extreme cases?\nIn the first case, you will always see the entire population if you are drawing without replacement. So, the sample mean will exactly match the population mean. The sampling distribution has no variation, so \\(SE = 0\\).\nIn the second case, since you take only one draw from the box, it doesn’t matter if you replace it or not. So the SE for a SRS should match the SE when sampling with replacement in this special case. In settings when \\(N\\) is large relative to \\(n\\), it effectively behaves as if you are sampling with replacement.↩︎\n\nThis is not always the case. We’ll come back to this point later.↩︎\n\nThis calculation results from casting the total number of 1’s in a sample of size \\(n\\) as a binomial random variable with success probability \\(p\\). Call that random variable \\(Y\\). The variance of a binomial random variable is \\(Var(Y) = np(1-p)\\). Observing that sample proportion can be considered a binomial count divided by \\(n\\), and applying the properties of variance, we can find the variance of \\(\\hat{p}\\) as,\n\\[\\begin{eqnarray}\nVar(\\hat{p}) &= Var(\\frac{1}{n}Y) \\\\\n&= \\frac{1}{n^2}Var(Y) \\\\\n&= \\frac{1}{n^2}np(1-p) \\\\\n&= \\frac{p(1-p)}{n}\n\\end{eqnarray}\\]\nSo the standard error can be calculated as:\n\\[\\begin{eqnarray}\nSE(\\hat{p}) &= \\sqrt{Var(\\hat{p})} = \\sqrt{\\frac{p(1-p)}{n}}\n\\end{eqnarray}\\]\nWhen estimating the SE from data, we plug in \\(\\hat{p}\\) for \\(p\\).↩︎"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/notes.html",
    "href": "4-generalization/03-bootstrapping/notes.html",
    "title": "Bootstrapping",
    "section": "",
    "text": "The confidence intervals we created in the last set of notes relied upon the normal distribution to build an interval to capture a population parameter with a particular confidence. Hopefully the thought-experiments assured you that sampling distributions often follow a bell-shaped distribution, so the use of the normal distribution was justified. If you’re itching for a formal explanation for why the sampling distribution will follow a normal distribution, this is precisely the role of one of the most central mathematical results in statistics: the Central Limit Theorem.\nThe Central Limit Theorem says, in brief, that sums of random variables will follow a normal distribution as the sample size grows large1. A vast array of useful statistics - means, proportions, differences in two means, differences in two proportions - can be cast as sums of random variables, so at large sample sizes we can be confident that the normal distribution is a good approximation of the sampling distribution. But what happens when you’re looking at a statistic that cannot be cast as a sum of random variables? What if your sample size is not large? How else can you approximate the sampling distribution of a statistic?\nUntil several decades ago, the toolbox for answering these questions was limited. With the advent of powerful computers and a brilliant and simple insight into the relationship between the sample and the population, however, we have a new tool for assessing sampling variability. That tool is the bootstrap."
  },
  {
    "objectID": "4-generalization/03-bootstrapping/notes.html#the-bootstrap",
    "href": "4-generalization/03-bootstrapping/notes.html#the-bootstrap",
    "title": "Bootstrapping",
    "section": "The Bootstrap",
    "text": "The Bootstrap\nThe bootstrap is based on the observation that if your sample is representative of the population, then the empirical distribution should be a good stand-in for the population distribution. One can then simulate the process of drawing multiple samples from the population by drawing new samples (called resamples) from the empirical distribution.\n\n\n\n\n\nThe Bootstrap Algorithm\n\nA procedure used to assess sampling variability in statistics. To bootstrap a statistic,\n\nTreat the sample as a bootstrap population\n\nDraw a new sample (with replacement) from the bootstrap population\nCalculate the statistic of interest on the new sample\nRepeat steps 2 and 3 many times to build up a bootstrap sampling distribution\n\n\n\n\nThe name of the procedure is derived from the idiom, “to pull yourself up by your own bootstraps”. This illustrates the somewhat miraculous nature of this procedure. While in reality you only ever get to see a single sample drawn from the population, the bootstrap allows you to use that sample to generate many more samples through the process of sampling with replacement.\nTo illustrate this procedure, consider the toy example below, where we aim to estimate the sampling variability in calculating a proportion using a sample of size \\(n=5\\) penguins.\n\n\n\n\nOn the left is the data frame containing the original sample containing one column of the names that identify each unit and a second column with the variable of interest, a variable generically called \\(X\\) that takes values 1 and 0. The observed statistic in this data frame is \\(\\hat{p}=2/5\\). This original sample will serve as the bootstrap population.\nIn step two of the algorithm, we draw a sample of size \\(n=5\\) from the bootstrap population, with replacement. This first bootstrap sample is shown at the top and features Ida, Gus, Abe, Gus again, and Ola. This is an important feature of sampling with replacement: some units might be drawn multiple times (Gus) and others might not be drawn at all (Luz). From this first bootstrap sample, we can compute the first bootstrap statistic \\(\\hat{p}_{b1} = 1/5\\). This is step three.\nFor step four we repeat this process of drawing a bootstrap sample and calculating another bootstrap statistic. In the diagram, two additional statistics are calculated, but in practice this process will be repeated many many times. The result of this procedure is the bootstrap distribution, on the right, which is the collection of all of the statistics you observed across the different bootstrap samples.\nNote this just a toy example. With \\(n=5\\), the sample is unlikely to represent the important features of the population, and the bootstrap should not be applied. At a more reasonable sample size, however, this bootstrap distribution will become a good approximation of the sampling distribution and therefore can be used to calculate a confidence interval.\n\nBootstrap Confidence Interval (percentile method)\n\nFor a 95% confidence interval, the interval spans the middle 95% of the bootstrap statistics which is equivalent to finding the 2.5% and 97.5% quantiles of the bootstrap distribution. The confidence level can be adapted by modifying the quantiles accordingly.\n\n\nLet’s see the bootstrap in practice in two settings with two very different statistics.\nExample 1: Food Safety Scores\nLet’s create the bootstrap sampling distribution for the example of food safety scores from the earlier notes on confidence intervals. In this example, we looked at a sample of 100 food safety scores drawn from all restaurants in San Francisco. When bootstrapping, that empirical distribution becomes the bootstrap population.\n\n\n\n\n\n\n\n\nNext, to approximate the bootstrap sampling distribution, we proceed with a simulation. We take a sample of 100, this time with replacement, from the bootstrap population, and we compute the mean food safety score. This process gets repeated 500 times, and the distribution of the 500 bootstrap means gives us an estimate of the bootstrap sampling distribution.\nThe bootstrap sampling distribution looks like the following.\nThis approach to bootstrapping uses the infer R package. Details of its use can be found in the Ideas in Code section at the end of these notes.\n\n\n\n\n\n\n\n\nA few things to note about this bootstrap sampling distribution:\n\nThe overall shape of the bootstrap sampling distribution mirrors the general shape of the true sampling distribution from the last notes.\nThe mean of the sampling distribution, 86.23, nearly matches the mean of the bootstrap population, 86.27, which in turn matches the original sample mean. But, it does not quite match the mean of the original population, 87.6.\nThe SE of the bootstrap sampling distribution, 0.99, matches the SD(bootstrap pop)/\\(\\sqrt{100}\\), which is also 0.99. This bootstrapped SE is a decent estimate of the SE of the true sampling distribution, 0.89.\n\nTo form a 95% confidence interval for the population mean, we can find the quantiles that mark off the middle 95% of the bootstrap distribution. For the distribution above, this would be:\n\n.get_ci(boot, level = .95)\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     84.4     88.0\n\n\nIf we compare this confidence interval to the one we created using the normal curve, it matches exactly (when rounded to the nearest tenths place). This is because we’re in a setting where the sample size is large enough for the sample to be a good stand-in for the population (so the bootstrap is accurate) and also large enough for the Central Limit Theorem to kick in (so the normal approximation is also accurate).\nNext we provide an example where we truly don’t know the population and where we need a confidence interval for a statistic that’s a bit more complex than the mean.\nExample 2: Adelie Penguins’ Body Mass\nWe wish to develop a model for the body mass of a penguin as a function of its other body measurements. Since the three kinds of penguins have quite different body mass distributions, we restrict ourselves to the Adelie penguins.\nThe best one-variable linear model to fit body mass is the model that explains a penguin’s body mass based on its sex. This model has an \\(R^2\\) of 0.54. But, when we add information about the penguin’s flipper length, the model does improve somewhat. The coefficients of the model with sex and flipper length as explanatory variables for body mass is:\n\n\n\nCall:\nlm(formula = body_mass_g ~ sex + flipper_length_mm, data = penguins_adelie)\n\nCoefficients:\n      (Intercept)            sexmale  flipper_length_mm  \n           305.09             599.34              16.31  \n\n\nRecall that this model is equivalent to fitting two parallel lines, one for each sex. Below is a scatter plot of body mass and flipper length with the two fitted lines added.\n\n\n\n\n\n\n\n\nThe lines aren’t particularly steep in slope. If the researchers went out and collected another set of data on the penguins, we would expect the relationship between body mass and flipper length to be roughly the same, but not exactly the same. The scatter plot would look a bit different, and the slope of these parallel lines would be a bit different too.\nSuppose we want to make an inference about the true slope of these lines. That is, we want to make an inference for all of the penguins in Antarctica about the coefficient for flipper length in the model:\nbody mass ~ sex + flipper length\nThe linear model that we fitted on the Adelie penguins collected for the research study gives us a point estimate for this coefficient, but a confidence interval has an added advantage. A confidence interval incorporates the variability in the point estimate. If 0 were found to be in the confidence interval, then it calls into question whether there is a relationship between flippter length and body mass when controlling for sex.\nHow can we find a confidence interval for the coefficient of flipper length?\nLet’s return to the box model and the associated thought-experiment. We would describe the box as:\n\none ticket for every Adelie penguin in Antarctica\neach ticket has the penguin’s body mass, flipper length, and sex written on it (we are ignoring the other measurements)\nthe number of tickets in the box is unknown, but it is known to be a large number that is thought to be over 100,000.\nthe population distribution of each variable (body mass, flipper length, and sex) is unknown\nthe joint population distribution of how these three measurements vary together is also unknown\n\nSince we are missing all of this information about the population, we use the bootstrap. What does that mean in this situation?\n\nThe data frame for the 146 Adelie penguins is our bootstrap population.\nTo sample from the bootstrap population we choose rows from the data frame at random with replacement.\nThe bootstrap statistic is the coefficient for flipper length from fitting the linear model on a bootstrap sample.\nThe bootstrap sampling distribution is the probability distribution of the bootstrap statistic.\n\nLet’s simulate the bootstrap sampling distribution with 500 rounds of drawing 146 penguins with replacement from the data frame. For each bootstrap sample of 146 penguins, we fit the linear model and retrieve the coefficient for flipper length. The resulting bootstrap sampling distribution of the coefficient for flipper length looks like the following.\n\n\n\n\n\n\n\n\nNotice that the distribution looks roughly normal. This is in part because the sample size is reasonably large and the coefficient from a linear model is an average of sorts.\nWe can make a 99% bootstrap confidence interval by finding the 0.5th percentile and the 99.5th percentile of the bootstrap sampling distribution.\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     8.04     26.8\n\n\nWhile the confidence interval is quite wide, running from 8.04 to 26.82, it does not contain 0. This implies that information about the flipper length is a reasonable addition to the model for body mass."
  },
  {
    "objectID": "4-generalization/03-bootstrapping/notes.html#summary",
    "href": "4-generalization/03-bootstrapping/notes.html#summary",
    "title": "Bootstrapping",
    "section": "Summary",
    "text": "Summary\nIn these notes, we have introduced the bootstrap as a technique for approximating confidence intervals. The bootstrap is a powerful tool, but it is important to keep in mind that it is not a cure-all. Here are some cautions about using the bootstrap:\n\nWhile a SRS (and other random mechanisms for selecting data) typically gives us representative data, that is not always the case. We may be unlucky and get an oddball sample. The bootstrap cannot recover from this problem. The bootstrap population will not look like the true population, and so the bootstrap sampling distribution will not be useful. Unfortunately, we don’t know when this is happening. However, this is usually not a problem for large samples.\nThe bootstrap works well when the statistic is a mean, or something like a mean, such as a regression coefficient or a standard deviation. The bootstrap tends to have difficulties when the statistic is influenced by outliers, the parameter is based on extreme values of a population distribution, or the sampling distribution of the statistic is far from bell-shaped.\nThe bootstrap cannot overcome a lack of randomness in the selection of the sample. The process of taking a bootstrap sample needs to mimic the selection process for taking the original sample. If a sample was not selected by a random process or the sample was chosen by a more complex process than the one used in bootstrapping, then the bootstrap can fail.\nA rule of thumb for the number of resamples needed for a reasonable bootstrap distribution is 10,000, however for the use of this class, use 500. Too few bootstrap samples can create problems for getting a good bootstrap sampling distribution."
  },
  {
    "objectID": "4-generalization/03-bootstrapping/notes.html#section",
    "href": "4-generalization/03-bootstrapping/notes.html#section",
    "title": "Bootstrapping",
    "section": "—————————",
    "text": "—————————"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/notes.html#the-ideas-in-code",
    "href": "4-generalization/03-bootstrapping/notes.html#the-ideas-in-code",
    "title": "Bootstrapping",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nThese notes utilize several functions from the infer library, which can be used to calculate confidence intervals and conduct hypothesis tests. It can be loaded with library(infer).\nWith infer, each step in the bootstrap procedure is controlled by one of four functions.\n\n\n\n\nFor a comprehensive list of templates that you can use to form intervals, see the online documentation: https://infer.netlify.app/articles/observed_stat_examples.html.\nspecify()\nThe specify function allows you to specify which column of a data frame you are using as your response variable (your variable of interest). When looking at the relationship between two variables you will specify both the response and the explanatory variables. As such, the main arguments are response and explanatory.\n\npenguins |&gt;\n  specify(response = bill_length_mm)\n\nResponse: bill_length_mm (numeric)\n# A tibble: 342 × 1\n   bill_length_mm\n            &lt;dbl&gt;\n 1           39.1\n 2           39.5\n 3           40.3\n 4           36.7\n 5           39.3\n 6           38.9\n 7           39.2\n 8           34.1\n 9           42  \n10           37.8\n# ℹ 332 more rows\n\n\nObserve that the output of specify is essentially the same data frame that went in. the only difference is that bill_length_mm is tagged as the response variable. That will be useful for downstream functions.\ngenerate()\nThe generate function generates many replicate data frames using simulation, the bootstrap procedure, or shuffling. Note that it must follow specify() so that it knows which column(s) to use.\nUseful functions include:\n\n\nreps: the number of data set replicates to generate. Generally set this to 500 when making confidence intervals.\n\ntype: the mechanism used to generate new data. Either \"bootstrap\", \"draw\", or \"permute\".\n\n\npenguins |&gt;\n  specify(response = bill_length_mm) |&gt;\n  generate(reps = 2, type = \"bootstrap\")\n\nResponse: bill_length_mm (numeric)\n# A tibble: 684 × 2\n# Groups:   replicate [2]\n   replicate bill_length_mm\n       &lt;int&gt;          &lt;dbl&gt;\n 1         1           51.3\n 2         1           41.1\n 3         1           46.2\n 4         1           48.6\n 5         1           43.2\n 6         1           47.5\n 7         1           38.1\n 8         1           44  \n 9         1           42.4\n10         1           35.9\n# ℹ 674 more rows\n\n\nObserve:\n\nthe output data frame has two columns, replicate, which keeps track of the replicate (1 or 2 here) and bill_length_mm.\nthe number of rows in the resulting data frame is the \\(n \\times reps\\), so this data frame is contains all of the bootstrap replicate stapled together one on top of another.\ncalculate()\nThe third link in an infer pipeline is the calculate function, which calculates a single summary statistic for each replicate data frame. The main argument is stat, which can take values \"mean\", \"median\", \"proportion\", \"diff in means\", \"diff in props\" and a few more.\n\npenguins |&gt;\n  specify(response = bill_length_mm) |&gt;\n  generate(reps = 2, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\")\n\nResponse: bill_length_mm (numeric)\n# A tibble: 2 × 2\n  replicate  stat\n      &lt;int&gt; &lt;dbl&gt;\n1         1  43.5\n2         2  44.2\n\n\nObserve:\n\nThe name of the summary statistic should be put in quotation marks.\nThe resulting data frame had reps rows, one statistic from every replicate.\n\nThe calculate function is a shortcut for an operation you’re familiar with:\ndf %&gt;%\n  group_by(replicate) %&gt;%\n  summarize(mean(bill_length_mm))\n\nfit()\nIf you would like to create bootstrapped coefficients for a linear model, you’ll have to do something a bit different since there is a more than 1 summary statistic involved for each replicate data set. This is the role of fit(). There are no arguments to fill-in; it inherits the formula for the linear model from specify().\n\npenguins_adelie &lt;- penguins |&gt;\n  filter(species == \"Adelie\")\n\npenguins_adelie |&gt;\n  specify(body_mass_g ~ sex + flipper_length_mm) |&gt;\n  generate(reps = 2, type = \"bootstrap\") |&gt;\n  fit()\n\n# A tibble: 6 × 3\n# Groups:   replicate [2]\n  replicate term              estimate\n      &lt;int&gt; &lt;chr&gt;                &lt;dbl&gt;\n1         1 intercept           803.  \n2         1 sexmale             589.  \n3         1 flipper_length_mm    13.8 \n4         2 intercept          1731.  \n5         2 sexmale             729.  \n6         2 flipper_length_mm     8.72\n\n\nObserve:\n\nThe data frame has a number of rows equal to reps times the number of coefficients in the linear model (in this case \\(2 \\times 3\\)).\nTo get the collection of all coefficients for flipper_length_mm, for example, follow your infer pipeline with filter(term == \"flipper_length_mm\").\ndrop_na()\nThis function drops rows that have missing values (NAs). Add as arguments any variables you would like it to look to for missing values. If no arguments are given it will drop a row if there is a missing value in any column (Be ware of this behavior. It might lead you to drop more rows that you mean to).\n\ndf &lt;- data.frame(rank = c(2, 3, 1, 4, NA),\n                 letter = c(NA, NA, NA, \"d\", \"e\"))\n\ndf |&gt;\n  drop_na(rank)\n\n  rank letter\n1    2   &lt;NA&gt;\n2    3   &lt;NA&gt;\n3    1   &lt;NA&gt;\n4    4      d\n\ndf %&gt;%\n  drop_na()\n\n  rank letter\n1    4      d"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/notes.html#footnotes",
    "href": "4-generalization/03-bootstrapping/notes.html#footnotes",
    "title": "Bootstrapping",
    "section": "Footnotes",
    "text": "Footnotes\n\nIf you’re curious about the mathematical underpinnings of the Central Limit Theorem, read the corresponding Wikipedia page and enroll in an upper division course in probability and mathematical statistics.↩︎"
  },
  {
    "objectID": "4-generalization/03-bootstrapping/ps.html",
    "href": "4-generalization/03-bootstrapping/ps.html",
    "title": "Bootstrapping",
    "section": "",
    "text": "Bootstrapping and the Central Limit Theorem are two ways to approximate the sampling distribution of statistics. But how good of an approximation are they?\nGo to abray.shinyapps.io/sampling-dist\n\nThe Triptych\n\nUsing the app, create a large slightly skewed population, draw a sample of size 100, then simulate the sampling distribution of the mean. Sketch the three distributions as density plots below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOf these three plots, which one(s) are observed in real-life settings?\n\n\n\n\nWhat mathematical result describes the particular shape seen in the sampling distribution?\n\n\n\n\nExperiment with the shape of the population, the sample size, and the statistic being studied. Report 3 different settings that lead to sampling distributions that look non-normal and sketch those non-normal shapes below.\n\n\n\n\nApproximations\nUsing the Triptych settings from one of your non-normal sampling distributions, click into the “Approximations” tab at the top.\n\nImport your empirical distribution to serve as the bootstrap population and then draw several bootstrap samples. How many observations are being visualized in each bootstrap sample? What is the highest and lowest value that can be seen in a bootstrap sample?\n\n\n\n\n\nGenerate a full bootstrap sampling distribution using the statistic specified in the Triptych. Of these three plots, which one(s) can be observed in a real-life setting?\n\n\n\n\nOverlay on top of the bootstrap distribution the shape of the true simulated sampling distribution. Draw that figure below, labeling both distributions, and comment on how good of an approximation the bootstrap sampling distribution is to the true sampling distribution.\n\n\n\n\n\n\n\n\n\n\nUse the app to develop general rules of thumb for each statistic for when the bootstrap algorithm can provide a good approximation to the shape of the sampling distribution. For each statistic, discuss the properties of the population and the sample."
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html",
    "title": "Hypothesis Tests II",
    "section": "",
    "text": "Announcements\nConcept Questions\nBreak\nPS 14: Hypothesis Tests II"
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#agenda",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#agenda",
    "title": "Hypothesis Tests II",
    "section": "",
    "text": "Announcements\nConcept Questions\nBreak\nPS 14: Hypothesis Tests II"
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#announcements",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#announcements",
    "title": "Hypothesis Tests II",
    "section": "Announcements",
    "text": "Announcements\n\nRQ: Wrong By Design due Wed/Thu night at 11:59pm\n\n. . .\n\nQuiz 3 is in the first half of class on Thursday/Friday.\n\n. . .\n\n\nProblem Set 14 is due the Tuesday after break."
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#section",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#section",
    "title": "Hypothesis Tests II",
    "section": "",
    "text": "Which pair of plots would have the greatest chi-squared distance between them? (consider one of them the “observed” and the other the “expected”)\n\n\n\n\n\n\n\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nThis question recalls the introduction of a new summary statistic in the notes: the chi-squared. It is used to measure the distance between two distributions of categorical variables. The pair with the highest statistic is D and F. The important mathematical intuition to have here is that the statistic adds together every squared normalized difference, so one very large difference can inflate the statistic."
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#chi-squareds-compared",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#chi-squareds-compared",
    "title": "Hypothesis Tests II",
    "section": "Chi-squareds Compared",
    "text": "Chi-squareds Compared\n\n\n\n\n\n\n\n\n\n\n\\[\n\\frac{(1-1)^2}{1} + \\frac{(10 - 1)^2}{1} + \\frac{(1 - 10)^2}{10} \\\\\n0 + 81 + \\frac{81}{10}  = 89.1\n\\]\n\n\n\n\n\n\n\n\n\n\n\\[\n\\frac{(3-5)^2}{5} + \\frac{(4-4)^2}{4} + \\frac{(5-3)^2}{3} \\\\\n\\frac{4}{5} + 0 + \\frac{4}{3} = 2.13\n\\]"
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#an-in-class-experiment",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#an-in-class-experiment",
    "title": "Hypothesis Tests II",
    "section": "An In-class Experiment",
    "text": "An In-class Experiment\nIn order to demonstrate how to conduct a hypothesis test through simulation, we will be collecting data from this class using a poll.\n\nYou will have only 15 seconds to answer the following multiple choice question, so please get ready at pollev.com…\n\nThis sets up a discussion of how to set up a hypothesis test that p = .5.\nThis is based on a series of experiments that show a link between sounds and shapes in people’s minds - sharper shapes correspond to sharper sounds (k, t) and rounder shapes to rounder sounds (m, b). Read more about it here: https://en.wikipedia.org/wiki/Bouba/kiki_effect.\nYou conduct the experiment on the students using a poll question, then you calculate a proportion, then entertain the notion that linking names to shapes is meaningless, so people were choosing randomly, then use that to motivate the hypothesis test that p = .5."
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#section-1",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#section-1",
    "title": "Hypothesis Tests II",
    "section": "",
    "text": "The two shapes above have simple first names:\n\nBooba\nKiki\n\nWhich of the two names belongs to the shape on the left?\n\n\n\n\n\n−&plus;\n\n00:15\n\n\n\n\nThis poll everywhere question is set up to record the counts of each category and display the total number of responses in the upper right. This allows you to discuss the results either in terms of a binomial or in terms of a proportion."
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#steps-of-a-hypothesis-test",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#steps-of-a-hypothesis-test",
    "title": "Hypothesis Tests II",
    "section": "Steps of a Hypothesis Test",
    "text": "Steps of a Hypothesis Test\n\nAssert a model for how the data was generated (the null hypothesis)\nSelect a test statistic that bears on that null hypothesis (a mean, a proportion, a difference in means, a difference in proportions, etc).\nApproximate the sampling distribution of that statistic under the null hypothesis (aka the null distribution)\nAssess the degree of consistency between that distribution and the test statistic that was actually observed (either visually or by calculating a p-value)"
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#the-null-hypothesis",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#the-null-hypothesis",
    "title": "Hypothesis Tests II",
    "section": "1. The Null Hypothesis",
    "text": "1. The Null Hypothesis\n\nLet \\(p_k\\) be the probability that a person selects Kiki for the shape on the left.\nLet \\(\\hat{p}_k\\) be the sample proportion of people that selected Kiki for the shape on the left.\n\n\nWhat is a statement of the null hypothesis that corresponds to the notion the link between names and shapes is arbitrary?\n\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#select-a-test-statistic",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#select-a-test-statistic",
    "title": "Hypothesis Tests II",
    "section": "2. Select a test statistic",
    "text": "2. Select a test statistic\n\\[\\hat{p}_k = \\frac{\\textrm{Number who chose \"Kiki\"}}{\\textrm{Total number of people}}\\]\n. . .\n\nNote: you could also simply \\(n_k\\), the number of people who chose “Kiki”.\n\nA follow-up question to ask: “For our class, what would we expect this number to be if \\(p_k = .5\\)?”\nIt should be half the number of students who responded to the poll."
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#approximate-the-null-distribution",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#approximate-the-null-distribution",
    "title": "Hypothesis Tests II",
    "section": "3. Approximate the null distribution",
    "text": "3. Approximate the null distribution\nOur technique: simulate data from a world in which the null is true, then calculate the test statistic on the simulated data.\n\nWhich simulation method(s) align with the null hypothesis and our data collection process?\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nA, C, and E all work."
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#simulating-the-null-using-infer",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#simulating-the-null-using-infer",
    "title": "Hypothesis Tests II",
    "section": "Simulating the null using infer\n",
    "text": "Simulating the null using infer\n\n\nCodelibrary(tidyverse)\nlibrary(infer)\n\n# update these based on the poll\nn_k &lt;- 40\nn_b &lt;- 20\n\nshapes &lt;- data.frame(name = c(rep(\"Kiki\", n_k),\n                              rep(\"Booba\", n_b)))\n\nshapes |&gt;\n  specify(response = name,\n          success = \"Kiki\") |&gt;\n  hypothesize(null = \"point\", p = .5) |&gt;\n  generate(reps = 1, type = \"draw\") |&gt;\n  calculate(stat = \"prop\")\n\n\n\n\nhave students open laptops and an R script to code along.\ncopy and paste the code cell from the slides into RStudio\nreplace n_k and n_b with the values from your poll.\n“break the pipe” to show what happens at every stage.\nrun several pipelines with reps = 1 to show it changing.\nset reps to 500 to show the full collection of p-hats."
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#assess-the-consistency-of-the-data-and-the-null",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#assess-the-consistency-of-the-data-and-the-null",
    "title": "Hypothesis Tests II",
    "section": "4. Assess the consistency of the data and the null",
    "text": "4. Assess the consistency of the data and the null\n\nCodenull &lt;- shapes |&gt;\n  specify(response = name,\n          success = \"Kiki\") |&gt;\n  hypothesize(null = \"point\", p = .5) |&gt;\n  generate(reps = 500, type = \"draw\") |&gt;\n  calculate(stat = \"prop\")\n\nobs_p_hat &lt;- shapes |&gt;\n  specify(response = name,\n          success = \"Kiki\") |&gt;\n  # hypothesize(null = \"point\", p = .5) |&gt;\n  # generate(reps = 500, type = \"simulate\") |&gt;\n  calculate(stat = \"prop\")"
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#assess-the-consistency-of-the-data-and-the-null-1",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#assess-the-consistency-of-the-data-and-the-null-1",
    "title": "Hypothesis Tests II",
    "section": "4. Assess the consistency of the data and the null",
    "text": "4. Assess the consistency of the data and the null\n\nCodenull |&gt;\n  visualise() +\n  shade_pvalue(obs_p_hat, direction = \"both\")\n\nnull |&gt;\n  get_p_value(obs_p_hat, direction = \"both\")"
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#the-p-value",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#the-p-value",
    "title": "Hypothesis Tests II",
    "section": "The p-value",
    "text": "The p-value\n\nWhat is the proper interpretation of this p-value?\n\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "4-generalization/05-hypothesis-tests-2/slides.html#the-bouba-kiki-effect",
    "href": "4-generalization/05-hypothesis-tests-2/slides.html#the-bouba-kiki-effect",
    "title": "Hypothesis Tests II",
    "section": "The Bouba / Kiki Effect",
    "text": "The Bouba / Kiki Effect"
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/notes.html",
    "href": "4-generalization/06-wrong-by-design/notes.html",
    "title": "Wrong by Design",
    "section": "",
    "text": "Hypothesis tests are not flawless1. There are many ways in which they can be misused: the hypotheses can be poorly formulated, the p-value miscalculated or, more often, misinterpreted. But even a hypothesis test conducted by an expert practitioner is subject to arriving at an erroneous conclusion.\nIf the setting of the problem requires that a binary decision be made regarding the null hypothesis - that it be either rejected or retained - then it’s possible to come to the wrong conclusion. Just as in the court system, where innocent people are sometimes wrongly convicted and the guilty sometimes walk free, so too can the conclusion of a hypothesis test be in error.\nWhat distinguishes statistical hypothesis tests from a court system, however, is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion."
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/notes.html#statistical-errors",
    "href": "4-generalization/06-wrong-by-design/notes.html#statistical-errors",
    "title": "Wrong by Design",
    "section": "Statistical Errors",
    "text": "Statistical Errors\nIn a hypothesis test, there are two competing hypotheses: the null and the alternative, often abbreviated as \\(H_0\\) and \\(H_A\\). When the p-value is sufficiently low, \\(H_0\\) is rejected as a viable explanation for the data. When the p-value is high, we fail to reject \\(H_0\\).\nA statistical error is made whenever the conclusion of the test is contrary to the underlying truth regarding the null hypothesis.\n\nType I Error\n\nRejecting the null hypothesis when it is actually true. Also called a false positive.\n\nType II Error\n\nFailing to reject the null hypothesis when the alternative hypothesis is actually true. Also called a false negative.\n\n\nThe test comes to the correct conclusion in settings where it fails to reject a null hypothesis that is actually true and when it rejects the null hypothesis when the alternative hypothesis is true. These four scenarios can be laid out as follows.\n\n.library(tidyverse)\nlibrary(kableExtra)\nht_scenarios &lt;- tribble(\n  ~Truth,       ~`Reject H0`, ~`Fail to reject H0`,\n  \"H0 is true\", \"Type I Error\",           \"Good decision\", \n  \"HA is true\", \"Good decision\",          \"Type II Error\"\n)\n\nht_scenarios %&gt;%\n  kbl(linesep = \"\", booktabs = TRUE) %&gt;%\n  add_header_above(c(\"\", \"Test conclusion\" = 2)) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"condensed\"), \n                latex_options = c(\"hold_position\"), full_width = FALSE) %&gt;%\n  column_spec(1, width = \"5em\", bold = T, border_right = T) %&gt;%\n  column_spec(2, color = \"white\", \n              background = c(rgb(150/255, 0, 0, .7),\n                             rgb(0, 150/255, 0, .7)), \n              width = \"5em\") %&gt;%\n  column_spec(3, color = \"white\", \n              background = c(rgb(0, 150/255, 0, .7),\n                             rgb(150/255, 0, 0, .7)),\n              width = \"5em\")\n\n\n\n\n\n\n\n\n\n\n\nTest conclusion\n\n\n\nTruth\nReject H0\nFail to reject H0\n\n\n\n\nH0 is true\nType I Error\nGood decision\n\n\nHA is true\nGood decision\nType II Error\n\n\n\n\n\nTo build your understanding of these different types of errors, work through a few exercises.\nExercise 1\nIn a US court, the defendant is either innocent \\((H_0)\\) or guilty \\((H_A).\\) What does a type I Error represent in this context? What does a type II Error represent? The table above may be useful.\n\nCheck your answer\n\nIf the court makes a type I Error, this means the defendant is innocent \\((H_0\\) true) but wrongly convicted. A type II Error means the court failed to reject \\(H_0\\) (i.e., failed to convict the person) when they were in fact guilty \\((H_A\\) true).\nExercise 2\nConsider the case of Kristen Gilbert, the nurse on trial for causing Code Blue emergencies at her hospital. The court eventually found her guilty of the charges and sentenced her to life in prison. If in fact she was innocent, what type of error did the court commit?\n\nCheck your answer\nThis would be a Type I error: rejecting the null hypothesis that she is innocent when it was in fact true.\nExercise 3\nHow could we reduce the probability of making a type I error in US courts? What influence would this have on the probability of making a type II error?\n\nCheck your answer\nTo lower the type I Error rate, we might raise our standard for conviction from “beyond a reasonable doubt” to “beyond a conceivable doubt” so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more type II Errors.\nExercise 4\nHow could we reduce the probability of making a type II error rate in US courts? What influence would this have on the probability of making a type I error?\n\nCheck your answer\nTo lower the type II Error rate, we want to convict more guilty people. We could lower the standards for conviction from “beyond a reasonable doubt” to “beyond a little doubt”. Lowering the bar for guilt will also result in more wrongful convictions, raising the type I Error rate.\n\nThe example and guided practice above provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type. This threshold for how much evidence is require is called the significance level.\n\nSignificance level, \\(\\alpha\\)\n\nA number between 0 and 1 that serves as the threshold for the p-value. The null hypothesis is rejected when the p-value \\(&lt; \\alpha\\), and the finding is found “statistically significant”.\n\n\nBy convention, \\(\\alpha = 0.05\\), however you should adjust the significance level based on the application. Certain scientific fields might tend to use a slightly higher or lower threshold for what constitutes statistical significance. In a setting where the decisions have very different real-world consequences, those, too, can factor into the choice of \\(\\alpha\\).\nIf making a type I error is dangerous or especially costly, you should choose a small significance level (e.g., 0.01 or 0.001). If you want to be very cautious about rejecting the null hypothesis, you should demand very strong evidence favoring the alternative \\(H_A\\) before we would reject \\(H_0.\\)\nIf a type II error is relatively more dangerous or much more costly than a type I error, then we should choose a higher significance level (e.g., 0.10). Here we want to be cautious about failing to reject \\(H_0\\) when the null is actually false.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Blood Thinners and Survival\nCardiopulmonary resuscitation (CPR) is a procedure used on individuals suffering a heart attack when other emergency resources are unavailable. This procedure is helpful in providing some blood circulation to keep a person alive, but CPR chest compression can also cause internal injuries. Internal bleeding and other injuries that can result from CPR complicate additional treatment efforts. For instance, blood thinners may be used to help release a clot that is causing the heart attack once a patient arrives in the hospital. However, blood thinners negatively affect internal injuries.\nHere we consider an experiment with patients who underwent CPR for a heart attack and were subsequently admitted to a hospital. Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours.\nExercise 5\nForm hypotheses for this study in plain and statistical language. Let \\(p_C\\) represent the true survival rate of people who do not receive a blood thinner (corresponding to the control group) and \\(p_T\\) represent the survival rate for people receiving a blood thinner (corresponding to the treatment group).\n\nCheck your answer\nWe want to understand whether blood thinners are helpful or harmful. We’ll consider both of these possibilities using a two-sided hypothesis test.\n\n\\(H_0:\\) Blood thinners do not have an overall survival effect, i.e., the survival proportions are the same in each group. \\(p_T - p_C = 0.\\)\n\\(H_A:\\) Blood thinners have an impact on survival, either positive or negative, but not zero. \\(p_T - p_C \\neq 0.\\)\n\nNote that if we had done a one-sided hypothesis test, the resulting hypotheses would have been:\n\n\\(H_0:\\) Blood thinners do not have a positive overall survival effect, i.e., the survival proportions for the blood thinner group is the same or lower than the control group. \\(p_T - p_C &lt; 0.\\)\n\\(H_A:\\) Blood thinners have a positive impact on survival. \\(p_T - p_C &gt; 0.\\)\n\nThere were 50 patients in the experiment who did not receive a blood thinner and 40 patients who did. The study results are shown in the table below.\n\n.library(openintro)\nlibrary(janitor)\ncpr %&gt;%\n  mutate(\n    outcome = str_to_title(outcome),\n    group   = str_to_title(group)\n    ) %&gt;%\n  count(group, outcome) %&gt;%\n  pivot_wider(names_from = outcome, values_from = n) %&gt;%\n  janitor::adorn_totals(where = c(\"row\", \"col\")) %&gt;%\n  kbl(linesep = \"\", booktabs = TRUE, \n    caption = \"Results for the CPR study. Patients in the treatment group were given a blood thinner, and patients in the control group were not.\",\n    col.names = c(\"Group\", \"Died\", \"Survived\", \"Total\"),\n    align = \"lccc\"\n    ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"condensed\"),\n                latex_options = c(\"striped\", \"hold_position\"), \n                full_width = FALSE) %&gt;%\n  column_spec(1:4, width = \"7em\")\n\n\nResults for the CPR study. Patients in the treatment group were given a blood thinner, and patients in the control group were not.\n\nGroup\nDied\nSurvived\nTotal\n\n\n\nControl\n39\n11\n50\n\n\nTreatment\n26\n14\n40\n\n\nTotal\n65\n25\n90\n\n\n\n\n\nExercise 6\nWhat is the observed survival rate in the control group? And in the treatment group? Also, provide a point estimate \\((\\hat{p}_T - \\hat{p}_C)\\) for the true difference in population survival proportions across the two groups: \\(p_T - p_C.\\)\n\nCheck your answer\nObserved control survival rate: \\(\\hat{p}_C = \\frac{11}{50} = 0.22.\\) Treatment survival rate: \\(\\hat{p}_T = \\frac{14}{40} = 0.35.\\) Observed difference: \\(\\hat{p}_T - \\hat{p}_C = 0.35 - 0.22 = 0.13.\\)\n\nAccording to the point estimate, for patients who have undergone CPR outside of the hospital, an additional 13% of these patients survive when they are treated with blood thinners. However, we wonder if this difference could be easily explainable by chance, if the treatment has no effect on survival.\nAs we did in the past study, we will simulate what type of differences we might see from chance alone under the null hypothesis. By randomly assigning each of the patient’s files to a “simulated treatment” or “simulated control” allocation, we get a new grouping. If we repeat this simulation 1,000 times, we can build a null distribution of the differences shown in the figure below.\n\n.library(infer)\nset.seed(47)\n\nobs_stat &lt;- cpr %&gt;%\n  specify(response = outcome, explanatory = group, success = \"survived\") %&gt;%\n  calculate(stat = \"diff in props\", order = c(\"treatment\", \"control\")) %&gt;%\n  # simplify by rounding\n  mutate(stat = round(stat, 3))\n\nnull &lt;- cpr %&gt;%\n  specify(response = outcome, explanatory = group, success = \"survived\") %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  calculate(stat = \"diff in props\", order = c(\"treatment\", \"control\")) %&gt;%\n  # simplify by rounding\n  mutate(stat = round(stat, 3))\n\ncpr_tail &lt;- null %&gt;%\n  get_p_value(obs_stat, direction = \"right\") %&gt;%\n  pull()\n\nnull %&gt;%\n  visualize() +\n  shade_p_value(obs_stat, direction = \"right\")\n\n\n\nNull distribution of the point estimate for the difference in proportions, \\(\\hat{p}_T - \\hat{p}_C.\\) The shaded right tail shows observations that are at least as large as the observed difference, 0.13.\n\n\n\nThe right tail area is 0.135. (Note: it is only a coincidence that we also have \\(\\hat{p}_T - \\hat{p}_C=0.13.)\\) However, contrary to how we calculated the p-value in previous studies, the p-value of this test is not actually the tail area we calculated, i.e., it’s not 0.135!\nThe p-value is defined as the chance of a test statistic as extreme or even more extreme than the one observed under the assumptions of the null hypothesis. Importantly, “more extreme” is defined based on the alternative hypothesis. If the alternative hypothesis suggests a two-sided test, then you must be open to deviations in either direction.\nIn this case, any differences less than or equal to -0.13 would also provide equally strong evidence favoring the alternative hypothesis as a difference of +0.13 did. A difference of -0.13 would correspond to 13% higher survival rate in the control group than the treatment group. In the figure below we have also shaded these differences in the left tail of the distribution. These two shaded tails provide a visual representation of the p-value for a two-sided test.\n\n.null %&gt;%\n  visualize() +\n  shade_p_value(obs_stat, direction = \"both\")\n\n\n\nNull distribution of the point estimate for the difference in proportions, \\(\\hat{p}_T - \\hat{p}_C.\\) All values that are at least as extreme as +0.13 but in either direction away from 0 are shaded.\n\n\n\nFor a two-sided test, take the single tail (in this case, 0.131) and double it to get the p-value: 0.262. Since this p-value is larger than 0.05, we do not reject the null hypothesis. That is, we do not find convincing evidence that the blood thinner has any influence on survival of patients who undergo CPR prior to arriving at the hospital.\nGenerally, to find a two-sided p-value we double the single tail area, which remains a reasonable approach even when the distribution is asymmetric. However, the approach can result in p-values larger than 1 when the point estimate is very near the mean in the null distribution; in such cases, we write that the p-value is 1. Also, very large p-values computed in this way (e.g., 0.85), may also be slightly inflated. Typically, we do not worry too much about the precision of very large p-values because they lead to the same analysis conclusion, even if the value is slightly off.\n\n\n\n\n\n\nTip\n\n\n\nDefault to a two-sided test.\nWe want to be rigorous and keep an open mind when we analyze data and evidence. Use a one-sided hypothesis test only if you truly have interest in only one direction.\n\n\n\n\n\n\n\n\nTip\n\n\n\nComputing a p-value for a two-sided test.\nFirst compute the p-value for one tail of the distribution, then double that value to get the two-sided p-value. That’s it!"
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/notes.html#controlling-the-type-i-error-rate",
    "href": "4-generalization/06-wrong-by-design/notes.html#controlling-the-type-i-error-rate",
    "title": "Wrong by Design",
    "section": "Controlling the Type I Error rate",
    "text": "Controlling the Type I Error rate\nNow that we understand the difference between one-sided and two-sided tests, we must recognize when to use each type of test. Because of the result of increased error rates, it is never okay to change two-sided tests to one-sided tests after observing the data. Let’s explore the consequences of ignoring this advice.\nSuppose we are interested in finding any difference from 0. We’ve created a smooth-looking null distribution representing differences due to chance in the figure below.\nSuppose the sample difference was larger than 0. Then if we can flip to a one-sided test, we would use \\(H_A:\\) difference \\(&gt; 0.\\) Now if we obtain any observation in the upper 5% of the distribution, we would reject \\(H_0\\) since the p-value would just be a the single tail. Thus, if the null hypothesis is true, we incorrectly reject the null hypothesis about 5% of the time when the sample mean is above the null value, as shown in the figure.\nSuppose the sample difference was smaller than 0. Then if we change to a one-sided test, we would use \\(H_A:\\) difference \\(&lt; 0.\\) If the observed difference falls in the lower 5% of the figure, we would reject \\(H_0.\\) That is, if the null hypothesis is true, then we would observe this situation about 5% of the time.\nBy examining these two scenarios, we can determine that we will make a type I error \\(5\\%+5\\%=10\\%\\) of the time if we are allowed to swap to the “best” one-sided test for the data. This is twice the error rate we prescribed with our significance level: \\(\\alpha=0.05\\) (!).\n\n.normTail(U = 1.65, L = -1.65, col = IMSCOL[\"blue\", \"full\"])\narrows(-2, 0.17, -1.9, 0.08, length = 0.07, lwd = 1.5)\ntext(-1.92, 0.21, \"5%\", cex = 1.2)\narrows(2, 0.17, 1.9, 0.08, length = 0.07, lwd = 1.5)\ntext(2.08, 0.21, \"5%\", cex = 1.2)\n\n\n\nThe shaded regions represent areas where we would reject \\(H_0\\) under the bad practices considered in when \\(\\alpha = 0.05.\\)\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nHypothesis tests should be set up before seeing the data.\nAfter observing data, it is tempting to turn a two-sided test into a one-sided test. Avoid this temptation. Hypotheses should be set up before observing the data."
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/notes.html#power",
    "href": "4-generalization/06-wrong-by-design/notes.html#power",
    "title": "Wrong by Design",
    "section": "Power",
    "text": "Power\nOften times in planning a study there are two competing considerations:\n\nWe want to collect enough data that we can detect important effects but…\nCollecting data can be expensive in terms of money, time, and suffering, so we want to minimize the amount of data we collect.\n\nAs an example, imagine you are working to develop a new drug to reduce to size of tumors and you would like to test the effectiveness of the drug on mice. The more data that you collect, the greater your ability to detect even slight reductions in tumor size due to your drug. But more is data is not always better. Here, collecting data means paying researchers and sacrificing mice, so there is a cost, both financial and ethical, to collecting more data.\nOne way to balance these two competing needs is to frame the problem as follows: what is the smallest sample size that I would need to have a high probability of detecting an effect of the drug, if it is in fact effective? This probability is a vital consideration when planning a study. It is called the statistical power of the test.\n\nPower\n\nThe probability of rejecting the null hypothesis when the null hypothesis is false This probability depends on how big the effect is (e.g., how good the medical treatment is) as well as the sample size.\n\n\nStatistical power is a good thing: the more power that you have, the lower the chance that you’ll make a decision error. But what kind exactly?\nExercise 7\nReview the definitions of Type I and Type II error at the beginning of these notes. How does the concept of power relate mathematically to the probability of committing these two types of errors?\n\nCheck your answer\nThe power is directly related to the concept of a Type II error, failing to reject a null hypothesis that is false. Since the power is the probability of correctly rejecting a null hypothesis that is false, it can be calculated as one minus the probability of a Type II error. The probability of committing a Type II error is often assigned the Greek letter beta, \\(\\beta\\), therefore you will sometimes see power written as \\(1 - \\beta\\).\n\nIn the example of the mouse study, if we collected very little data and the effect of our drug is very slight, then it’s conceivable that our power could be very very low, in the neighborhood of 10%. This means that there was only a 10% chance of that we’d be able to detect an effect of our drug. In this case, it could be argued that the design of our study was unethical because the mice that we studied were sacrificed in vain.\nOn the other hand, if we used 100,000 mice in our study, then the power would be very very high, say 99.99%. That is good, because we’re quite certain that we’d be able to detect an effect if it exists. But it would also be considered unethical because we sacrificed many mice unnecessarily. It is possible that we could have had almost as high a power, say 90%, with only 400 mice.\nCalibrating the appropriate sample size that achieves a high enough statistical power - 80% or 90% - without incurring unnecessary costs is challenging work that is beyond the scope of this class. But the concept of power is essential to good science, so it’s important to be aware of. Whenever you come across a study that has a high p-value (that is, they failed to reject the null hypothesis), ask yourself: is it possible that this is just a low-powered study?"
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/notes.html#summary",
    "href": "4-generalization/06-wrong-by-design/notes.html#summary",
    "title": "Wrong by Design",
    "section": "Summary",
    "text": "Summary\nAlthough hypothesis testing provides a framework for making decisions based on data, as the analyst, you need to understand how and when the process can go wrong. That is, always keep in mind that the conclusion to a hypothesis test may not be right! Sometimes when the null hypothesis is true, we will accidentally reject it and commit a type I error; sometimes when the alternative hypothesis is true, we will fail to reject the null hypothesis and commit a type II error. The power of the test quantifies how likely it is to obtain data which will reject the null hypothesis when indeed the alternative is true; the power of the test is increased when larger sample sizes are taken."
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/notes.html#footnotes",
    "href": "4-generalization/06-wrong-by-design/notes.html#footnotes",
    "title": "Wrong by Design",
    "section": "Footnotes",
    "text": "Footnotes\n\nThese lecture notes adapted from Introduction to Modern Statistics, First Edition by Mine Çetinkaya-Rundel and Johanna Hardin, a textbook from the OpenIntro Project.↩︎"
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/ps.html",
    "href": "4-generalization/06-wrong-by-design/ps.html",
    "title": "Wrong by Design",
    "section": "",
    "text": "For each of the following sets of hypotheses, determine whether committing a Type I error or a Type II error has greater real-world consequences. Explain your decision.\n\n\nImagine that the San Francisco Giants hold a book donation promotion for one of their games where every individual who brings a used book to the stadium is allowed free entry.\n\\(H_0\\): The number of individuals who wish attend the game will be no more than the capacity of the stadium.\n\\(H_A\\): The number of individuals who wish to attend the game will exceed the capacity of the stadium.\n\n\n\n\n\n\n\n\n\n\\(H_0\\): A new operating system is no more effective for Carbon Health to use on the computers at its clinics than the old operating system.\n\\(H_A\\): A new operating system is more effective for Carbon Health to use on the computers at its clinics than the old operating system.\n\n\n\n\n\n\n\n\n\n\\(H_0\\): The rate of COVID-19 infection on campus has not changed since last month.\n\\(H_A\\): The rate of COVID-19 infection on campus has increased since last month."
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/ps.html#question-1",
    "href": "4-generalization/06-wrong-by-design/ps.html#question-1",
    "title": "Wrong by Design",
    "section": "",
    "text": "For each of the following sets of hypotheses, determine whether committing a Type I error or a Type II error has greater real-world consequences. Explain your decision.\n\n\nImagine that the San Francisco Giants hold a book donation promotion for one of their games where every individual who brings a used book to the stadium is allowed free entry.\n\\(H_0\\): The number of individuals who wish attend the game will be no more than the capacity of the stadium.\n\\(H_A\\): The number of individuals who wish to attend the game will exceed the capacity of the stadium.\n\n\n\n\n\n\n\n\n\n\\(H_0\\): A new operating system is no more effective for Carbon Health to use on the computers at its clinics than the old operating system.\n\\(H_A\\): A new operating system is more effective for Carbon Health to use on the computers at its clinics than the old operating system.\n\n\n\n\n\n\n\n\n\n\\(H_0\\): The rate of COVID-19 infection on campus has not changed since last month.\n\\(H_A\\): The rate of COVID-19 infection on campus has increased since last month."
  },
  {
    "objectID": "4-generalization/06-wrong-by-design/ps.html#question-2",
    "href": "4-generalization/06-wrong-by-design/ps.html#question-2",
    "title": "Wrong by Design",
    "section": "Question 2",
    "text": "Question 2\nA food safety inspector is called upon to investigate a restaurant with a few customer reports of poor sanitation practices. The food safety inspector uses a hypothesis testing framework to evaluate whether regulations are not being met. If he decides the restaurant is in gross violation, its license to serve food will be revoked.\n\nPart A\nWrite the hypotheses in words.\n\n\n\n\n\nPart B\nWhat is a Type 1 Error in this context?\n\n\n\n\n\nPart C\nWhat is a Type 2 Error in this context?\n\n\n\n\n\nPart D\nWhich error is more problematic for the restaurant owner? Why?\n\n\n\n\n\nPart E\nWhich error is more problematic for the diners? Why?\n\n\n\n\n\nPart F\nAs a diner, would you prefer that the food safety inspector requires strong evidence or very strong evidence of health concerns before revoking a restaurant’s license? Explain your reasoning."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Navigate the notes using the menu in the left sidebar or the search in the upper right corner of the page."
  },
  {
    "objectID": "staff-guide/index.html",
    "href": "staff-guide/index.html",
    "title": "Staff Guide",
    "section": "",
    "text": "This guide serves as a resource for the Stat 20 course staff."
  },
  {
    "objectID": "staff-guide/roles-and-expectations.html",
    "href": "staff-guide/roles-and-expectations.html",
    "title": "Roles and Expectations",
    "section": "",
    "text": "Tutors are the secondary point of contact in the course.\n\n\nTutors’ roles include:\n\nHelping with section on MWF\nHolding office hours\nAnswering questions on the class forum.\nAssisting with the proctoring of exams.\n\n\n\n\nTutors’ responsibilities include:\n\nAttending section\nReviewing the day’s materials before attending section\nWithin section, fielding and answering student questions during unstructured work time\nBeing present on the class forum during their assigned time.\nAttending a weekly/biweekly meeting with the instructors"
  },
  {
    "objectID": "staff-guide/roles-and-expectations.html#tutors",
    "href": "staff-guide/roles-and-expectations.html#tutors",
    "title": "Roles and Expectations",
    "section": "",
    "text": "Tutors are the secondary point of contact in the course.\n\n\nTutors’ roles include:\n\nHelping with section on MWF\nHolding office hours\nAnswering questions on the class forum.\nAssisting with the proctoring of exams.\n\n\n\n\nTutors’ responsibilities include:\n\nAttending section\nReviewing the day’s materials before attending section\nWithin section, fielding and answering student questions during unstructured work time\nBeing present on the class forum during their assigned time.\nAttending a weekly/biweekly meeting with the instructors"
  },
  {
    "objectID": "staff-guide/roles-and-expectations.html#readers",
    "href": "staff-guide/roles-and-expectations.html#readers",
    "title": "Roles and Expectations",
    "section": "Readers",
    "text": "Readers\n\nRoles\nReaders are the primary graders in the course.\n\n\nResponsibilities\nReaders are responsible for:\n\nGrading their assigned work in a timely manner\nResolving regrade requests in a timely manner\nAttending a weekly/biweekly meeting held by the instructors which is meant to track their progress."
  },
  {
    "objectID": "staff-guide/roles-and-expectations.html#instructorsgsi-instructors",
    "href": "staff-guide/roles-and-expectations.html#instructorsgsi-instructors",
    "title": "Roles and Expectations",
    "section": "Instructors/GSI-Instructors",
    "text": "Instructors/GSI-Instructors\nInstructors are the primary point of contact in the course.\n\nRoles\nThe roles of instructors include:\n\nDesigning the course syllabus\nLeading one, two or three sections on MWF\nCreating and disseminating course materials. These includes, but are not limited to:\n\nLabs\nProblem Sets\nIn-class slides and handouts\nQuizzes and Exams\nReview material\n(to the tutors) solutions to these course materials\n\nHolding office hours\nMaintaining the course website\nAnswering questions on the class forum.\nAdministering exams\nPosting final grades.\n\n\n\nResponsibilities\nInstructor responsibilities include:\n\nAssigning tutors to sections\nPreparing a general lesson plan for each day of section and disseminating this plan to the tutors by a reasonable time before section\nDisseminating solutions to in-class handouts, labs and problem sets to tutors by the time of assignment release\nDuring class, walking around to groups and assessing their understanding of the material, as well as fielding and answering student questions\nCreating rubrics for labs\nCommunicating with readers with regards to grading questions\nSetting up meetings with tutors and readers regularly (weekly or biweekly)\nGranting assignment extensions\nResolving exam conflicts\nArranging student accommodations"
  },
  {
    "objectID": "5-causation/labs/06-taste-test/slides.html",
    "href": "5-causation/labs/06-taste-test/slides.html",
    "title": "Lab 7: A Matter of Taste",
    "section": "",
    "text": "Suggested timeline - [10 min] Slides and making teams - [45 min] Working on the protocol\nGeneral Advice - The students have seen nothing about experimental design yet in class. The idea is to have them use their own mental models to solve this problem, then later review and revise them. That’s to say: coach them only to be precise in their protocol, not to be using best practices in experimental design. Errors made here are great learning opportunities later. - The first phase of the lab can all be done with paper and pencil, so you can ask that students stow their laptops. - It might be helpful to guide students to start at the end, what they expect their data and plots to look like, then work back towards the hypotheses and claims. - As written, most null hypotheses will suggest the use of a difference in proportions as the test statistic."
  },
  {
    "objectID": "5-causation/labs/06-taste-test/slides.html#making-claims-with-data",
    "href": "5-causation/labs/06-taste-test/slides.html#making-claims-with-data",
    "title": "Lab 7: A Matter of Taste",
    "section": "Making Claims with Data",
    "text": "Making Claims with Data\n\n\nSo far: Pose question -&gt; observe data -&gt; analyze data -&gt; make claim.\n\n. . .\n\ne.g. Iranian election:\n\nDid fraud occur?\nObserve vote counts by city.\nTest of hypothesis that votes follow Benford’s law.\nVotes don’t follow Benford’s law well but not clear that this means fraud."
  },
  {
    "objectID": "5-causation/labs/06-taste-test/slides.html#making-claims-with-data-1",
    "href": "5-causation/labs/06-taste-test/slides.html#making-claims-with-data-1",
    "title": "Lab 7: A Matter of Taste",
    "section": "Making Claims with Data",
    "text": "Making Claims with Data\n\n\nThis week:\n\nIdentify possible claim\nImagine analysis that would show it.\nPlan data collection to allow that analysis.\nCarry out data collection + analysis.\nCheck claim.\n\n\n\n\nThis week students will work backwards: start with a claim that they think may be true, then decide how to collect data in a manner that will be most effective at determining whether or not the claim is true."
  },
  {
    "objectID": "5-causation/labs/06-taste-test/slides.html#but-first",
    "href": "5-causation/labs/06-taste-test/slides.html#but-first",
    "title": "Lab 7: A Matter of Taste",
    "section": "But first",
    "text": "But first\nPlease get into groups of 3 (or 4). Take turns introducing yourselves."
  },
  {
    "objectID": "5-causation/labs/06-taste-test/slides.html#a-matter-of-taste",
    "href": "5-causation/labs/06-taste-test/slides.html#a-matter-of-taste",
    "title": "Lab 7: A Matter of Taste",
    "section": "A Matter of Taste",
    "text": "A Matter of Taste\nYour challenge: Determine whether you can affect one your teammates’ perceptions of bubble water by manipulating their experience of tasting.\n\n\n\n\nEach team will have access to\n\n50 minutes (half of next class)\n2 cans of soda water, each one from a different flavor\nsmall paper cups\nsaltine crackers\nother materials welcome"
  },
  {
    "objectID": "5-causation/labs/06-taste-test/slides.html#group-members",
    "href": "5-causation/labs/06-taste-test/slides.html#group-members",
    "title": "Lab 7: A Matter of Taste",
    "section": "Group Members",
    "text": "Group Members\n\n\nThe main reason for the roles here is to dedicate just one person to handling the materials for health safety."
  },
  {
    "objectID": "5-causation/labs/06-taste-test/slides.html#question-and-hypotheses",
    "href": "5-causation/labs/06-taste-test/slides.html#question-and-hypotheses",
    "title": "Lab 7: A Matter of Taste",
    "section": "Question and Hypotheses",
    "text": "Question and Hypotheses\n\n\nPrompt them to think carefully about their wording here in the question. How ambitious and general do they want to be?\nIt is also very common for people to suggest non-causal questions. It will help to underline the idea of intervention on this slide."
  },
  {
    "objectID": "5-causation/labs/06-taste-test/slides.html#hypotheses",
    "href": "5-causation/labs/06-taste-test/slides.html#hypotheses",
    "title": "Lab 7: A Matter of Taste",
    "section": "Hypotheses",
    "text": "Hypotheses"
  },
  {
    "objectID": "5-causation/labs/06-taste-test/slides.html#protocol",
    "href": "5-causation/labs/06-taste-test/slides.html#protocol",
    "title": "Lab 7: A Matter of Taste",
    "section": "Protocol",
    "text": "Protocol\n\n\nHere they must be very precise. We’re spoken about reproducible science in class, so use that language to encourage careful work. They must be able to pass this protocol off to another group, and they should be able to carry out the intended experiment with no ambiguity."
  },
  {
    "objectID": "5-causation/labs/06-taste-test/slides.html#data",
    "href": "5-causation/labs/06-taste-test/slides.html#data",
    "title": "Lab 7: A Matter of Taste",
    "section": "Data",
    "text": "Data\n\n\nThis is intended to be a blank table with n rows and at least 1 column. They may plan to record either several extraneous variables or not enough to draw the conclusions they intend. As before, don’t correct them. They may discover mid-experiment that they did this part wrong, and that’s great."
  },
  {
    "objectID": "5-causation/labs/06-taste-test/slides.html#graphics",
    "href": "5-causation/labs/06-taste-test/slides.html#graphics",
    "title": "Lab 7: A Matter of Taste",
    "section": "Graphics",
    "text": "Graphics\n\n\nEncourage them to check that their viz is possible given their data frame and their data frame is possible give their protocol, and that their viz bears directly on their hypotheses."
  },
  {
    "objectID": "5-causation/labs/06-taste-test/slides.html#things-to-remember",
    "href": "5-causation/labs/06-taste-test/slides.html#things-to-remember",
    "title": "Lab 7: A Matter of Taste",
    "section": "Things to remember",
    "text": "Things to remember\n\nBe precise in your protocol.\nBe sure your claim corresponds to your protocol corresponds to your data corresponds to your plots.\n\nImportant: Lab 7.1 must be fully completed and shown to instructor before beginning Lab 7.2. Finish today if possible, by Friday night (due date) at latest.\n\n\n\n\n\n−&plus;\n\n50:00"
  },
  {
    "objectID": "5-causation/labs/06-taste-test/lab-context.html",
    "href": "5-causation/labs/06-taste-test/lab-context.html",
    "title": "A Matter of Taste",
    "section": "",
    "text": "Group Members\nWrite to the right of each role the first and last name of the group member filling that role (if there are groups of 4, two can serve as facilitators).\n\nTaster/Subject: Undergoes the experiment, tastes soda water, provides data.__________\nMaterials: Handles the materials (water et. al) for the experiment.________________\nFacilitator(s): Carries out the experiment, keeps track of time, records data.__________\n\n\n\nCausal Question\nIn one sentence, write the causal question you intend to test. It should be composed of a response variable having to do with perception of bubble water and whether it is affected by a given intervention / explanatory variable.\n\n\n\n\nNull Hypothesis\nWrite down the corresponding null hypothesis that you will gather evidence against, the alternative hypothesis, and the significance level, \\(\\alpha\\), that you intend to use. Also identify your test statistic.\n\n\n\n\n\n\nProtocol\nRecord below the step-by-step protocol that you will use to collect data to bear on the claim above. Be precise. You should be able to hand this off to another group and have them conduct your experiment.\n\n\n\nData\nIn the space below, sketch out an empty data frame that identifies the important characteristics of your data. Create the data frame such that the number of rows reflect your sample size, \\(n\\), and the number of columns represent the variables that you intend to collect. Label the columns with the names of those variables and note how you are defining your unit of observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis\nSketch a plot that you will use to use to determine how consistent your data are with your original claim. Please label the axes. Since you do not yet have data, create two sketches of what the plot might look like in two scenarios. Below each plot, write the ggplot2 code that will create that graphic. Note: these are plots of data, not null distributions of statistics."
  },
  {
    "objectID": "5-causation/04-time/slides.html",
    "href": "5-causation/04-time/slides.html",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set 20"
  },
  {
    "objectID": "5-causation/04-time/slides.html#agenda",
    "href": "5-causation/04-time/slides.html#agenda",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set 20"
  },
  {
    "objectID": "5-causation/04-time/slides.html#announcements",
    "href": "5-causation/04-time/slides.html#announcements",
    "title": "Using Time to Measure Causal Effects",
    "section": "Announcements",
    "text": "Announcements\n\nPS 19 and PS 20 both due Tuesday 4/30 at 9:00 AM\nFinal exam review sessions:\n\n\nSummarization: 12pm-1pm Monday 4/29, Stanley 105\n\nCausality: 3pm-4pm Monday 4/29, Stanley 105\n\nGeneralization: 3pm-4pm Wednesday 5/1, VLSB 2050\n\nProbability: 4pm-5pm Wednesday 5/1, VLSB 2050\n\nPrediction: 3pm-4pm Friday 5/3, Stanley 105\n\n\nFinal exam: 7pm-10pm, Thursday 5/9, room TBA.\nPlease fill out course evals!\n\nConsider applying to join Fall 2024 Stat 20 course staff (deadline 4/26)"
  },
  {
    "objectID": "5-causation/04-time/slides.html#section",
    "href": "5-causation/04-time/slides.html#section",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "Based on the plot, which of these analyses will give us a good estimate of the treatment effect?\n\nPre/post comparison.\nInterrupted time series.\nDifference-in-differences.\nNone of the above.\n\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\n\n\nThis is the first of a series of five questions that each ask students to choose an analysis method for a study with repeated measures. Each considers a dataset with repeated measures for 8 unique subjects. The response data is plotted with a line for each subject, color-coded by treatment status.\nEither before diving in or after the first question, you may want to go over some key rules for the three designs. Pre/post requires only two observations per subject but only works with flat time trends; interrupted time series requires more than two observations per subjects and can handle any linear time trend; difference-in-difference reequires units that never get treated and requires those units to have have parallel (not necessarily linear) trends to the treated units.\nFor the first question, the best answer is B: there are more than two observations per subject and the trends are linear (it may not look quite linear overall, but that’s because there’s a jump at the time of treatment). Neither pre/post nor diff-in-diff works, because there is a trend and because all units receive treatment."
  },
  {
    "objectID": "5-causation/04-time/slides.html#section-1",
    "href": "5-causation/04-time/slides.html#section-1",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "Based on the plot, which of these analyses will give us a good estimate of the treatment effect?\n\nPre/post comparison.\nInterrupted time series.\nDifference-in-differences.\nNone of the above.\n\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\n\n\nThe answer here is C. Neither pre/post nor ITS works because of the curved time trend, but there are pure controls available and their curved trend is parallel to that of the treated."
  },
  {
    "objectID": "5-causation/04-time/slides.html#section-2",
    "href": "5-causation/04-time/slides.html#section-2",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "Based on the plot, which of these analyses will give us a good estimate of the treatment effect?\n\nPre/post comparison.\nInterrupted time series.\nDifference-in-differences.\nNone of the above.\n\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\n\n\nThe answer here is D. It’s the same setting as the last question except that now we don’t have pure controls so diff-in-diff doesn’t work either."
  },
  {
    "objectID": "5-causation/04-time/slides.html#section-3",
    "href": "5-causation/04-time/slides.html#section-3",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "Based on the plot, which of these analyses will give us a good estimate of the treatment effect?\n\nPre/post comparison.\nInterrupted time series.\nDifference-in-differences.\nNone of the above.\n\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\n\n\nThe answer here is either A or B. The trends are flat so pre/post works, but ITS does too. If we saw only two observations, pre/post would be the only good option."
  },
  {
    "objectID": "5-causation/04-time/slides.html#section-4",
    "href": "5-causation/04-time/slides.html#section-4",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "Based on the plot, which of these analyses will give us a good estimate of the treatment effect?\n\nPre/post comparison.\nInterrupted time series.\nDifference-in-differences.\nNone of the above.\n\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\n\n\nThe answer here is B. Trends are present so pre/post doesn’t work; although we have pure controls their trend is very differently shaped than the treated trend so diff-in-diff doesn’t work either. However, the treated trend is linear and we have more than two observations per subject."
  },
  {
    "objectID": "5-causation/04-time/slides.html#section-5",
    "href": "5-causation/04-time/slides.html#section-5",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "A statistician conducts a pre/post comparison and attempts to obtain a confidence interval for their treatment effect estimate using the bootstrap. Shown below is the original data (at left) and one of the bootstrap samples (at right).\n\n\nOriginal Sample:\n\n\nSubject\nResponse\nTime_Period\n\n\n\nJimmy\n1.0\nPre\n\n\nJimmy\n1.5\nPost\n\n\nSarita\n4.0\nPre\n\n\nSarita\n4.2\nPost\n\n\nMin\n1.8\nPre\n\n\nMin\n2.3\nPost\n\n\n\n\nBootstrap Sample:\n\n\nSubject\nResponse\nTime_Period\n\n\n\nJimmy\n1.5\nPost\n\n\nJimmy\n1.5\nPost\n\n\nSarita\n4.0\nPre\n\n\nSarita\n4.2\nPost\n\n\nSarita\n4.0\nPre\n\n\nMin\n2.3\nPost\n\n\n\n\n\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "5-causation/04-time/slides.html#section-6",
    "href": "5-causation/04-time/slides.html#section-6",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "What is the problem with this way of using the bootstrap?\nA. The bootstrap sample does not contain the right number of observations.\nB. Some of the observations in the bootstrap sample are exact copies of each other.\nC. Unique subjects in the bootstrap sample do not have one “pre” and one “post” observation each.\nD. There is no problem, this is a valid use of the bootstrap.\n\n\nThe point of this question is to illustrate why we get into problems when we bootstrap observations ignoring dependence within subjects. The correct answer is C; the logic that motivated the entire pre/post approach (compare always within subjects) no longer makes any sense in the bootstrap sample."
  },
  {
    "objectID": "5-causation/04-time/slides.html#section-7",
    "href": "5-causation/04-time/slides.html#section-7",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(infer)\ntoy_example &lt;- data.frame('Subject' = c(rep('Jimmy',2),\n                                        rep('Sarita',2),\n                                        rep('Min',2)),\n                          'Response' = c(1.0,1.5,4.0,4.2,1.8,2.3),\n                          'Time_Period' = rep(c('Pre','Post'),3))\n\n\n\nThis slide sets up to explore the actual bootstrap confidence intervals produced by both the naive bootstrap and the correct “bootstrap differences” approach. This is here mostly so students can copy in the dataframe and work with it, and you shouldn’t need to spend much time on it."
  },
  {
    "objectID": "5-causation/04-time/slides.html#section-8",
    "href": "5-causation/04-time/slides.html#section-8",
    "title": "Using Time to Measure Causal Effects",
    "section": "",
    "text": "Incorrect:\n\nCodetoy_example |&gt;\n  specify(response = Response,\n          explanatory = Time_Period) |&gt;\n  generate(reps = 500, \n           type = 'bootstrap') |&gt;\n  calculate(stat = 'diff in means', \n            order = c('Post','Pre')) |&gt;\n  visualize()\n\n\n\n\n\n\n\n\nCorrect:\n\nCodetoy_example |&gt;\n  pivot_wider(names_from = Time_Period, \n              values_from = Response) |&gt;\n  mutate(diff = Post - Pre) |&gt;\n  specify(response = diff) |&gt;\n  generate(reps = 500, \n           type = 'bootstrap') |&gt;\n  calculate(stat = 'mean') |&gt;\n  visualize(bins=4) + xlim(-2.5,2.5)\n\n\n\n\n\n\n\n\n\n\nOe thing to highlight on this slide is the differences in the code, especially the use of pivot_wider on the RHS to create a new data frame with one row per subject. It’s a good idea to break the pipe after that command so students can see exactly what’s going on. Note also how we don’t need both response and explanatory variables in specify after taking the differences.\nThe other big thing to emphasize is how different the bootstrap distributions look. Because the subjects look very different from each other in response but have relatively small pre/post changes, the bad bootstrap throws in all kinds of extra noise that muddies up our effect."
  },
  {
    "objectID": "5-causation/02-experiments/notes.html",
    "href": "5-causation/02-experiments/notes.html",
    "title": "Randomized Experiments",
    "section": "",
    "text": "Causation has a tricky relationship with data. We can’t observe the precise counterfactual that would allow us to identify an individual cause. Instead, statisticians use a host of methods to find a good approximation of the counterfactual, usually by focusing on a group-level quantity called an average treatment effect.\nWe now introduce the most important tool for approximating group-level counterfactuals, the randomized experiment."
  },
  {
    "objectID": "5-causation/02-experiments/notes.html#principles-of-experimental-design",
    "href": "5-causation/02-experiments/notes.html#principles-of-experimental-design",
    "title": "Randomized Experiments",
    "section": "Principles of Experimental Design",
    "text": "Principles of Experimental Design\nAn experiment is generally characterized as being a setting where the researcher actively assigns subjects or units to one particular condition or another. The most potent design of an experiment to determine whether one variable, the treatment, affects the outcome at the group level is the aptly named Randomized Controlled Trial (RCT).\nAs a running example, consider an immediately relevant question: would reading these notes as a PDF lead to a deeper understanding and a correspondingly better score on the final exam than reading them as a webpage? Let’s run through each term one-by-one to think through how to design an RCT and effectively answers this question.\nControl\n\nControl\n\n(noun) A second condition to which a subject or unit could be assigned that draws a contrast with the treatment condition.\n\n\n(verb) Actions taken to eliminate other possible causal factors and ensure that the only difference between the treatment and control group is the factor of interest.\n\n\nWhen designing an RCT, an essential decision is the nature of your control group. Our research question is: will reading these notes as a webpage result in a deeper understanding? Deeper . . . than what? Deeper than if you didn’t read the notes at all? Deeper than if you read them aloud?\nIf we’re most interested in the difference between reading the notes as a webpage and reading them as a pdf, we could declare those who read the pdf part of the control group and those who read the pdf as part of the treatment group. Small changes to the control group can make an important difference in the precise question that you’ll be answering.\nWe can visualize the distinction between the two groups in a small example with three people in the control group (pdf) and three people in the treatment group (website).\n\n\n\n\n\n\nname\ngroup\nunderstanding\n\n\n\nEvelyn\npdf\ndeep\n\n\nGrace\npdf\nshallow\n\n\nJuan\npdf\ndeep\n\n\nAlex\nwebsite\ndeep\n\n\nMonica\nwebsite\nshallow\n\n\nSriya\nwebsite\nshallow\n\n\n\n\n\n\n\n\nThe process of computing a difference in proportions and conducting a hypothesis test for this dataset is no different than in the examples from the generalization notes. What makes this study special is the process that generated this data. Because this was a randomized controlled experiment, the difference in proportions provides a good estimate of the average treatment effect, and if it differs significantly from zero we have compelling evidence that reader format has a causal impact on understanding. We now describe some aspects of experimental design that help establish this stronger interpretation.\nRandom Assignment\n\nRandom Assignment\n\nThe process of assigning subjects to either the treatment or control group in a random fashion, where they’re equally likely to be assigned to each group.\n\n\nBecause we are conducting an experiment, we are intervening in the process and directly assigning subjects to either the treatment (pdf readers) and control (website readers). There are many ways we could do this. The morning sections could all be assigned the pdf and the afternoon sections the website. Or the instructors could choose to exclusively assign either the pdf or the website version of the notes to their individual sections.\nThe problem with both of these approaches is that our two groups might differ on more characteristics than just their reading format. The morning sections perhaps have students who are early risers and more conscientious students. Or perhaps the instructors who choose the website are more tech-savvy and more effective at teaching computing. In both cases, we invite the skeptical observer to question whether it was truly the medium of the notes that led to a difference in course grades or if it was something else.\nThe mechanism of random assignment is brilliant because it can snuff out every such skeptical inquiry in one fell swoop. If instead we assigned each student to read the pdf or the website totally at random, every other possible characteristic between these groups should, on average, be balanced. If students are assigned at random, we’d expect an equal number of early-risers to be in both the pdf and the website group. We’d also expect the students with the more effective instructors to be evenly represented in both groups. And so on and so forth, for every possible other characteristic that might muddy the waters of our causal claim.\nReplication\n\nReplication\n\nThe practice of assigning multiple subjects to both the treatment and control group. Also, the practice of repeating an experiment a second time to see if the result is consistent.\n\n\nThe careful reader will have noted a weakness in the brilliance of the random assignment mechanism for balancing the characteristics between the groups. What if, purely due to bad luck, we happen to randomly assign all of the early-risers to the pdf group and all of the late-risers to the website group? That would indeed bring us back to the problem of there being many ways in which our treatment group is different from our control group.\nThe random assignment mechanism will balance out all possible confounding factors on average, but for a given experiment that is not guaranteed. However, it becomes much more likely if we have a large sample size. If you just have four students total in the class, two of whom are early risers, it’s quite easy for both of them to end up in the pdf group if they were assigned at random. If instead you have 800 students, 400 of whom are early risers, it’s very very unlikely that all 400 will have made their way into the pdf group.\nBlinding\nRandomized controlled trials have long been considered the gold standard for establishing a group-level causal claim, but care must be taken to ensure that your result means what you think it means. Here we reconsider a study where a new drug was used to treat heart attack patients. In particular, researchers wanted to know if the drug reduced deaths in patients.\nThese researchers designed a randomized control trial because they wanted to draw causal conclusions about the drug’s effect. Study volunteers were randomly assigned to one of two study groups. One group, the treatment group, received the drug. The control group did not receive any drug treatment.\nPut yourself in the place of a person in the study. If you are in the treatment group, you are given a fancy new drug that you anticipate will help you. If you are in the control group, you are not treated at all but instead sit by idly, knowing you are missing out on potentially life-saving treatment. These perspectives suggest there are actually two effects in this study: the one of interest is the effectiveness of the drug, and the second is the emotional effect of (not) taking the drug, which is difficult to quantify. In order to control for the emotional effect of taking a drug, the researchers decide to hide from patients which group they are in.\n\nBlinding\n\nThe practice of keeping someone uninformed about which subjects in the study have been assigned to treatment.\n\n\nWhen researchers keep the patients uninformed about the treatments they will receive, the study is said to be single-blind. But there is one problem: if a patient does not receive a treatment, they will know they’re in the control group. A solution to this problem is to give a fake treatment to patients in the control group. This is called a placebo, and an effective placebo is the key to making a study truly blind. A classic example of a placebo is a sugar pill that is made to look like the actual treatment pill. However, offering such a fake treatment may not be ethical in certain experiments. For example, in medical experiments, typically the control group must get the current standard of care. Oftentimes, a placebo results in a slight but real improvement in patients. This effect has been dubbed the placebo effect.\nThe patients are not the only ones who should be blinded: doctors and researchers can also unintentionally affect the outcome. When a doctor knows a patient has been given the real treatment, they might inadvertently give that patient more attention or care than a patient that they know is on the placebo. To guard against this, which again has been found to have a measurable effect in some instances, most modern studies employ a double-blind setup where doctors or researchers who interact with patients are, just like the patients, unaware of who is or is not receiving the treatment."
  },
  {
    "objectID": "5-causation/02-experiments/notes.html#using-additional-variables-in-experimental-design",
    "href": "5-causation/02-experiments/notes.html#using-additional-variables-in-experimental-design",
    "title": "Randomized Experiments",
    "section": "Using additional variables in experimental design",
    "text": "Using additional variables in experimental design\nCovariates\nLet’s return to our hypothetical pdf vs. website experiment and imagine that in addition to our subject’s names and understanding level we were able to collect some additional information about each of them.\n\n\n\n\n\n\nname\ngroup\nunderstanding\nmajor\nGPA\nnative_speaker\n\n\n\nEvelyn\npdf\ndeep\nStatistics\n3.81\nYes\n\n\nGrace\npdf\nshallow\nEconomics\n3.63\nNo\n\n\nJuan\npdf\ndeep\nEconomics\n3.20\nYes\n\n\nAlex\nwebsite\ndeep\nStatistics\n2.85\nNo\n\n\nMonica\nwebsite\nshallow\nEconomics\n3.19\nYes\n\n\nSriya\nwebsite\nshallow\nStatistics\n3.80\nYes\n\n\n\n\n\n\nWe introduce a new term to draw an important distinction based on when these additional variables were collected.\n\nCovariate\n\nA variable collected in a randomized experiment that was measured or determined prior to administering the treatment.\n\n\nNotice that covariates cannot be affected by the treatment since they were measured first. Variables measured after treatment, including the outcome variable, may be affected by the treatment.  In our example, it’s reasonable to imagine that major, GPA, and native speaker status could all have been measured prior to the assignment, so they are covariates.\n\nCovariate balance\nNote that many of these covariates seem likely to be associated with the outcome. For example, students with a higher GPA may be more committed to doing class readings and be more likely to gain a deep understanding regardless of format. If one group had more high-GPA students, that group might look like it had benefited from its reading format even if there were no difference between the formats for any single student. This motivates us to look for a quality called balance.\n\nCovariate balance\n\nA covariate is balanced if its empirical distribution in the treated group is similar to its empirical distribution in the control group.\n\n\nOn the other hand, if the distributions are very different between the treatment group and the control group, we say that the covariate is imbalanced.\nThere is a close link between covariate balance and randomized treatment assignment. Because treatment is randomly assigned with equal probability for each individual in our study, there is no way it should be able to systematically move higher-GPA students into one of the two groups. The same is true of all our other covariates. Thus we expect randomization to balance our covariates in general.\n\nTo check the balance of our GPA variable we can calculate a measure of difference between groups for a single covariate \\(x\\):\n\\[\nSMD(x) = \\frac{\\overline{x}_{treatment} - \\overline{x}_{control}}{\\sqrt{\\frac{1}{2}\\widehat{\\sigma}_{treatment}^2 + \\frac{1}{2}\\widehat{\\sigma}_{control}^2}}\n\\]\nThis quantity is called the standardized mean difference or SMD. The numerator is just the difference in means (subtracting the control mean from the treated mean). To make the differences in means comparable across variables with very different scales, we divide by a `pooled’ standard error estimate which comes from averaging the sample variances of the covariate in the two groups and taking its square error. For GPA in our data frame above, the difference in means between groups is 3.28 \\(-\\) 3.55 \\(=\\) -0.27 and the pooled standard error is 0.41, so the SMD is  -0.66.\nEvaluating covariate balance across an experiment\nTo get a quick look at balance across all our variables, we can create a Love plot:\n\n\n\n\n\n\n\n\nThe Love plot displays standardized mean differences (on a common scale on the x-axis) for all the covariates in our study, highlighting which covariates are most different between groups and in which direction.\nClearly in our experiment not all variables are perfectly balanced. This alone isn’t a red flag — after all, randomized experiments only balance covariates on average, and some small differences are expected — but it can be challenging to determine whether the differences are big enough to matter, especially in a study this small.\nOne way to address this problem is by using hypothesis tests. For each covariate, we can test the null hypothesis that this covariate is independent of treatment and produce a p-value. In practice this is just like the tests from the generalization unit, but it uses the covariate in the test statistic instead of the outcome. Here we conduct this test for GPA.\n\n\n\n\n\n\n\n\nThe p-value is 0.396. Clearly the difference in means we observed for GPA is not unexpected under random treatment assignment given our small sample size. Note that although our ultimate project here is investigating a causal claim, this particular step is an example of generalization: based on the particular set of treatment assignments we see in this dataset (the sample), we are making a claim about the broader process that generated the treatment assignments.\n\n\n\n\n\nOne of the most important reasons to check covariate balance is to catch problems with how the experiment was run. Suppose that we’d intended to sort a spreadsheet in a random order and assign the top half of the subjects to pdf, but that by accident we sorted on GPA instead of randomly. In this case our covariate balance test would likely have rejected the null hypothesis that treatment was randomly assigned, allowing us to catch the issue. For this reason it is often a good idea to check balance before actually running the experiment!"
  },
  {
    "objectID": "5-causation/02-experiments/notes.html#the-ideas-in-code",
    "href": "5-causation/02-experiments/notes.html#the-ideas-in-code",
    "title": "Randomized Experiments",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nFirst we create a data frame to store the pdf vs. website data. ::: {.cell}\nformat_data &lt;- data.frame(name = c(\"Evelyn\", \"Grace\", \"Juan\", \"Alex\", \"Monica\", \"Sriya\"),\n                 group = c(\"pdf\", \"pdf\", \"pdf\", \"website\", \"website\", \"website\"),\n                 understanding = c(\"deep\", \"shallow\", \"deep\", \"deep\", \"shallow\", \n                                   \"shallow\"),\n                 major = c('Statistics', 'Economics', 'Economics', 'Statistics', \n                           'Economics', 'Statistics'),\n                 GPA = c(3.81, 3.63, 3.20, 2.85, 3.19, 3.80),\n                 native_speaker = c('Yes','No','Yes','No','Yes','Yes'))\n:::\nThe cobalt package in R contains the function bal.tab to create tables of standardized differences. By passing its output to plot you can create a Love plot. Note that cobalt expect treatment variables to be numeric or logical, so we begin by converting group to the logical variable is_website1.\n\nlibrary(cobalt)\nformat_data &lt;- format_data |&gt;\n                mutate(is_website = group == 'website')\n\nbal.tab(is_website ~ major + GPA + native_speaker, data = format_data,\n        s.d.denom = 'pooled', binary = 'std') |&gt;\n  plot()\n\n\n\n\n\n\n\nRunning balance tests uses infer much like we did in the generalization unit. The covariate for which we are testing balance is the response and the treatment is the explanatory variable.\n\nlibrary(infer)\nset.seed(2024-3-25)\nobs_stat &lt;- format_data |&gt;\n   specify(explanatory = group,\n          response = GPA) |&gt;\n  calculate(stat = \"diff in means\", order = c(\"website\",\"pdf\"))\n\nnull &lt;- format_data |&gt;\n  specify(response = GPA,\n          explanatory = group) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 500, type = \"permute\") |&gt;\n  calculate(stat = \"diff in means\", order = c(\"website\",\"pdf\"))\n\nnull |&gt;\n  visualize() +\n  shade_p_value(obs_stat, direction = 'both')"
  },
  {
    "objectID": "5-causation/02-experiments/notes.html#summary",
    "href": "5-causation/02-experiments/notes.html#summary",
    "title": "Randomized Experiments",
    "section": "Summary",
    "text": "Summary\nYou may heard the phrase “Correlation does not imply causation … unless it’s seen in data from a randomized control trial.” While it is sometimes possible to make compelling causal claims from correlations found in data - a topic we will explore in depth next week - it does highlight the particular strength of RCTs. RCTs are able to isolate the effect of interest by creating a carefully selected control group and then assigning subjects to groups at random. By using a large number of replicates and checking balance on measured covariates we can demonstrate convincingly that our control group on average serves as a close counterfactual to our treatment group."
  },
  {
    "objectID": "5-causation/01-defining-causality/notes.html",
    "href": "5-causation/01-defining-causality/notes.html",
    "title": "Defining Causality",
    "section": "",
    "text": "At the beginning of this course, we considered four different claims associated with the following news article1.\nEach one of these four claims illustrates a different type of claim made using data. As a brief recap of where we are in this course, let’s take them each in turn.\nThis is a claim concerning the nature of the data that is on hand, a descriptive claim. While these seem like the most straightforward type of claim, don’t underestimate their utility or the challenges involved in crafting them. Deciding which measure is most appropriate is tricky work and the process of wrangling the data takes careful thought and time.\nThis claim looks deceptively like the first but there is one important difference. The first claim concerns the CPI, which is calculated using data from the US. This second claim is about the broader global population of which the US data is a subset. In other words, this is a generalization from a sample to a population.\nFor a generalization to be sound, we must take several considerations into account. First off: is the sample representative of the population or is it biased in some way? Secondly: what sources of variability are present? When working with a sample that originated from a chance method, it’s important to consider the degree to which sampling variability might be able to explain the structure you see in the data. Our primary tools in this area are the confidence interval, to assess the uncertainty in a statistic, and the hypothesis test, to assess whether a particular statistic is consistent with an assertion about the state of the population parameters.\nThis is a prediction, a claim that uses the structure of the data at hand to predict the value of observations about which we have only partial information. In midsummer, we know the date will be July 15th, that’s the x-coordinate. But what will the y-coordinate be, the Consumer Price Index? Now we recognize this as a regression problem.\nThis bring us to the final claim, which is one concerning causation. The claim asserts that the structure in the data (the rise in the CPI) can be attributed to specific cause (the war in Ukraine). Causal claims are often the most challenging claims to craft but they are also some of the most useful. Uncovering causes and effects is at the heart of many sciences from Economics to Biomedicine. They also help guide decision making for individuals (is it worth my time to study for the final?) as well as for organizations (will Twitter’s new option to pay for verification result in a net increase in revenue for the company?).\nFor the remainder of Stat 20, we lay the foundation for causation, first by defining it precisely, then identifying a few of the most powerful strategies for inferring it from data."
  },
  {
    "objectID": "5-causation/01-defining-causality/notes.html#causality-defined",
    "href": "5-causation/01-defining-causality/notes.html#causality-defined",
    "title": "Defining Causality",
    "section": "Causality Defined",
    "text": "Causality Defined\n\nWhat exactly does it mean to say that “A causes B”?\nWe speak of causes and effects all the time, even though the language we use varies widely. “I took an aspirin and my headache got better” implies that taking the aspirin is what caused your headache to get better. “She was able to find a good job because she graduated from Berkeley” is more direct: graduating from Berkeley was the cause of her being able to find a good job.\nIdentifying a causal statement is one thing, but we’re still left the conundrum: what definition can we use to be precise about the meaning of a causal statement?\nLet’s see what your intuition tells you about what is a cause and what is not a cause. Which causes do you identify in the following scenario2?\n\nSuppose that a prisoner is about to be executed by a firing squad. A certain chain of events must occur for this to happen. First, the judge orders the execution. The order goes to a captain, who signals the two soldiers of the firing squad (soldier 1 and soldier 2) to fire. They are obedient and expert marksmen, so they only fire on command, and if either one of them shoots, the prisoner dies.\nWho caused the death of the prisoner?\nA. The judge\nB. The captain\nC. Soldier 1\nD. Soldier 2\n\nAs you ponder where to draw the line to determine which of the these four people are the cause of the death of the prisoner, you are working out your own internal definition of causation. Keep your answers on hand; we will discuss this example in class. For now, though, let’s introduce the most widely used definitions of cause and effect.\nThe Conditional Counterfactual\nOne of the earliest articulations of what it means to be a cause can be found in the writing of Thucydides, the ancient Greek historian. It comes at the end of a passage where he describes a village called Orobiae, which experienced an earthquake followed by a tsunami.\n\nAbout the same time that these earthquakes were so common, the sea at Orobiae, in Euboea, retiring from the then line of coast, returned in a huge wave and invaed part o the town, and retreated leaving some of it still under water; so that what was once land is now sea…\nThe cause, in my opinion of this phenomenon must be sought in the earthquake. At the point where its shock has been the most violent, the sea is driven back, and suddenly recoiling with redoubled force, causes the inundation.\nWithout the earthquake, I do not see how such an accident could happen.\n\nIn the final line, Thucydides makes a leap: he imagines a world where the earthquake didn’t happen, and can’t imagine the tsunami happening. This, for him, is what makes the earthquake the cause of the tsunami. This form of reasoning about causation was summarized centuries later by the Scottish philosopher David Hume, who characterized a cause as a scenario in which “If the first object had not been, the second never had existed.”\nBoth of these definitions rely upon imagining a world that was different from the one that was observed, a notion in logic called a counterfactual.\n\nCounterfactual\n\nRelating to or expressing what has not happened or is not the case.\n\n\nThis notion is the core component of the most widely used definition of a cause, the conditional counterfactual defintion.\n\nCause\n\nWe say “A causes B” if, in a world where A didn’t happen, B no longer happens.\n\n\nUsing this definition of causality, let’s revisit two examples from above.\n\n\nConsider the claim, “I took an aspirin and my headache got better”.\nUsing the conditional counterfactual definition, what would you need to know to determine if the aspirin caused the headache to improve?\n\n\n\nCheck your answer\nYou would need to know that if they hadn’t taken an aspirin, that their headache didn’t get better.\n\n\nConsider the claim, “She was able to find a good job because she graduated from Berkeley”.\nUsing the conditional counterfactual definition, what would you need to know to determine if graduating from Berkeley caused her to find a good job?\n\n\n\nCheck your answer\nYou would need to know that if she hadn’t graduated from Berkeley, that she wasn’t able to find a good job.\nIn both of these examples, reasoning about the meaning of causation requires identifying the counterfactual. The language of counterfactuals can be awkward and that awkwardness points to the primary challenge of identifying a causal claim.\nThe Challenge of Causation\n\nCounterfactuals have a particularly problematic relationship wth data because data are, by definition, facts. - Judea Pearl\n\nThe conditional counterfactual definition of causation is sound in an abstract sense, but it is challenging when you start to think through what sort of data you could collect as evidence of causation. In the second example, we have data on the fact that she found a good job and that she graduated from Berkeley, but the counterfactual - that remains purely hypothetical. In fact, the word counterfactual means counter-to-fact, and “fact” is the meaning of the Latin word “datum” (a single piece of data). That is to say, for airtight evidence of a cause-and-effect, you must observe some data and then something that is somehow also the contrary to what you observed.\nIn an idealized world, to demonstrate that graduating from Cal was the cause of getting a good job, you would observe this data frame3.\n\n\n\n\nStudent\nCal Grad\nGood Job\n\n\n\nEvelyn Fix\nyes\nyes\n\n\nEvelyn Fix\nno\nno\n\n\n\n\n\nIn this idealized data frame, the two rows are both observations of the same person, so they have the same values of every possible variable: work experience, GPA, letters of recommendation, etc. The primary difference is one of them graduated from Cal and the other (from the counterfactual world) did not. Because they differed on their outcome variables (getting a good job), this would serve as rock solid proof that graduating from Cal caused Evelyn to get a good job.\nThe challenge of using data to make causal clams is that we only ever get to observe one of the two rows above. Said another way, there are two potential outcomes for this scenario. One was observed (the job outcome after going to Cal) and the other was not (the job outcome without going to Cal).\nIf you’ve ever used a GPS navigation app, you’re already accustomed to thinking in terms of potential outcomes. Here is the guidance Google Maps gives me to travel from Pimentel Hall at Cal to downtown Oakland by car.\n\n\n\n\nEach one of the three paths is a potential route I could take and each of those times are the app’s predictions for what the potential outcomes will be. Importantly, though, these are just predictions, not data. To collect data, I have to select one of these routes to drive, then I could record data on the time it took me. If I choose the blue path and it ends up taking me 16 minutes, I’ll never know for sure that it was my choice of the blue route that led to this apparently short drive time. To know that, I’d have to rewind the clock and, in a different world, decide to take one of the gray routes and observe a drive time that is more than 16 minutes.\nWhile our definition of causation prevents us from ever making completely airtight conclusions about cause and effect in scenarios like these, over the years scientists and statisticians have crafted many clever strategies for working around these constraints to build compelling causal claims."
  },
  {
    "objectID": "5-causation/01-defining-causality/notes.html#from-individual-level-claims-to-group-level-claims",
    "href": "5-causation/01-defining-causality/notes.html#from-individual-level-claims-to-group-level-claims",
    "title": "Defining Causality",
    "section": "From individual-level claims to group-level claims",
    "text": "From individual-level claims to group-level claims\nOur inability to observe both counterfactuals makes it hard to make reliable claims about causal effects for an individual. However, it can sometimes be easier to make a claim about the typical causal effect among a group of individuals. The distinction between individual and group level causation is demonstrated in following two statements.\n\nEvelyn got a good job because she graduated from Cal.\nGraduating from Cal helps people get a good job.\n\nThe first is a strong statement about a single individual, Evelyn. The second is a much more general statement that compares people who have have graduated from Cal with a counterfactual group who has not.\nGroup-level causation is the focus of many sciences, which aim to make general claims about the causal mechanisms of the world. The goal is to estimate the average treatment effect.\n\nAverage Treatment Effect\n\nA population parameter that captures the effect of being administered a particular treatment, averaged over each group. Most often estimated by a difference in sample means or a difference in sample proportions.\n\n\nIn statement two above, a natural estimate for the average treatment effect would be the difference between the proportion of Cal graduates who got a good job and the proportion of non-Cal graduates who got a good job. That can be visualized in a simple example of three students who graduated from Cal and three who did not. The difference in the proportion with a good job is \\(2/3 - 1/3 = 1/3\\).\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nName\nCal Grad\nGood Job\n\n\n\nEvelyn\nTRUE\nTRUE\n\n\nGrace\nTRUE\nFALSE\n\n\nJuan\nTRUE\nTRUE\n\n\nAlex\nFALSE\nFALSE\n\n\nMonica\nFALSE\nFALSE\n\n\nSriya\nFALSE\nTRUE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCal Grad\nP(Good Job)\n\n\n\nTRUE\n0.67\n\n\nFALSE\n0.33\n\n\n\n\n\n\n\n \n\n\n\nIt is easier to come up with plausible-seeming estimates of average treatment effects than individual treatment effects because we see subjects under both treatment and control conditions. But to decide if estimates like the one we’ve just given are reliable we need to think about how the data for the two groups was generated and whether they serve as good approximations to each other’s counterfactuals. This question will be the focus of the rest of the causality unit."
  },
  {
    "objectID": "5-causation/01-defining-causality/notes.html#summary",
    "href": "5-causation/01-defining-causality/notes.html#summary",
    "title": "Defining Causality",
    "section": "Summary",
    "text": "Summary\nWe set the stage for reasoning about causation by defining cause and effect in terms of a conditional counterfactual. We say “A causes B” if, in a world where A didn’t happen, B no longer happens. This definition is problematic because we can’t simultaneously observe the same subjects under A and not-A. However, there is more hope to say something meaningful about average treatment treatment effects, which can also be defined using counterfactuals.\nIn the remainder of the causality unit, we will discuss strategies for estimating average treatment effects using carefully-designed comparisons that leverage knowledge about how the data was produced. \n\n\n\n\nIllustration by Mayaan Hayal4"
  },
  {
    "objectID": "5-causation/01-defining-causality/notes.html#footnotes",
    "href": "5-causation/01-defining-causality/notes.html#footnotes",
    "title": "Defining Causality",
    "section": "Footnotes",
    "text": "Footnotes\n\nSmialek, Jeanna (2022, May 11). Consumer Prices are Still Climbing Rapidly. The New York Times. https://www.nytimes.com/2022/05/11/business/economy/april-2022-cpi.html↩︎\nThis example appears in The Book of Why (2018) by Pearl and Mackenzie, as do subsequent historical quotations from Thucydides and Hume.↩︎\nAn a historical aside, Evelyn Fix is the name of a past professor of statistics at UC Berkeley and the co-inventor of the k-nearest neighbors algorithm.↩︎\nA drawing from “The Book of Why” depicting the notion of potential outcomes described in Robert Frost’s poem The Road Not Taken.↩︎"
  },
  {
    "objectID": "5-causation/03-matching/notes.html",
    "href": "5-causation/03-matching/notes.html",
    "title": "Causal Effects in Observational Studies",
    "section": "",
    "text": "Randomized experiments are very useful for learning about causal claims; because comparison groups are guaranteed to be similar on average before the treatment is given, differences in outcomes between the groups must be the result of the treatment.\nHowever, we frequently need to evaluate causal claims without having access to data from a randomized experiment.  Today we’ll explore how to take some of the ideas important in understanding and analyzing randomized experiments and use them to make progress with data where treatment assignments are not determined by researchers. Such non-experimental studies are called observational studies, emphasizing that all researchers do is observe treatment assignments rather than making them."
  },
  {
    "objectID": "5-causation/03-matching/notes.html#causal-claims-without-randomization",
    "href": "5-causation/03-matching/notes.html#causal-claims-without-randomization",
    "title": "Causal Effects in Observational Studies",
    "section": "",
    "text": "Randomized experiments are very useful for learning about causal claims; because comparison groups are guaranteed to be similar on average before the treatment is given, differences in outcomes between the groups must be the result of the treatment.\nHowever, we frequently need to evaluate causal claims without having access to data from a randomized experiment.  Today we’ll explore how to take some of the ideas important in understanding and analyzing randomized experiments and use them to make progress with data where treatment assignments are not determined by researchers. Such non-experimental studies are called observational studies, emphasizing that all researchers do is observe treatment assignments rather than making them."
  },
  {
    "objectID": "5-causation/03-matching/notes.html#natural-experiments",
    "href": "5-causation/03-matching/notes.html#natural-experiments",
    "title": "Causal Effects in Observational Studies",
    "section": "Natural experiments",
    "text": "Natural experiments\n1\nDo evaluators tend to favor members of the same race? In the mid-2000s two economists interested in this question, Joseph Price and Justin Wolfers, obtained data on 12 years of referee decisions from professional National Basketball Association (NBA) games.  They compared the rate at which referees gave fouls to players of the same racial category (Black or non-Black) to the rate at which fouls were given to players of the opposite category.\nThis study aims to investigate a causal claim: for a given player in a given game, would the player’s foul rate have been different if a different referee had been assigned whose racial categorization did (or did not) match the player’s racial categorization?\nIf Price and Wolfers had reached an agreement with the NBA that allowed them to assign referees to games at random, they could have conducted a randomized experiment to answer this question using the ideas from the last set of notes. This did not happen; however, Price and Wolfers argue that the actual manner in which referees were assigned to games closely resemble what they would have done in their ideal hypothetical experiment:\n\nAssignments of referees to crews are made to balance the experience of referees across games, with groups of three referees working together for only a couple of games before being regrouped. According to the NBA, assignments of refereeing crews to specific (regular season) games is “completely arbitrary” with no thought given to the characteristics of the competing teams. Each referee works 70 to 75 games each year, and no referee is allowed to officiate more than nine games for any team, or referee twice in a city within a fourteen-day period. Although these constraints mean that assignment of refereeing crews to games is not literally random, the more relevant claim for our approach is that assignment decisions are unrelated to the racial characteristics of either team.2\n\nTo back up their claim, Price and Wolfers conduct hypothesis tests for balance on a variety of covariates. Here is an excerpt from one of their balance tables. Each row gives a p-value for a test of the null hypothesis that this covariate is independent of the number of non-Black referees.\n\n\nCovariate\nP-value from balance test\n\n\n\nYear\n0.00\n\n\n#Black starters (home team)\n0.75\n\n\n#Black starters (away team)\n0.72\n\n\nAttendance\n0.49\n\n\nHome team out of contention\n0.94\n\n\nAway team out of contention\n0.81\n\n\n\nAlthough the table suggests that the number of non-Black referees did vary over time (notice the .00 p-value in the first row) the other p-values are fairly large, suggesting approximate balance.\n\nIf indeed the process of assigning referees is independent of the race, team, and popularity of any particular team or player, as well as any other covariates that might be good predictors of foul rate, then each group provides a good approximation to the missing counterfactual outcome distribution for the other group, and the study can be analyzed as though it were from a truly-randomized experiment to produce a good average treatment effect estimate.\nAt this point the problem has been reduced to one of generalization (what does the difference in foul rates in this sample say about the true difference in counterfactual foul rates for all NBA players and games?), and Price and Wolfers use a hypothesis test to address it. They find a significant discrimination effect: on average basketball players facing an officiating referee of the same racial category enjoyed a foul rate 4% lower than when facing an officiating referee of the opposite category, and also scored 2.5% more points.3\nStudies like this are often known as natural experiments (or quasi-experiments).\n\nNatural experiment\n\nA study in which researchers did not randomly assign treatment but claim that the treatment process is sufficiently independent of covariates to justify treatment effect estimation."
  },
  {
    "objectID": "5-causation/03-matching/notes.html#similar-units-with-different-treatments",
    "href": "5-causation/03-matching/notes.html#similar-units-with-different-treatments",
    "title": "Causal Effects in Observational Studies",
    "section": "Similar units with different treatments",
    "text": "Similar units with different treatments\nWhile in natural experiments we can learn about causal effects from purely observational data, only rarely are treatments we care about known to be quasi-randomly assigned as in the NBA refereeing example. More generally, we may have only amorphous knowledge of how treatment assignment occurs, or we may have compelling reasons to believe that treated and control individuals differ systematically. In this setting we need a new strategy for causal effect estimation.\nSiblings\nLet’s return to our example of evaluating the impact of graduating from Cal on obtaining a good job. Imagine that Cal student Evelyn Fix had a sister, Eleanor Fix. If Evelyn graduated from Cal and Eleanor did not, then we could observe this data frame.\n\n\n\n\nStudent\nGPA\nYears exp\nRec\nCal Grad\nGood Job\n\n\n\nEvelyn Fix\n3.9\n3\nstrong\nyes\nyes\n\n\nEleanor Fix\n3.9\n3\nstrong\nno\nno\n\n\n\n\n\nTo understand the causal effect of college for Evelyn, we need to know the counterfactual value of the ‘Good Job’ variable in the setting where she didn’t attend college. Could we use Eleanor to stand in for Evelyn’s counterfactual?\nWell, it depends on how similar they are to one another in ways that matter to the mechanism of cause and effect. As we see in the table above, they’re a perfect match on several variables that probably matter: GPA, the number of years of work experience, and the strength of letters of recommendation.\n\n\n\n\n\nThis idea has led to clever and impactful studies to evaluate whether smoking causes cancer. Imagine you are a doctor with a patient who smokes and has been recently diagnosed with cancer. When you tell them, “I’m afraid your smoking habit has caused your cancer”, they protest: “Not at all! I’m quite sure I have a gene that causes me to want to smoke and also causes me to get cancer. If I had stopped smoking, it wouldn’t have changed a thing!”.\nThat is a difficult explanation to refute. What would the counterfactual look like? You’d need either to run an RCT (which would be unethical) or find someone with the exact same genetic makeup who happened to not be a smoker. But surely this close of a match doesn’t exist…\nUnless your patient is one of an identical pair of twins. While this scenario is rare, there are plenty of pairs of identical twins that can be used to evaluate precisely this kind of scenario. At the end of the 20th century, researchers in Finland compiled a large data base of identical twins where one of them smoked and the other did not. In pair after pair, they found it much more likely that the twin who smoked was more likely to develop cancer. This technique, using identical twins to get perfect matches on genetics, has been a rich source of breakthroughs in understanding genetic determinants of disease4.\nMatching\nIn most data sets, you are not guaranteed to have a sibling for each subject. However, we can still use the idea of the Evelyn-Eleanor study by looking directly at the covariates for each treated subject and searching for a control subject with similar values. This general approach is called matching5.\nThere are many methods for determining which two units in a data set are the closest matches for one another. One simple idea is to compute the Euclidean distance between the covariates of any two units. Here is the formula for Euclidean distance \\(d_{ij}\\) between two subjects \\(i\\) and \\(j\\) each with \\(k\\) measaured covariates. We let \\((x_{1i}, \\ldots, x_{ki}\\) represent subject \\(i\\)’s covariates, and \\((x_{1j}, \\ldots, x_{kj}\\) represent subject \\(j\\)’s covariates.\n\\[\nd_{ij} = \\sqrt{\\sum^k_{\\ell=1}(x_{\\ell i} - x_{\\ell j})^2}\n\\]\nIf \\(k=1\\) this reduces to the absolute difference \\(|x_{1i} - x_{1j}|\\). If \\(k=2\\), then Euclidean distance is the normal two-dimensional distance we are familiar with i.e. what you would get if you use a ruler to measure distance between two points on a piece of paper6.\nAfter computing the Euclidean distance between each treated unit and each control with the smallest distance, we pair each treated unit to a different control (or if the number of controls is smaller, we pair each control to a different treated unit). We would like the “best” set of pairs according to the Euclidean distance, so we solve an optimization problem to identify the set of pairs with the lowest possible average Euclidean distance. Conceptually, this is similar to picking a regression line to minimize \\(RSS\\), although we need to use a different kind of algorithm than Nelder-Mead because we are picking sets of pairs instead of coefficient values (we won’t go over the algorithm). This is called optimal matching. We demonstrate this method of optimal matching using a small synthetic data set based on the college graduation data. We start with 5 treated units and 15 controls.\n\n\nlibrary(stat20data)\nhead(grad_job, n = 20)\n\n    GPA years_exp      rec cal_grad good_job\n1  3.73         3 moderate    FALSE      yes\n2  3.36         0 moderate    FALSE       no\n3  3.25         1   strong    FALSE      yes\n4  3.19         0 moderate     TRUE      yes\n5  2.66         3     weak    FALSE      yes\n6  3.67         3 moderate    FALSE       no\n7  3.22         3 moderate    FALSE       no\n8  3.16         1   strong     TRUE      yes\n9  2.70         1   strong    FALSE       no\n10 3.11         2 moderate    FALSE      yes\n11 3.65         2 moderate     TRUE      yes\n12 3.84         0     weak    FALSE      yes\n13 3.42         3 moderate    FALSE      yes\n14 3.27         2 moderate     TRUE      yes\n15 2.67         1   strong     TRUE      yes\n16 3.76         2     weak    FALSE      yes\n17 3.11         0   strong    FALSE       no\n18 3.29         2 moderate    FALSE      yes\n19 3.48         2   strong    FALSE      yes\n20 3.73         1 moderate    FALSE      yes\n\n\nWe conduct optimal matching. Since there are 5 treated units and 15 controls, we end up forming five matched pairs and discarding the 10 leftover controls (if we had started with more treated units than controls, we would have discarded treated units instead).\nThese plots show how covariate balance has improved thanks to matching, using density plots for treated and control groups overlaid on one another (the gray lines give the density of the control distribution for each covariate and the black lines give the density of the treated distributions). \n\n\n\n\n\n\n\n\nWe can also look at a Love plot comparing the standardized differences before and after7 matching.\n\n\n\n\n\n\n\n\nPrior to matching, students who did not go to Cal looked very different from those who did; on average, they had higher GPAS and more years of experience, but the Cal grads were much more likely to have strong letters and much less likely to have weak letters. However, after matching the two groups look almost identical, albeit with a slightly higher average GPA among the Cal grads8."
  },
  {
    "objectID": "5-causation/03-matching/notes.html#unobserved-confounding",
    "href": "5-causation/03-matching/notes.html#unobserved-confounding",
    "title": "Causal Effects in Observational Studies",
    "section": "Unobserved confounding",
    "text": "Unobserved confounding\nThere is at least one important difference between the matched studies discussed above and a natural experiment. In the matched study, we don’t have any compelling reason to believe that graduation from Cal was randomly assigned between siblings or matched subjects. It’s possible that the decision was random, but it’s also possible that some hidden factor drove the decision. For example, we don’t have any information about the career goals or interests of these students. If the Berkeley students all chose to attend because they love statistics and their siblings or matched controls all skipped college and spent their time practicing the guitar because they dream of becoming rock stars then maybe the real reason the former group is employed is because of their interest in a field with good job opportunities, not because of the college they attended. In the smoking twins study, it could be that the smoking twins pursued other kinds of risky behaviors (e.g. drug use, poor diet, working in dangerous jobs) at a higher rate than the non-smoking twins despite their genetic similarity. In either case, we can’t be sure whether any apparent effect we observe is due to our treatment or due to these hidden differences.\nUnless we happened to measure these covariates, the data does not help us much to refute these considerations. We have to rely on our judgment about the context to evaluate whether they are more plausible than the treatment effect. This situation is called unobserved confounding, and if it is present it can lead to substantially biased treatment effect estimates.\n\nUnobserved confounding\n\nWhen unmeasured covariates predictive of our outcome variable are present and imbalanced in our study.\n\n\nIn order to trust the causal claims arising from a matched observational study, we need to assume that there is no unobserved confounding.\nNotice that if we had been able to randomly assign college graduation or smoking status and had a large number of replicates, unobserved confounding would not be a concern. Because we know randomized treatment assignment is independent of everything about study subjects, whether observed or unobserved, we expect it to balance unobserved quantities on average and can ignore them. This is the biggest thing you give up when you don’t run an RCT."
  },
  {
    "objectID": "5-causation/03-matching/notes.html#the-ideas-in-code",
    "href": "5-causation/03-matching/notes.html#the-ideas-in-code",
    "title": "Causal Effects in Observational Studies",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nWe use the MatchIt package in R to conduct optimal matching using the Euclidean distance. The command matchit uses a formula argument much like lm to specify the treatment variable and the covariates on which to match. We also specify that we’re doing optimal matching and using a Euclidean distance. \n\nlibrary(MatchIt)\ngrad_matched &lt;- matchit(cal_grad ~ GPA + years_exp + rec, method = 'optimal',\n                        distance = 'euclidean',\n                        data = grad_job)\n\n\nCalling plot on objects created by matchit gives you diagnostics comparing the similarity of treated and control covariates before and after matching\n\nplot(grad_matched, type = 'density')\n\n\n\n\n\n\n\nWe can also pass matchit objects into the bal.test command and use the argument un = TRUE to create a Love plot with both pre- and post-matching covariate SMDs.\n\ngrad_matched |&gt;\n  bal.tab(s.d.denom = 'pooled', binary = 'std', un = TRUE) |&gt;\n  plot()\n\n\n\n\n\n\n\nSince after matching our comparison groups closely resemble what we might have seen in a randomized experiment, we report the sample difference in means as our estimate of the average treatment effect and conduct a hypothesis test as we would have in a randomized trial where we were able to assign Berkeley graduation status at random. To use the data only from the matched individuals, rather than everyone in grad_job, we start with the output object from the matching step and pipe it to the match.data command to obtain the smaller data frame.\n\nlibrary(infer)\ngrad_matched |&gt;\n   match.data() |&gt;\n   specify(response = good_job,\n          explanatory = cal_grad,\n          success = \"yes\") |&gt;\n  calculate(stat = \"diff in props\", order = c(\"TRUE\",\"FALSE\"))\n\nResponse: good_job (factor)\nExplanatory: cal_grad (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1   0.4\n\n\nWe estimate that graduation from Cal increases the probability of obtaining a good job by 0.4.\nSummary\nWe don’t need to give up on evaluating causal claims just because we are unable to assign treatments at random as in an experiment. In the best case, we can find natural experiments, or situations where nature or someone else assigns treatments essentially at random. Even if a natural experiment isn’t available, we can attempt to approximate one by finding similar individuals with different treatment status in the same data set by matching subjects on their covariates. When matched comparisons balance observe covariates well we can argue in favor of causal claims, although concerns about unobserved covariates (which are absent in randomized trials) remain."
  },
  {
    "objectID": "5-causation/03-matching/notes.html#footnotes",
    "href": "5-causation/03-matching/notes.html#footnotes",
    "title": "Causal Effects in Observational Studies",
    "section": "Footnotes",
    "text": "Footnotes\n\nImage credit Keith Allison, obtained from Wikimedia Commons.↩︎\nFrom Price, J., & Wolfers, J. (2010). Racial discrimination among NBA referees. , 125(4), 1859-1887.↩︎\nInterestingly, after Price’s and Wolfers’ study led to a back-and-forth debate with NBA statisticians and received significant media attention, they repeated their study on mid-2010s data and found that the effect had disappeared. Based on these results, they claimed that awareness of the potential for racial bias can cause evaluators to eliminate it from their process. You can read about their follow-up study in Pope, D. G., Price, J., & Wolfers, J. (2018). Awareness reduces racial bias. Management Science, 64(11), 4988-4995.↩︎\nThere is a rich literature that uses data bases of twins to determine genetic determinants of health outcomes. For a recent study on smoking and cancer, see Cancer in twin pairs discordant for smoking: The Nordic Twin Study of Cancer by Korhonen et al., 2022.↩︎\nMatching is conceptually related to prediction, since you can think of it as a way to predict the missing counterfactual outcome for an observed subject. However, compared to the problems we tackled in the prediction unit, we can’t as easily evaluate how good the predictions are, since we never get to see the counterfactual outcomes or calculate the equivalent of a test MSE.↩︎\nFor \\(k=2\\) the Euclidean distance is also the same notion of distance that we get from the Pythagorean theorem. The general formula extends this idea to many dimensions.↩︎\nNote that when we compute SMDs after matching, we use the same denominator as for the original SMD (based on the entire dataset). This ensures that the “before matching” and “after matching” SMDs use the same standard deviation measure as units, which allows us to compare them meaningfully.↩︎\noptimal matching may not always fix balance this effectively. Sometimes one of the treated units has no comparable control available, or not all variables end up balanced well. In these scenarios, modifications to the matching strategy can help, including eliminating some treated units or using a different kind of distance. Although we won’t discuss these modifications in detail, the MatchIt package in R supports many of them and provides a handy online guide: https://kosukeimai.github.io/MatchIt/articles/matching-methods.html.↩︎"
  },
  {
    "objectID": "5-causation/03-matching/ps.html",
    "href": "5-causation/03-matching/ps.html",
    "title": "Causal Effects in Observational Studies",
    "section": "",
    "text": "In 1992, New Jersey raised its minimum wage to $5.05 per hour, giving it the highest minimum wage in the country. To study the impact of the new policy on employment levels, economists David Card and Alan Krueger surveyed 410 fast-food restaurants in New Jersey and in the neighboring state of Pennsylvania (which did not change its minimum wage law)1. They collected a variety of covariates prior to the institution of the law, and then called back afterwards and collected information about the number of full-time workers employed.\nA version of their data is available in the dataframe fastfood1 in the stat20data package2. Note that columns labeled with a _2 suffix are those collected as part of the second callback (after the law went into effect).\n\nWhat is the causal question this study seeks to answer? Identify the treatment and control conditions.\n\n\n\n\n\n\nDo you think this study best considered a randomized experiment, a natural experiment, or an observational study requiring matching? Explain your reasoning.\n\n\n\n\n\n\nLoad the fastfood1 dataframe in R. Write code to create a new column giving effective full-time employment by adding the number of full time employees at each restaurant to half the number of part-time employees at each restaurant.\n\n\n\n\n\n\nExamine the covariate names in the fastfood1 dataframe and the first few values of each. Without looking more deeply at the data, sketch a Love plot for a few potentially important covariates showing the level of balance across treatment groups you expect to be present. Remember to label your axes.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a Love plot in R and sketch its actual appearance below. Compare it to your answer from the previous question and discuss any differences.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPick the variable with the largest absolute standardized mean difference and write code to conduct a hypothesis test to see if its imbalance differs significantly from zero.\n\n\n\n\n\n\n\n\n\nDo the answers to questions 6 and 7 support your answer to question 2, or do they suggest you should revise your answer? Explain.\n\n\n\n\n\nWrite code to conduct optimal matching and create a new version of the Love plot showing both pre-match and post-match balance.\n\n\n\n\n\n\n\n\n\nBased on your answer to question 2 (and any revisions to that answer in question 8), compute and report the associated effect estimate for effective full time employment. Write a sentence summarizing the associated causal claim.\n\n\n\n\n\n\n\nFor one of the study types that you did NOT select in question 2 or question 8, describe how a hypothetical study of minimum wage and fast food employment could be conducted using this strategy instead (expensive/impractical hypothetical approaches are acceptable for this question)."
  },
  {
    "objectID": "5-causation/03-matching/ps.html#case-study-minimum-wage-and-the-njpa-fast-food-industry",
    "href": "5-causation/03-matching/ps.html#case-study-minimum-wage-and-the-njpa-fast-food-industry",
    "title": "Causal Effects in Observational Studies",
    "section": "",
    "text": "In 1992, New Jersey raised its minimum wage to $5.05 per hour, giving it the highest minimum wage in the country. To study the impact of the new policy on employment levels, economists David Card and Alan Krueger surveyed 410 fast-food restaurants in New Jersey and in the neighboring state of Pennsylvania (which did not change its minimum wage law)1. They collected a variety of covariates prior to the institution of the law, and then called back afterwards and collected information about the number of full-time workers employed.\nA version of their data is available in the dataframe fastfood1 in the stat20data package2. Note that columns labeled with a _2 suffix are those collected as part of the second callback (after the law went into effect).\n\nWhat is the causal question this study seeks to answer? Identify the treatment and control conditions.\n\n\n\n\n\n\nDo you think this study best considered a randomized experiment, a natural experiment, or an observational study requiring matching? Explain your reasoning.\n\n\n\n\n\n\nLoad the fastfood1 dataframe in R. Write code to create a new column giving effective full-time employment by adding the number of full time employees at each restaurant to half the number of part-time employees at each restaurant.\n\n\n\n\n\n\nExamine the covariate names in the fastfood1 dataframe and the first few values of each. Without looking more deeply at the data, sketch a Love plot for a few potentially important covariates showing the level of balance across treatment groups you expect to be present. Remember to label your axes.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a Love plot in R and sketch its actual appearance below. Compare it to your answer from the previous question and discuss any differences.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPick the variable with the largest absolute standardized mean difference and write code to conduct a hypothesis test to see if its imbalance differs significantly from zero.\n\n\n\n\n\n\n\n\n\nDo the answers to questions 6 and 7 support your answer to question 2, or do they suggest you should revise your answer? Explain.\n\n\n\n\n\nWrite code to conduct optimal matching and create a new version of the Love plot showing both pre-match and post-match balance.\n\n\n\n\n\n\n\n\n\nBased on your answer to question 2 (and any revisions to that answer in question 8), compute and report the associated effect estimate for effective full time employment. Write a sentence summarizing the associated causal claim.\n\n\n\n\n\n\n\nFor one of the study types that you did NOT select in question 2 or question 8, describe how a hypothetical study of minimum wage and fast food employment could be conducted using this strategy instead (expensive/impractical hypothetical approaches are acceptable for this question)."
  },
  {
    "objectID": "5-causation/03-matching/ps.html#footnotes",
    "href": "5-causation/03-matching/ps.html#footnotes",
    "title": "Causal Effects in Observational Studies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCard, D., & Krueger, A. B. (1994). Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania. The American Economic Review, 84(4), 772-793.↩︎\n11 stores that were either permanently closed by the time of Card & Krueger’s second callback or refused to participate in the second callback have been removed from the data. Missing values have also been filled in with synthetic values for the purposes of this assignment.↩︎"
  },
  {
    "objectID": "docs/slides.html",
    "href": "docs/slides.html",
    "title": "Slides",
    "section": "",
    "text": "Be sure your personal fork of the course-materials is up to date by syncing it to the one on github.com/stat20.\nPull the most recent changes from your fork to your local machine.\nMake changes to slides.qmd and save.\n\n\n\n\nQuarto Pub is a free service for publishing quarto documents, including slides, online. Start by setting up an account at https://quartopub.com/.\n\nOnce you are set up with an account, in the terminal, navigate to the directory that contains your slides, for example: cd summarizing-numerical-data.\nOpen slides.qmd and, ensure that under revealjs: that you add the option self-contained: true. (read more here)\nIn that directory run:\n\n\n\nTerminal\n\nquarto publish quarto-pub slides.qmd\n\n\nYou may be prompted to answer a few questions at the terminal. If it successfully published, it will provide a link to your published document\nAt this point, you’re all set!\n\nIf you’d like to learn more about publishing with Quarto Pub, see the official documentation.\n\n\n\nTest it out by working in a much simpler directory than course-materials, one called practice-repo. Since you won’t be pushing any commits back up to GitHub, you don’t need a create your own fork.\n\nCreate a new project in Rstudio from a version control repository and paste in https://github.com/stat20/practice-repo for the url.\nModify slides.qmd in the summarizing-numerical-data directory by, say, replacing \"Instructor\" with your name in the header.\nIn the terminal, navigate into the sub-directory with the slides: cd summarizing-numerical-data\nStill at the terminal, publish your slides:\n\n\nTerminal\n\nquarto publish quarto-pub slides.qmd\n\nClick through any prompts you might see at the terminal.\nIf it publishes successfully, the terminal will print a link to your published document. Visit that site and check the link at the top that it says that it has been “Published at …”. It should appear as &lt;user_name&gt;.quarto.pub/summarizing-categorical-data/."
  },
  {
    "objectID": "docs/slides.html#modifying-your-own-slides",
    "href": "docs/slides.html#modifying-your-own-slides",
    "title": "Slides",
    "section": "",
    "text": "Be sure your personal fork of the course-materials is up to date by syncing it to the one on github.com/stat20.\nPull the most recent changes from your fork to your local machine.\nMake changes to slides.qmd and save."
  },
  {
    "objectID": "docs/slides.html#publishing-your-own-slides",
    "href": "docs/slides.html#publishing-your-own-slides",
    "title": "Slides",
    "section": "",
    "text": "Quarto Pub is a free service for publishing quarto documents, including slides, online. Start by setting up an account at https://quartopub.com/.\n\nOnce you are set up with an account, in the terminal, navigate to the directory that contains your slides, for example: cd summarizing-numerical-data.\nOpen slides.qmd and, ensure that under revealjs: that you add the option self-contained: true. (read more here)\nIn that directory run:\n\n\n\nTerminal\n\nquarto publish quarto-pub slides.qmd\n\n\nYou may be prompted to answer a few questions at the terminal. If it successfully published, it will provide a link to your published document\nAt this point, you’re all set!\n\nIf you’d like to learn more about publishing with Quarto Pub, see the official documentation."
  },
  {
    "objectID": "docs/slides.html#test-it-out",
    "href": "docs/slides.html#test-it-out",
    "title": "Slides",
    "section": "",
    "text": "Test it out by working in a much simpler directory than course-materials, one called practice-repo. Since you won’t be pushing any commits back up to GitHub, you don’t need a create your own fork.\n\nCreate a new project in Rstudio from a version control repository and paste in https://github.com/stat20/practice-repo for the url.\nModify slides.qmd in the summarizing-numerical-data directory by, say, replacing \"Instructor\" with your name in the header.\nIn the terminal, navigate into the sub-directory with the slides: cd summarizing-numerical-data\nStill at the terminal, publish your slides:\n\n\nTerminal\n\nquarto publish quarto-pub slides.qmd\n\nClick through any prompts you might see at the terminal.\nIf it publishes successfully, the terminal will print a link to your published document. Visit that site and check the link at the top that it says that it has been “Published at …”. It should appear as &lt;user_name&gt;.quarto.pub/summarizing-categorical-data/."
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Documentation",
    "section": "",
    "text": "Welcome to the technical documentation of the Stat 20 curriculum and website."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html",
    "title": "The Taxonomy of Data",
    "section": "",
    "text": "In the beginning was data, and from that data was built an understanding of the world.\nIn the beginning was understanding, and from that understanding sprung questions that sought to be answered with data.\nSo, which is it?\nThis is a philosophical question and it is up for debate. What is clearer is that in the process of engaging in data science, you will inevitably find yourself at one of these beginnings, puzzling over how to make your way to the other one.\nThe defining element of data science is the centrality of data as the means of advancing our understanding of the world. The word “data” is used in many different ways, so let’s write down a definition to get everyone on the same page.\nThis broad definition permits a staggering diversity in the forms that data can take. When you conducted a chemistry experiment in high school and recorded your measurements in a table in a lab notebook, that was data. When you registered for this class and your name showed on CalCentral, that was data. When the James Webb Space Telescope took a photo of the distant reaches of our solar system, recording levels of light pixel-by-pixel, that was data.\nSuch diversity in data is more precisely described as diversity in the types of variables that are being measured in a data set.\nIn your chemistry notebook you may have recorded the temperature and pressure of a unit of gas, two variables that are of scientific interest. In the CalCentral data set, name is the variable that was recorded (on you!) but you can imagine other variables that the registrars office might have recorded: your year at Cal, your major, etc. Each of these are called variables because the value that is measured generally varies as you move from one object to the next. While your value of the name variable might be Penelope, if we record the same variable on another student we’ll likely come up with different value."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#a-taxonomy-of-data",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#a-taxonomy-of-data",
    "title": "The Taxonomy of Data",
    "section": "A Taxonomy of Data",
    "text": "A Taxonomy of Data\nWhile the range of variables that we can conceive of is innumerable, there are recurring patterns in those variables that allow us to group them into persistent types that have shared properties. Such a practice of classification results in a taxonomy, which has been applied most notably in evolutionary biology to classify all forms of life.\nWithin the realm of data, an analogous taxonomy has emerged.\n\n\n\n\n\nFigure 2: the Taxonomy of Data.\n\n\nTypes of Variables\nThe principle quality of a variable is whether it is numerical or categorical.\n\n\nNumerical Variable\n\nA variable that take numbers as values and where the magnitude of the number has a quantitative meaning.\n\n\n\n\n\nCategorical Variable\n\nA variable that take categories as values. Each unique category is called a level.\n\n\n\nWhen most people think “data” they tend to think about numerical variables (like the temperature and pressure recorded in your lab notebook) but categorical variables (like the name recorded on CalCentral) are very common.\nAll numerical variables can be classified as either continuous or discrete.\n\n\nContinuous Numerical Variable\n\nA numerical variable that takes values on an interval of the real number line.\n\n\n\n\n\nDiscrete Numerical Variable\n\nA numerical variable that takes values that have jumps between them.\n\n\n\nA good example of a continuous numerical variable is temperature. If we are measuring outside air temperature on Earth in Fahrenheit, it is possible that we would record values anywhere from around -125 degrees F and +135 degrees F. While we might end up rounding our measurement to the nearest integer degree, we can imagine that the phenomenon of temperature itself varies smoothly and continuously across this range.\nA good example of a discrete numerical variable is household size. When the US Census goes door-to-door every year collecting data on every household, they record the number of people living in that household. A household can have 1 person, or 2 people, or 3 people, or 4 people, and so on, but it cannot have 2.83944 people. This makes it discrete.\nWhat unites both types of numerical variables is that the magnitude of the numbers have meaning and you can perform mathematical operations on them and the result also has meaning. It is possible and meaningful to talk about the average air temperature across three locations. It is also possible and meaningful to talk about the sum total number of people across ten households.\nThe ability to perform mathematical operations drops away when we move to ordinal variables. All categorical variables can be classified as either ordinal or nominal.\n\n\nOrdinal Categorical Variable\n\nA categorical variable with levels that have a natural ordering.\n\n\n\n\n\nNominal Categorical Variable\n\nA categorical variable with levels with no ordering.\n\n\n\nYou have likely come across ordinal categorical variables if you have taken an opinion survey. Consider the question:“Do you strongly agree, agree, feel neutral about, disagree, or strongly disagree with the following statement: Dogs are better than cats?” When you record answers to this question, you’re recording measurements on a categorical variable that takes values “strongly agree”, “agree”, “neutral”, “disagree”, “strongly disagree”. Those are the levels of the categorical variable and they have a natural ordering: “strongly agree” is closer to “agree” than it is to “strongly disagree”.\nYou can contrast this with a nominal categorical variable. Consider a second question that asks (as the registrar does): “What is your name?” There are many more possible levels in this case - “Penelope”, “David”, “Shobhana”, etc. - but those levels have no natural ordering. In fact this is very appropriate example of a nominal variable because the word itself derives from the Latin nomen, or “name”.\nLet’s take a look at a real data set to see if we can identify the variables and their types.\nExample: Palmer Penguins\nDr. Kristen Gorman is a fisheries and wildlife ecologist at the University of Alaska, Fairbanks whose work brought her to Palmer Station, a scientific research station run by the National Science Foundation in Antarctica. At Palmer Station, she took part in a long-term study to build an understanding of the breeding ecology and population structure of penguins.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Dr. Gorman recording measurements on penguins and Palmer Station, a research station in Antarctica.\n\n\nIn order to build her understanding of this community of penguins, she and fellow scientists spent time in the field recording measurements on a range of variables that capture important physical characteristics.\n\n\n\nTwo of the variables that were recorded were bill length and bill depth1. Each of these capture a dimension of the bill of a penguin recorded in millimeters These are identifiable as continuous numerical variables. They’re numerical because the values have quantitative meaning and they’re continuous because bill sizes don’t come in fixed, standard increments. They vary continuously.\nAnother variable that was recorded was the species of the penguin, either “Adelie”, “Gentoo”, or “Chinstrap”. Because these values are categories, this is a categorical variable. More specifically, it’s a nominal categorical because there is no obvious natural ordering between these three species.\n\n\n\nThese are just three of many variables that recorded in the penguins data set and published along their scientific findings in the paper, Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis)2. We will return throughout this course to this data set and this study. It is a prime example of how careful data collection and careful scientific reasoning can expand our understanding of a corner of our world about which we know very little.\nWhy Types Matter\nThe Taxonomy of Data is a useful tool of statistics and data science because it helps guide the manner in which data is recorded, visualized, and analyzed. Many confusing plots have been made by not thinking carefully about whether a categorical variable is ordinal or not or by mistaking a continuous numerical variable for a categorical variable. You will get plenty of practice using this taxonomy to guide your data visualization in the next unit.\nLike many tools built by scientists, though, this taxonomy isn’t perfect. There are many variables that don’t quite seem to fit into the taxonomy or that you can argue should fit into multiple types. That’s usually a sign that something interesting is afoot and is all the more reason to think carefully about the nature of the variables and the values it might take before diving into your analysis."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#a-structure-for-data-the-data-frame",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#a-structure-for-data-the-data-frame",
    "title": "The Taxonomy of Data",
    "section": "A Structure for Data: The Data Frame",
    "text": "A Structure for Data: The Data Frame\nWhen we seek to grow our understanding of a phenomenon, sometimes we select a single variable that we go out and collect data on. More often, we’re dealing with more complex phenomenon that are characterized by a few, or a few dozen, or hundreds (or even millions!) of variables. CalCentral has far more than just your name on file. To capture all of the complexity of class registration at Cal, it is necessary to record dozens of variables.\nTo keep all of this data organized, we need a structure. While there are several different ways to structure a given data set, the format that has become most central to data science is the data frame.\n\n\nData Frame\n\nAn array that associates the observations (downs the rows) with the variables measured on each observation (across the columns). Each cell stores a value observed for a variable on an observation.\n\n\n\nWhile this definition might seem opaque, you are already familiar with a data frame. You are you just more accustomed to seeing it laid out this like this:\n\n\n\n\nbill_length_mm\nbill_depth_mm\nspecies\n\n\n\n43.5\n18.1\nChinstrap\n\n\n48.1\n15.1\nGentoo\n\n\n49.0\n19.5\nChinstrap\n\n\n45.4\n18.7\nChinstrap\n\n\n34.6\n21.1\nAdelie\n\n\n49.8\n17.3\nChinstrap\n\n\n40.9\n18.9\nAdelie\n\n\n45.3\n13.7\nGentoo\n\n\n\n\n\nYou might be accustomed to calling this a “spreadsheet” or a “table”, but the organizational norm of putting the variables down the columns and the observations across the rows make this a more specific structure.\nOne of the first questions that you should address when you first come across a data frame is to determine what the unit of observation is.\n\n\nUnit of Observation\n\nThe class of object on which the variables are observed.\n\n\n\nIn the case of data frame above, the unit of observation is a single penguin near Palmer Station. The first row captures the measurements on the first penguin, the second row captures the measurements of the second penguin, and so on. If I log into CalCentral to see the data frame that records information on the students enrolled in this class, the unit of observation is a single student enrolled in this class.\nNot a Data Frame\nBefore you leave thinking that “data frame” = “spreadsheet”, consider this data set3:\n\n\n\n\n\nFor it to be a data frame, we would have to read across the columns and see the names of the variables. You can imagine recording whether or not someone is right-handed or left-handed, but those variables would take the values “yes” and “no”, not the counts that we see here. Furthermore, total is not a variable that we’ve recorded a single unit; this column captures aggregate properties of the whole data set.\nWhile this structure might well be called a “table” or possibly a “spreadsheet”, it doesn’t meet our definition for a data frame."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#the-ideas-in-code",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#the-ideas-in-code",
    "title": "The Taxonomy of Data",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nThe concepts of a variable, its type, and the structure of a data frame are useful because they help guide our thinking about the nature of a data. But we need more than definitions. If our goal is to construct a claim with data, we need a tool to aid in the construction. Our tool must be able to do two things: it must be able to store the data and it must be able to perform computations on the data. This is where R comes in!\nFirst, we will discuss how R can store and perform computations on data. Then, we will relate these basics to the Taxonomy of Data we have just discussed."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#saving-objects",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#saving-objects",
    "title": "The Taxonomy of Data",
    "section": "Saving Objects",
    "text": "Saving Objects\nWhenever you want to save the output of an R command, add an assignment arrow &lt;- (less than, minus) as well as a name, such as “answer” to the left of the command.\n\nanswer &lt;- 2 ^ (3 + 1)\n\nWhen you run this command, there are two things to notice.\n\nThe word answer appears in the upper right hand corner of RStudio, in the “Environment” tab.\nNo output is returned at the console.\n\nEvery time you run a command, you can ask yourself: do I want to just see the output at the console or do I want to save it for later? If the latter, you can always see the contents of what you saved by just typing its name at the console and pressing Enter.\n\nanswer\n\n[1] 16\n\n\nThere are a few rules around the names that R will allow for the objects that you’re saving. First, while all letters are fair game, special characters like +, -, /, !, $, are off-limits. Second, names can contain numbers, but not as the first character. That means names like answer, a, a12, my_pony, and FOO will all work. 12a and my_pony! will not.\nBut just because I’ve told you that those names won’t work doesn’t mean you shouldn’t give it a try…\n\nmy_pony! &lt;- 2 ^ (3 + 1)\n\nError: &lt;text&gt;:1:8: unexpected '!'\n1: my_pony!\n           ^\n\n\nThis is an example of an error message and, though they can be alarming, they’re also helpful in coaching you how to correct your code. Here, it’s telling you that you had an “unexpected !” and then it points out where in your code that character popped up."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#creating-vectors",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#creating-vectors",
    "title": "The Taxonomy of Data",
    "section": "Creating Vectors",
    "text": "Creating Vectors\nWhile it is helpful to be able to store a single number as an R object, to store data sets we’ll need to store a series of numbers. You can combine multiple values by putting them inside c() separated by commas.\n\nmy_fav_numbers &lt;- c(9, 11, 19, 28)\nmy_fav_numbers\n\n[1]  9 11 19 28\n\n\nThis is object is called a vector.\n\nVector (in R)\n\nA set of contiguous data values that are of the same type.\n\n\nAs the definition suggests, you can create vectors out of many different types of data. To store words as data, use the following:\n\nmy_fav_colors &lt;- c(\"green\", \"orange\", \"purple\")\nmy_fav_colors\n\n[1] \"green\"  \"orange\" \"purple\"\n\n\nAs this example shows, R can store more than just numbers as data. \"green\", \"orange“, and \"purple\" are each called character strings and when combined together with c() they form a character vector. You can identify a string because it is wrapped in quotation marks and gets highlighted a different color in RStudio.\nVectors are often called atomic vectors because, like atoms, they are the simplest building blocks in the R language. Most of the objects in R are, at the end of the day, constructed from a series of vectors."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#functions",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#functions",
    "title": "The Taxonomy of Data",
    "section": "Functions",
    "text": "Functions\nWhile the vector will serve as our atomic method of storing data in R, how do we perform computations on it? That is the role of functions.\nLet’s use a function to find the arithmetic mean of the vector my_fav_numbers.\n\nmean(my_fav_numbers)\n\n[1] 16.75\n\n\nA function in R operates in a very similar manner to functions that you’re familiar with from mathematics.\n\n\n\n\n\nFigure 4: A mathematical function as a box with inputs and outputs.\n\n\nIn math, you can think of a function, \\(f()\\) as a black box that takes the input, \\(x\\), and transforms it to the output, \\(y\\). You can think of R functions in a very similar way. For our example above, we have:\n\n\nInput: the vector of four numbers that serves as the input to the function, my_fav_numbers.\n\nFunction: the function name, mean, followed by parentheses.\n\nOutput: the number 16.75."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#functions-on-vectors",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#functions-on-vectors",
    "title": "The Taxonomy of Data",
    "section": "Functions on Vectors",
    "text": "Functions on Vectors\nmean() is just one of thousands of different functions that are available in R. Most of them are sensibly named, like the following, which compute square roots and natural logarithms.\n\n\nBy default, log() computes the natural log. To use other bases, see ?log.\n\nsqrt(my_fav_numbers)\n\n[1] 3.000000 3.316625 4.358899 5.291503\n\nlog(my_fav_numbers)\n\n[1] 2.197225 2.397895 2.944439 3.332205\n\n\nNote that with these two functions, the input was a vector of length four and the output is a vector of length four. This is a distinctive aspect of the R language and it is helpful because it allows you to perform many separate operations (taking the square root of four numbers, one by one) with just a single command."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#the-taxonomy-of-data-in-r",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#the-taxonomy-of-data-in-r",
    "title": "The Taxonomy of Data",
    "section": "The Taxonomy of Data in R",
    "text": "The Taxonomy of Data in R\nIn the last lecture notes, we introduced the Taxonomy of Data as a broad system to classify the different types of variables on which we can collect data. If you recall, a variable is a characteristic of an object that you can measure and record. When Dr. Gorman walked up to her first penguin (the unit of observation) and measured its bill length, she collected a single observation of the variable bill_length_mm. You could record that in R using,\n\nbill_length_mm &lt;- 50.7\n\nShe continued on to measure the next penguin, then the next, then the next… Instead of recording these as separate objects, it is more efficient to store them as a vector.\n\nbill_length_mm &lt;- c(50.7, 48.5, 52.8, 44.5, 42.0, 46.9, 50.2, 37.9)\n\nThis example shows that\n\nA vector in R is a natural way to store observations on a variable.\n\nso in the same way that we have asked, “what is the type of that variable?” we can now ask “what is the class of that variable in R?”.\n\nClass (R)\n\nA collection of objects, often vectors, that share similar attributes and behaviors.\n\n\nWhile there are many classes in R, you can get a long way only knowing three. The first is represented by our vector my_fav_numbers. Let’s check it’s class using the class() function.\n\nclass(my_fav_numbers)\n\n[1] \"numeric\"\n\n\nHere we learn that my_fav_numbers is a numeric vector. Numeric vectors, as the name suggests, are composed only of numbers and can include measurements from both discrete and continuous numerical variables.\nWhat about my_fav_colors?\n\nclass(my_fav_colors)\n\n[1] \"character\"\n\n\nR stores that as a character vector. This is a very flexible class that can be used to store text as data. But what if there are only a few fixed values that a variable can take? In that case, you can do better than a character vector by usinggit a factor. Factor is a very useful class in R because it encodes the notion of levels discussed in the last notes.\nTo illustrate the difference, let’s make a character vector but then enrich it by turning it into a factor using factor().\n\nchar_vec &lt;- c(\"cat\", \"cat\", \"dog\")\nfac &lt;- factor(char_vec)\nchar_vec\n\n[1] \"cat\" \"cat\" \"dog\"\n\nfac\n\n[1] cat cat dog\nLevels: cat dog\n\n\nThe original character vector stores the same three strings that we used as input. The factor adds some additional information: the possible values that this vector can take.\nThis is particularly useful when you want to let R know that these levels have a natural ordering. If you have strong opinions about the relative merit of dogs over cats, you could specify that using:\n\nordered_fac &lt;- factor(char_vec, levels = c(\"dog\", \"cat\"))\nordered_fac\n\n[1] cat cat dog\nLevels: dog cat\n\n\n\n\nThis example also demonstrates that you can create a (character) vector inside a function.\nWhile this doesn’t change the way the levels are ordered in the vector itself, it will effect the way they behave when we use them to create plots, as we’ll do in the next set of notes.\nThese three vector classes do a good job of putting into flesh and bone (or at least silicon) the abstract types captured in the Taxonomy of Data.\n\n\n\n\n\nFigure 5: The Taxonomy of Data with equivalent classes in R."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#data-frames-in-r",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#data-frames-in-r",
    "title": "The Taxonomy of Data",
    "section": "Data Frames in R",
    "text": "Data Frames in R\nWhile vectors in R do a great job of capturing the notion of a variable, we will need more than that if we’re going to represent something like a data frame. Conveniently enough, R has a structure well-suited to this task called…(drumroll…)\n\nDataframe (R)\n\nA two dimensional data structure used to store vectors of the same length. A direct analog of the data frame defined previously4.\n\n\nLet’s use R to recreate the penguins data frame collected by Dr. Gorman.\n\n\n\n\nbill_length_mm\nbill_depth_mm\nspecies\n\n\n\n43.5\n18.1\nChinstrap\n\n\n48.1\n15.1\nGentoo\n\n\n49.0\n19.5\nChinstrap\n\n\n45.4\n18.7\nChinstrap\n\n\n34.6\n21.1\nAdelie\n\n\n49.8\n17.3\nChinstrap\n\n\n40.9\n18.9\nAdelie\n\n\n45.3\n13.7\nGentoo\n\n\n\n\n\nCreating a data frame\nIn the data frame above, there are three variables; the first two numeric continuous, the last one categorical nominal. Since R stores variables as vectors, we’ll need to create three vectors.\n\nbill_length_mm &lt;- c(50.7, 48.5, 52.8, 44.5, 42.0, 46.9, 50.2, 37.9)\nbill_depth_mm &lt;- c(19.7, 15.0, 20.0, 15.7, 20.2, 16.6, 18.7, 18.6)\nspecies &lt;- factor(c(\"Chinstrap\", \"Gentoo\", \"Chinstrap\", \"Gentoo\", \"Adelie\", \n             \"Chinstrap\", \"Chinstrap\", \"Adelie\"))\n\n\n\nCheck the class of these vectors by using the as input to class().\nWhile bill_length_mm and bill_depth_mm are both being stored as numeric vectors, species was first collected into a character vector, then passed directly to the factor() function. This is an example of nesting one function inside of another and it combined two lines of code into one.\nWith the three vectors stored in the Environment, all you need to do is staple them together with data.frame().\n\npenguins_df &lt;- data.frame(bill_length_mm, bill_depth_mm, species)\npenguins_df\n\n  bill_length_mm bill_depth_mm   species\n1           50.7          19.7 Chinstrap\n2           48.5          15.0    Gentoo\n3           52.8          20.0 Chinstrap\n4           44.5          15.7    Gentoo\n5           42.0          20.2    Adelie\n6           46.9          16.6 Chinstrap\n7           50.2          18.7 Chinstrap\n8           37.9          18.6    Adelie"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#summary",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#summary",
    "title": "The Taxonomy of Data",
    "section": "Summary",
    "text": "Summary\nIn this lecture note we have focused on the nature of the data that will serve as the currency from which we’ll construct an improved understanding of the world. A first step is to identify the characteristics of the variables that are being measured and determine their type within the Taxonomy of Data. A second step is to organize them into a data frame to clearly associate the value that is measured for a variable with a particular observational unit.\nWith these ideas in hand, we learned how to bring data onto our computer, so that in our next class, we can begin the process of identifying its structure and communicating that structure numerically and visually."
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/notes.html#footnotes",
    "href": "1-questions-and-data/02-taxonomy-of-data/notes.html#footnotes",
    "title": "The Taxonomy of Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nPenguin artwork by @allison_horst.↩︎\nGorman KB, Williams TD, Fraser WR (2014). Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis). PLoS ONE 9(3):e90081. https://doi.org/10.1371/journal.pone.0090081↩︎\nAn image from the Wikipedia article on contingency tables, https://en.wikipedia.org/wiki/Contingency_table.↩︎\nR is an unusual language in that the data frame has been for decades a core structure of the language. The analogous structure in Python is the data frame found in the Pandas library.↩︎"
  },
  {
    "objectID": "1-questions-and-data/02-taxonomy-of-data/ps.html",
    "href": "1-questions-and-data/02-taxonomy-of-data/ps.html",
    "title": "Taxonomy of Data",
    "section": "",
    "text": "What type of variable is being measured by each of the following questions?\n\n\n\nLast class you collected data on six different variables on your classmates. List the data types of each of those variables, the units that they were measured in, and the range of possible values that you would expect them to take.\n\n\nName:\nContact info:\nHometown:\n# Siblings + Pets:\n# Semesters in college:\nFarthest distance from campus during the break:\n\n\n\n\n\n\nOn the back of this page, sketch out a data frame that contains the data from your group mates you collected last class on the questions above. Introduce yourself to other nearby groups to extend your data frame until it has 6 rows.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs discussed in the lecture notes, this is not a data frame:\n\n\n\n\n\n\nIt does, however, depict a data set, just in a different format. Sketch the data that is summarized here but structured as a data frame. Think through: what was the unit of observation? What were the variables? How many rows are there? How many columns? (you need not fill out the entire data frame; just a schematic)"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/notes.html",
    "href": "1-questions-and-data/01-understanding-the-world/notes.html",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "Welcome to Stat 20! We are very excited to have you here this semester. There are no reading questions for today’s content, but make sure that you have:\n\ngotten the name of your instructor and in-class tutors\nread the syllabus and asked any questions you have about it on your lecture’s corresponding Ed thread\ntaken a look at the first problem set.\n\nThe goal of our course is to construct and critique claims made using data. This raises the question: what type of claims can be made? Four of the five units in this course will center around a specific type of claim (the other unit being probability.) We provide definitions for each of the claims below!"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/notes.html#intro-and-syllabus",
    "href": "1-questions-and-data/01-understanding-the-world/notes.html#intro-and-syllabus",
    "title": "Understanding the World with Data",
    "section": "",
    "text": "Welcome to Stat 20! We are very excited to have you here this semester. There are no reading questions for today’s content, but make sure that you have:\n\ngotten the name of your instructor and in-class tutors\nread the syllabus and asked any questions you have about it on your lecture’s corresponding Ed thread\ntaken a look at the first problem set.\n\nThe goal of our course is to construct and critique claims made using data. This raises the question: what type of claims can be made? Four of the five units in this course will center around a specific type of claim (the other unit being probability.) We provide definitions for each of the claims below!"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/notes.html#types-of-claims",
    "href": "1-questions-and-data/01-understanding-the-world/notes.html#types-of-claims",
    "title": "Understanding the World with Data",
    "section": "Types of Claims",
    "text": "Types of Claims\n\n\n\n\n\n\nSummary\n\nA numerical, graphical, or verbal description of an aspect of data that is on hand.\n\n\n\nExample: Using data from the Stat 20 class survey, the proportion of respondents to the survey who reported having no experience writing computer code is 70%.\n\n\nGeneralization\n\nA numerical, graphical, or verbal description of a broader set of units than those on which data was been recorded.\n\n\n\n\nExample: Using data from the Stat 20 class survey, the proportion of Berkeley students who have no experience writing computer code is 70%.\n\n\nCausal Claim\n\nA claim that changing the value of one variable will influence the value of another variable.\n\n\n\nExample: Data from a randomized controlled experiment shows that taking a new antibiotic eliminates more than 99% of bacterial infections.\n\n\nPrediction\n\nA guess about the value of an unknown variable, based on other known variables.\n\n\n\nExample: Based on reading the news and the price of Uber’s stock today, I predict that Uber’s stock price will go up 1.2% tomorrow."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/notes.html#intro-to-r-and-rstudio",
    "href": "1-questions-and-data/01-understanding-the-world/notes.html#intro-to-r-and-rstudio",
    "title": "Understanding the World with Data",
    "section": "Intro to R and RStudio",
    "text": "Intro to R and RStudio\nIf our goal is to construct a claim with data, we need a tool to aid in the construction. Our tool must be able to do two things: it must be able to store the data and it must be able to perform computations on the data.\nIn high school, you gained experience with one such tool: the graphing calculator. This fits our needs: you can enter a list of number into a graphing calculator like the Ti-84, it can store that list and it can execute computations on it, such as taking its sum. But the types of data that a calculator can store are very limited, as is the volume of data, as are the options for computation.\nIn this class, we will use a tool that is far more powerful: the computer language called R. The Ti-84 is to R what a tricycle is to the space ship. One of these tools can bring you to the end of the block; the other to the moon.\nR is one of the most powerful languages for doing statistics and data science. One of the reasons for its power and popularity is that it is both free and open-source. This turns languages like R into something that resembles Wikipedia: a collaborative effort that is constantly evolving. Extensions to the R language have been authored by professional programmers1, people working in industry and government2, professors3, and students like you4.\nYou’ll be writing and running code through an app called RStudio. Beyond writing R code, RStudio allows you to manage your files and author polished documents that weave together code and text. RStudio can be run through a browser and we have set up an account for you that you can access by sending a browser tab to https://stat20.datahub.berkeley.edu/ or clicking the  link in the upper right corner of the course website.\nWhen you log into RStudio, the place where you can type and run R code is called the console and it’s located right here:\n\n\n\n\n\nFigure 1: The R console in RStudio.\n\n\n\n\n\n\n\n\nCode along\n\n\n\nAs you read through these notes, keep RStudio open in another window to code along at the console."
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/notes.html#r-as-a-calculator",
    "href": "1-questions-and-data/01-understanding-the-world/notes.html#r-as-a-calculator",
    "title": "Understanding the World with Data",
    "section": "R as a Calculator",
    "text": "R as a Calculator\nAlthough R is like a space ship capable of going to the moon, it’s also more than able to go to the end of the block. Type the sum 1 + 2 into the console (the area to the right of the &gt;) and press Enter. What you should see is this:\n\n1 + 2\n\n[1] 3\n\n\nAll of the arithmetic operations work in R.\n\n1 - 2\n\n[1] -1\n\n1 * 2\n\n[1] 2\n\n1 / 2\n\n[1] 0.5\n\n\nEach of these four lines of code is called a command and the response from R is the output. The [1] at the beginning of the output is there just to indicate that it is the first element of the output. This helps you keep track of things when the output spans many lines.\nAlthough it is easiest to read code when the numbers are separated from the operator by a single space, it’s not necessary. R ignores all spaces when it runs your code, so each of the following also work.\n\n1/2\n\n[1] 0.5\n\n1   /         2\n\n[1] 0.5\n\n\nYou can add exponents by using ^, but don’t forget about the order of operations. If you want an alternative ordering, use parentheses.\n\n2 ^ 3 + 1\n\n[1] 9\n\n2 ^ (3 + 1)\n\n[1] 16"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/notes.html#footnotes",
    "href": "1-questions-and-data/01-understanding-the-world/notes.html#footnotes",
    "title": "Understanding the World with Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe googlesheets4 package, which reads spreadsheet data into R was authored by Jenny Bryan, a developer at Posit: :https://googlesheets4.tidyverse.org/.↩︎\nThe statistics office of the province of British Columbia maintains a public R package with all of their data: https://bcgov.github.io/bcdata/↩︎\nDr. Christopher Paciorek in the Department of Statistics at UC Berkeley maintains a package to fit a very broad class of statistical models called Bayesian Models: https://r-nimble.org/.↩︎\nSimon Couch wrote the stacks package for model ensembling while an undergraduate https://stacks.tidymodels.org/index.html.↩︎"
  },
  {
    "objectID": "1-questions-and-data/01-understanding-the-world/worksheet.html",
    "href": "1-questions-and-data/01-understanding-the-world/worksheet.html",
    "title": "Stat 20",
    "section": "",
    "text": "Name:\n\nContact info:\n\nHometown:\n\n# Siblings + Pets\n\n# Semesters in college:\n\nFarthest distance from campus over the break:\n\n\nName:\n\nContact info:\n\nHometown:\n\n# Siblings + Pets\n\n# Semesters in college:\n\nFarthest distance from campus over the break:"
  },
  {
    "objectID": "6-prediction/labs/08-cancer/lab.html",
    "href": "6-prediction/labs/08-cancer/lab.html",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "6-prediction/labs/08-cancer/lab.html#part-i-understanding-the-context-of-the-data",
    "href": "6-prediction/labs/08-cancer/lab.html#part-i-understanding-the-context-of-the-data",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "Part I: Understanding the Context of the Data",
    "text": "Part I: Understanding the Context of the Data\n\nLab 6.1: Cancer Diagnosis"
  },
  {
    "objectID": "6-prediction/labs/08-cancer/lab.html#part-ii-computing-on-the-data",
    "href": "6-prediction/labs/08-cancer/lab.html#part-ii-computing-on-the-data",
    "title": "Lab 6: Diagnosing Cancer",
    "section": "Part II: Computing on the Data",
    "text": "Part II: Computing on the Data\nYou can load in the biopsies data frame using the code below:\n\nlibrary(tidyverse)\nbiopsies &lt;- \n  read_csv(\"https://www.dropbox.com/s/0rbzonyrzramdgl/cells.csv?dl=1\") |&gt;\n  mutate(diagnosis = factor(diagnosis, levels = c(\"B\", \"M\")))\n\nThe diagnosis is in the column named diagnosis; each other column should be used to predict the diagnosis.\n\nMake a single plot that examines the association between radius_mean and radius_sd separately for each diagnosis (hint: aes() should have three arguments).\nCalculate the correlation between these two variables for each diagnosis.\nGive at least a two-sentence interpretation of the results in the last two questions. In particular, comment on:\n\n\nIs the relationship between radius_mean and radius_sd different for benign biopsies vs. malignant biopsies?\nIf so, can you give an explanation for this difference?\n\n\nSplit the data set into a roughly 80-20 train-test set split.\nUsing the training data, fit a simple logistic regression model that predicts the diagnosis using the mean of the texture index.\nUsing a threshold of .5, What would your model predict for a biopsy with a mean texture of 15? What probability does it assign to that outcome?\nCalculate and report two misclassification rates for your simple model: first on the training data and then on the testing data.\nBuild a more complex model to predict the diagnosis using five predictors of your choosing.\nCalculate and report two misclassification rates for your complex model: first on the training data and then on the testing data.\nIs there any evidence that your model is overfitting? Explain in at least two sentences.\nMove back to your simple model for the next few questions.Report the total number of false negatives in the test data set.\nWhat can you change about your classification rule to lower the number of false negatives?\nMake the change you identified in the previous question and calculate the new number of false negatives.\nCalculate the testing misclassification rate using your new classification rule.\nDid your misclassification rule go up or down? Answer this question and explain why it went up or down in at least two sentences."
  },
  {
    "objectID": "6-prediction/labs/07-baseball/slides.html",
    "href": "6-prediction/labs/07-baseball/slides.html",
    "title": "Lab 5: Multiple Regression with Baseball",
    "section": "",
    "text": "Key terms to define here are Runs (R) and Wins (W). Here’s a helpful glossary from MLB: https://www.mlb.com/glossary"
  },
  {
    "objectID": "6-prediction/labs/07-baseball/slides.html#baseball-rules",
    "href": "6-prediction/labs/07-baseball/slides.html#baseball-rules",
    "title": "Lab 5: Multiple Regression with Baseball",
    "section": "",
    "text": "Key terms to define here are Runs (R) and Wins (W). Here’s a helpful glossary from MLB: https://www.mlb.com/glossary"
  },
  {
    "objectID": "6-prediction/labs/07-baseball/slides.html#baseball-rules-1",
    "href": "6-prediction/labs/07-baseball/slides.html#baseball-rules-1",
    "title": "Lab 5: Multiple Regression with Baseball",
    "section": "Baseball Rules",
    "text": "Baseball Rules"
  },
  {
    "objectID": "6-prediction/labs/07-baseball/slides.html#sabermetrics",
    "href": "6-prediction/labs/07-baseball/slides.html#sabermetrics",
    "title": "Lab 5: Multiple Regression with Baseball",
    "section": "Sabermetrics",
    "text": "Sabermetrics\nCoined by Bill James in 1980, sabermetrics is\n\n“the search for objective knowledge about baseball.”"
  },
  {
    "objectID": "6-prediction/labs/07-baseball/slides.html#history-of-sabermetrics",
    "href": "6-prediction/labs/07-baseball/slides.html#history-of-sabermetrics",
    "title": "Lab 5: Multiple Regression with Baseball",
    "section": "History of Sabermetrics",
    "text": "History of Sabermetrics\n\nHenry Chadwick, a NY sportswriter, developed the box score in 1859\n“Percentage Baseball” by Earnshaw Cook in 1964\nThe Bill James Baseball Abstract, annual book beginning in 1977\n\nMoneyball"
  },
  {
    "objectID": "6-prediction/labs/07-baseball/lab-context.html",
    "href": "6-prediction/labs/07-baseball/lab-context.html",
    "title": "Baseball",
    "section": "",
    "text": "One source of data for this lab is the public Lahman database which contains a number of data sets with different units of observation. Below are the first few rows and some of the columns for two of these data sets: Teams and Batting. They contain data going back to 1871. Use these excerpts to help you answer the following questions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyearID\nteamID\nfranchID\nG\nW\nL\nR\nRA\nname\n\n\n\n2014\nNYA\nNYY\n162\n84\n78\n633\n664\nNew York Yankees\n\n\n2013\nWAS\nWSN\n162\n86\n76\n656\n626\nWashington Nationals\n\n\n1997\nNYA\nNYY\n162\n96\n66\n891\n688\nNew York Yankees\n\n\n1981\nPIT\nPIT\n103\n46\n56\n407\n425\nPittsburgh Pirates\n\n\n1931\nNYA\nNYY\n155\n94\n59\n1067\n760\nNew York Yankees\n\n\n1928\nBSN\nATL\n153\n50\n103\n631\n878\nBoston Braves\n\n\n\n\n\n\n\nplayerID\nyearID\nteamID\nG\nAB\nR\nH\nBB\nSO\n\n\n\nhammeja01\n2016\nCHN\n35\n65\n6\n16\n1\n24\n\n\nmcewijo01\n2002\nNYN\n105\n196\n22\n39\n9\n50\n\n\njeffcmi01\n1985\nCLE\n9\n0\n0\n0\n0\n0\n\n\nrichmbe01\n1933\nCHN\n5\n1\n0\n0\n0\n0\n\n\ncooneji02\n1925\nSLN\n54\n187\n27\n51\n4\n5\n\n\nmcinnst01\n1919\nBOS\n120\n440\n32\n134\n23\n11\n\n\n\n\n\n\nWhat is the unit of observation for the Teams data set? What about for the Batting data set?\n\n\n\nWrite out two questions about baseball that could answered purely through summaries of these data sets (numerical summaries or plots).\n\n\n\nWrite out predictive questions (two classification and two regression) that you could answer about baseball using the data sets above. Identify a response variable for each.\n\n\n\nWhat is a question that we would need more granular (measured on a finer/more specific part of the game) data than the Teams and Batting data sets provide to answer?\n\n\n\nRoughly since 1962 MLB Teams have played 162 games in a season. What do you think the distribution of wins (W) looks like? Sketch a plot of what you think the entire wins column looks like, adding axis tick marks with plausible values, and describe the shape in words.\n\n\n\nWhat do you think the relationship between wins (W) and runs (R) looks like? Sketch a plot, adding axis tick marks with plausible values, and describe the shape in words.\n\n\n\nSome people believe analytics is ruining baseball because Teams are more cautious which makes the games less entertaining. Do you agree or disagree? Why? Answer in two or more sentences."
  },
  {
    "objectID": "6-prediction/03-overfitting/slides.html",
    "href": "6-prediction/03-overfitting/slides.html",
    "title": "Overfitting",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set: Overfitting\nLab: Cancer Diagnosis"
  },
  {
    "objectID": "6-prediction/03-overfitting/slides.html#agenda",
    "href": "6-prediction/03-overfitting/slides.html#agenda",
    "title": "Overfitting",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set: Overfitting\nLab: Cancer Diagnosis"
  },
  {
    "objectID": "6-prediction/03-overfitting/slides.html#announcements",
    "href": "6-prediction/03-overfitting/slides.html#announcements",
    "title": "Overfitting",
    "section": "Announcements",
    "text": "Announcements\n\nProblem Sets:\n\nPS 18: Overfitting releases Tuesday and due next Tuesday at 9am\nExtra Practice: Logistic Regression releases Thursday (non-turn in)\n\n\n\n. . .\n\nLab 6:\n\nLab 6.1 releases Tuesday and due next Tuesday at 9am\nLab 6.2 releases Thursday and due next Tuesday at 9am\n\n\n\n. . .\n\nQuiz 4:\n\nnext Monday in-class.\ncovers Wrong By Design through Logistic Regression (Thu/Fri)"
  },
  {
    "objectID": "6-prediction/03-overfitting/slides.html#rq-1",
    "href": "6-prediction/03-overfitting/slides.html#rq-1",
    "title": "Overfitting",
    "section": "RQ 1",
    "text": "RQ 1\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nWhich one of these (open pollev.com) is not an example of overfitting (either in real life or in statistics)?"
  },
  {
    "objectID": "6-prediction/03-overfitting/slides.html#where-is-overfitting-worse",
    "href": "6-prediction/03-overfitting/slides.html#where-is-overfitting-worse",
    "title": "Overfitting",
    "section": "Where is overfitting worse?",
    "text": "Where is overfitting worse?\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nSuppose I overfit my model to the training data. In which scenario (for which training data) would I expect the test set performance to be significantly worse? Assume that the testing sets A and B look like their corresponding training sets.\n\n\nCodelibrary(tidyverse)\nlibrary(patchwork)\nset.seed(1)\nn=200\nplot_A &lt;- tibble(x=runif(n=n, min=0, max=1)) |&gt;\n    mutate(y = x + rnorm(n=n, sd=.25))  |&gt;\n    ggplot(aes(x=x, y=y)) +\n    geom_point() + \n    ggtitle(\"Training data A\")\n\nplot_B &lt;- tibble(x=runif(n=n, min=0, max=1)) |&gt;\n    mutate(y = x + rnorm(n=n, sd=.01))  |&gt;\n    ggplot(aes(x=x, y=y)) +\n    geom_point() + \n    ggtitle(\"Training data B\")\n\nplot_A + plot_B\n\n\n\n\n\n\n\n\nIf you overfit the data to training set A, you will likely have a complex curve (maybe a polynomial). This curve will fit poorly a testing set that looks similar to A. If you overfit to B, since the data has low variance you will have a positively sloping line. The testing set performance will be very similar to the training set performance (it will not be markedly worse).\nAnswer: Training set A"
  },
  {
    "objectID": "6-prediction/01-method-of-least-squares/extra-practice.html",
    "href": "6-prediction/01-method-of-least-squares/extra-practice.html",
    "title": "The Method of Least Squares",
    "section": "",
    "text": "lm() uses the closed form equations to find the least squares estimates but there are other ways to do this optimization; one was discussed in the notes. We will have you go through this method here, comparing to the linear model you made in the problem set. Load the flights data into your R session.\nThe function rss below is meant to mirror the formula for \\(\\text{RSS}\\) for a simple linear regression model:\n\\[ \\text{RSS}(b_0,b_1) = \\sum_{i = 1}^n \\left(y_i - (b_{0} + b_1x_i    )\\right)^2\\]\n\nCoderss &lt;- function(coef) {\n  sum((flights$avg_speed - (coef[1] + coef[2] * flights$distance))^2)\n}\n\n\ncoef[1] and coef[2] represent \\(b_0\\) and \\(b_1\\), respectively and refer to the first two elements of a vector called coef which is the argument of the function.\n\nUse the optim() template given to find the slope \\(b_0\\) and intercept \\(b_1\\) that minimizes the RSS function. You can do this by using the table below. In this table, we have given you two different sets of starting values for \\(b_0\\) and \\(b_1\\) to try:\n\n\n\\(b_0 = 0\\), \\(b_1 = 0\\)\n\nEyeball the slope and intercept from the scatterplot with the line superimposed on the data (from the problem set).\n\n\n\n\nWrite in your final values in each case in the spaces given. Comment on whether they match the results you got from your linear model object in the problem set.\n\n\nCodeoptim(par = c( , ), fn = )\n\n\n\n\n\n\n\n\n\n\nStarting \\(b_0\\)\n\nStarting \\(b_1\\)\n\nFinal \\(b_0\\)\n\nFinal \\(b_1\\)\n\n\n\n\n0\n0\n\n\n\n\nEyeballed intercept: ___\nEyeballed slope: ___\n\n\n\n\n\n\nChange the rss function from the last question so it captures the sum of the absolute value of the residuals. This is given by the function below.\n\n\\[ \\text{RSABS}(b_0,b_1)= \\sum_{i = 1}^n \\left|y_i - (b_{0} + b_1x_i)    \\right|\\]\n\nCall this new function you make rsabs. You can fill in the template below.\n\n\nCodersabs &lt;- function(coef) {\n  \n  \n  \n  \n  \n}\n\n\n\nUse rsabs to estimate the parameters \\(b_0\\) and \\(b_1\\) using optim() as you did before for rss. Comment on whether your results matches the estimates that come out of lm() and from the rss function.\n\n\n\n\n\n\n\n\n\nStarting \\(b_0\\)\n\nStarting \\(b_1\\)\n\nFinal \\(b_0\\)\n\nFinal \\(b_1\\)\n\n\n\n\n0\n0\n\n\n\n\nEyeballed intercept: ___\nEyeballed slope: ___"
  },
  {
    "objectID": "6-prediction/01-method-of-least-squares/slides.html",
    "href": "6-prediction/01-method-of-least-squares/slides.html",
    "title": "Method of Least Squares",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set 16: The Method of Least Squares\n\nLab 5.1: Understanding the Context of the Data"
  },
  {
    "objectID": "6-prediction/01-method-of-least-squares/slides.html#agenda",
    "href": "6-prediction/01-method-of-least-squares/slides.html#agenda",
    "title": "Method of Least Squares",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set 16: The Method of Least Squares\n\nLab 5.1: Understanding the Context of the Data"
  },
  {
    "objectID": "6-prediction/01-method-of-least-squares/slides.html#announcements",
    "href": "6-prediction/01-method-of-least-squares/slides.html#announcements",
    "title": "Method of Least Squares",
    "section": "Announcements",
    "text": "Announcements\n\nProblem Sets:\n\nPS 16 released Tuesday and due next Tuesday at 9am\nExtra Practice released Thursday (non-turn in)\n\n\n\n. . .\n\nLab 5:\n\nLab 5.1 released Tuesday and due next Tuesday at 9am\nLab 5.2 released Thursday and due next Tuesday at 9am\nLab 5 Workshop next Monday"
  },
  {
    "objectID": "6-prediction/01-method-of-least-squares/slides.html#concept-question-1",
    "href": "6-prediction/01-method-of-least-squares/slides.html#concept-question-1",
    "title": "Method of Least Squares",
    "section": "Concept Question 1",
    "text": "Concept Question 1\nAn engineer working for Waymo self-driving cars is working to solve a problem. When it rains, reflections of other cars in puddles can disorient the self-driving car. Their team is working on a model to determine when the self-driving car is seeing a reflection of a car vs a real car.\n\nThink of a potential response and predictor, and about whether this is a regression or classification problem.\n\n\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nThis is a serious challenge encountered by self-driving cars at the moment. This is probably best thought of as a classification problem, with the response being either reflection / not or puddle / not. The predictors could be sensor input (cameras, lidar) as well as weather info (to know if it’s been raining)."
  },
  {
    "objectID": "6-prediction/01-method-of-least-squares/slides.html#concept-question-2",
    "href": "6-prediction/01-method-of-least-squares/slides.html#concept-question-2",
    "title": "Method of Least Squares",
    "section": "Concept Question 2",
    "text": "Concept Question 2\n\nHere is a function f.\n\n\nCodef &lt;- function(x, y) {\n  y*(x + 3) \n}\n\n\n. . .\n\nWhat will the following line of code return?\n\n. . .\n\nCodef(3,5)\n\n\n\nThis question reminds students that if function arguments are unnamed, they are assumed to have been input in the order they were originally specified as in the function definition. x will be assigned 3, and y will be assigned 5, so the correct answer is 30."
  },
  {
    "objectID": "6-prediction/02-improving-predictions/notes.html",
    "href": "6-prediction/02-improving-predictions/notes.html",
    "title": "Evaluating and Improving Predictions",
    "section": "",
    "text": "In the last lecture we built our first prediction machine: a line drawn through a scatter plot that that minimizes the sum of squared residuals. In these lecture notes we focus on two questions: How can we evaluate the quality of our predictions? and How can we improve them?"
  },
  {
    "objectID": "6-prediction/02-improving-predictions/notes.html#evaluating-the-fit-to-your-data",
    "href": "6-prediction/02-improving-predictions/notes.html#evaluating-the-fit-to-your-data",
    "title": "Evaluating and Improving Predictions",
    "section": "Evaluating the fit to your data",
    "text": "Evaluating the fit to your data\nOnce you have fit a linear model to a scatter plot, you are able to answer questions such as:\n\nWhat graduation rate would you expect for a state with a poverty rate of 15%?\n\nGraphically, this can be done by drawing a vertical line from where the poverty rate is 15% and finding where that line intersects your linear model. If you trace from that intersection point horizontally to the y-axis, you’ll find the predicted graduation rate.\n\n\n\n\n\n\n\n\nFrom the plot above, we can tell that the model yields a prediction around roughly 82.5%. To be more precise, we could plug the x-value into our equation for the line and solve.\n\\[ \\hat{y} = 96.2 + -0.89 \\cdot 15 = 82.85 \\]\nSo how good of a prediction is 82.85%? Until we observe a state with a poverty rate of 15%, we’ll never know! What we can know, however, is how well our model explains the structure found in the data that we have observed. For those observations, we have both the predicted (or fitted) values \\(\\hat{y}_i\\) as well as their actual y-values \\(y_i\\). These can be used to calculate a statistic that measures the explanatory power of our model.\nMeasuring explanatory power: \\(R^2\\)\n\n\\(R^2\\) is a statistic that captures how good the predictions from your linear model are (\\(\\hat{y}\\)) by comparing them another even simpler model: \\(\\bar{y}\\). To understand how this statistic is constructed please watch this short video found in the Media Gallery on bCourses (14 minutes).\n\n\n\n\nR-squared (\\(R^2\\))\n\nA statistic that measures the proportion of the total variability in the y-variable (total sum of squares, TSS) that is explained away using our model involving x (sum of squares due to regression, SSR).\n\\[R^2 = \\frac{SSR}{TSS} = \\frac{\\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\\]\nBecause the total variablity is composed of the explained and the unexplain variability, \\(R^2\\) can be equivalent formulated as 1 minus the proportion of total variability that is unexplained by the model, which uses the more familiar residual sum of squares (RSS).\n\\[R^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\\]\n\\(R^2\\) has the following properties:\n\nAlways takes values between 0 and 1.\n\n\\(R^2\\) near 1 means predictions were more accurate.\n\n\\(R^2\\) near 0 means predictions were less accurate.\n\n\n\n\n\n\n\n\n\nExample: Poverty and Graduation\nTo fit the least squares linear regression model to predict graduation rate using the poverty rate, we turn to the familiar lm() function.\n\nm1 &lt;- lm(Graduates ~ Poverty, data = poverty)\n\nFor this particular model, \\(R^2 = .56\\). This means that poverty rate is able to explain about 56% of the variability found in graduation rates. That’s a good start!"
  },
  {
    "objectID": "6-prediction/02-improving-predictions/notes.html#improving-predictions",
    "href": "6-prediction/02-improving-predictions/notes.html#improving-predictions",
    "title": "Evaluating and Improving Predictions",
    "section": "Improving predictions",
    "text": "Improving predictions\n\\(R^2\\) allows us to quantify how well the model explains the structure found in the data set. From a model-building standpoint, it gives us a goal: to find a model with the highest possible \\(R^2\\). Here we outline three different methods for pursuing this goal - adding predictors, transformations, and polynomials - and we’d look at a different data set for each one.\nAdding Predictors\nLet’s return to the data set that that we studying when we first learned about multiple linear regression: ratings of Italian restaurants from the ZAGAT guide. For each of the 168 restaurants in the data set, we have observations on the average price of a meal, the food quality, the quality of the decor, the quality of the service, and whether it is east or west of Fifth Avenue.\n\n\n# A tibble: 168 × 6\n   restaurant          price  food decor service geo  \n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1 Daniella Ristorante    43    22    18      20 west \n 2 Tello's Ristorante     32    20    19      19 west \n 3 Biricchino             34    21    13      18 west \n 4 Bottino                41    20    20      17 west \n 5 Da Umberto             54    24    19      21 west \n 6 Le Madri               52    22    22      21 west \n 7 Le Zie                 34    22    16      21 west \n 8 Pasticcio              34    20    18      21 east \n 9 Belluno                39    22    19      22 east \n10 Cinque Terre           44    21    17      19 east \n# ℹ 158 more rows\n\n\nMaybe we want a model that will tell us how much we will have to spend at a new restaurant that is not upfront about its pricing; or maybe we just opened a new restaurant and want to know how much customers expect to spend. So price will serve as our response variable, leaving us four possible predictor variables. Let’s fit four different regression models, each one incorporating more more information by adding a predictor.\n\nm1 &lt;- lm(price ~ food, data = zagat)\nm2 &lt;- lm(price ~ food + geo, data = zagat)\nm3 &lt;- lm(price ~ food + geo + decor, data = zagat)\nm4 &lt;- lm(price ~ food + geo + decor + service, data = zagat)\n\nUnfortunately we can’t visualize these four linear models as four lines on a scatterplot because only the first model describes a line. The second describes two parallel lines; the third describes two parallel planes in 3D; the fourth describes two parallel hyperplanes in 4D (🤯).\nWe can, however, compare these four models in an arena where they’re all on the same playing field: how well they predict price. To quantify that, we can calculate the \\(R^2\\) value for each.\n\n\n  model R_squared\n1    m1 0.3931835\n2    m2 0.3987720\n3    m3 0.6278808\n4    m4 0.6278809\n\n\nObserve that the more information we provide the model - by adding predictors - the greater the \\(R^2\\) becomes! This is not a particular characteristic of the ZAGAT data set but of \\(R^2\\) in general. Adding new predictors will never lower the \\(R^2\\) of a model fit using least squares.\nNon-linear transformation\nThe world is not always linear. We can create non-linear prediction models by building off the above linear model machinery. To demonstrate how to use this approach to increase the predictive power of our model, we’ll turn to a non-linear trend that should look familiar…\nA single non-linear term\nTake a question from flights lab as an example where we plot the average airspeed vs. flight distance. First let’s try fitting a linear model.\n\n\n\n\n\n\n\n\nA linear model does not seem appropriate to model average speed from distance. There does appear to be a monotonically increasing trend, but it starts out steeper then flattens out1. This trend is reminiscent of functions like log or square root.\nLets try transforming our predictor (distance) with the log function to create a new variable called log_dist.\n\nflights &lt;- flights |&gt; \n    mutate(log_dist = log(distance))\n\nWe can then fit a linear model using this new log_dist variable as the predictor.\n\nlm_speed_from_log_dist &lt;- \n  lm(formula = avg_speed ~ log_dist, data=flights)\n\nLooking at the data below, we see there does seem to be a linear relationship between avg_speed and our new variable log_dist! Notice the x-axis in the below plot is log_dist whereas it was distance in the above plot.\n\n\n\n\n\n\n\n\nThe linear model with log_dist (\\(R^2=0.843\\)) predicts avg_speed better than the linear model with distance (\\(R^2=0.72\\))\nWe can now think of our predictive model as\n\\[\n\\widehat{y} = b_0 + b_1 \\cdot \\log(x)\n\\]\nIn other words, our model is non-linear since \\(x\\) appears inside of a logarithm. We can plot the non-linear prediction function in the original predictor distance and we see the prediction function is curved!\n\n\n\n\n\n\n\n\nSo is this a linear model or a non-linear model? It’s both. We created a new variable log_dist by transforming the original variable; the prediction function is a linear function of this new variable. But we can also think of this as a function of the original variable distance; the prediction function is a non-linear function of this original variable.\nPolynomials\nSometimes we need an more complex transformation than just a simple function (e.g. \\(\\sqrt{x}, \\log(x),  x^2,...\\)). Take the following example where there is a strong association between x and y, but it’s not linear (this data, admitted, was simulated in R).\n\n\n\n\n\n\n\n\nSo how should we model this? Polynomials to the rescue!\nA polynomial is a function like\n\\[\nf(x) = -20 + 34 x - 16 x^2 + 2 x^3\n\\]\nMore generally a polynomial is a function like\n\\[\nf(x) = c_0 + c_1 \\cdot x + c_2 \\cdot x^2 + \\dots + c_d \\cdot x^d\n\\]\nwhere the \\(d+1\\) coefficients \\(c_0, c_1, \\dots, c_d\\) are constants The number \\(d\\) is called the degree of the polynomial – this is the largest exponent that appears.\nPolynomials are flexible functions that can be quite useful for modeling. We can fit a polynomial model by adding new transformed variables to the data frame then fitting a linear model with these new transformed variables. This is just like how we fit a logarithmic function before by adding a new log transformed variable to the data frame then fit a linear model.\n\nThe prediction function here is a polynomial given by\n\\[\n\\widehat{y} = -20.086 + 34.669 \\cdot x -16.352 \\cdot x^2 + 2.042 \\cdot x^3\n\\]"
  },
  {
    "objectID": "6-prediction/02-improving-predictions/notes.html#the-ideas-in-code",
    "href": "6-prediction/02-improving-predictions/notes.html#the-ideas-in-code",
    "title": "Evaluating and Improving Predictions",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nInspect model output with the broom library\nConsider the code we ran earlier to fit a linear model which can predict graduation rate using the poverty rate.\n\nm1 &lt;- lm(Graduates ~ Poverty, data = poverty)\n\nWhen you run this code, you’ll see a new object appear in your environment: m1. This new object, though, is not a vector or a data frame. It’s a much richer object called a list that stores all sorts of information about your linear model. You can click through the different part of m1 in your environment pane, or your can use functions from the broom package to extract the important components using code.\n\nlibrary(broom)\nglance(m1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.558         0.549  2.50      61.8 3.11e-10     1  -118.  242.  248.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe glance() function returns a series of different metrics used to evaluate the quality of your model. First among those is r-squared. Because the output of glance() is just another data frame, we can extract just the r-squared column using select().\n\nglance(m1) |&gt;\n    select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.558\n\n\nHere’s the \\(R^2\\) we got earlier!\nFitting polynomials in R with poly()\n\nIn R, we can fit polynomials using the poly() function. Here is the code that was used to fit the polynomial earlier in the notes.\nYou do not need to worry about the meaning behind the raw = TRUE argument. The simulated data frame mentioned earlier is called df, and has two variables in it: predictor and response.\n\nlm(formula = response ~ poly(x = predictor, \n                   degree = 3, \n                   raw = TRUE), data = df)\n\n\nCall:\nlm(formula = response ~ poly(x = predictor, degree = 3, raw = TRUE), \n    data = df)\n\nCoefficients:\n                                 (Intercept)  \n                                     -20.086  \npoly(x = predictor, degree = 3, raw = TRUE)1  \n                                      34.669  \npoly(x = predictor, degree = 3, raw = TRUE)2  \n                                     -16.352  \npoly(x = predictor, degree = 3, raw = TRUE)3  \n                                       2.042  \n\n\nMaking predictions on a new observation with predict()\n\nWe have spending a lot of time talking about how to fit a model meant for predicting, but have not actually done any predicting! The predict() function can help us do this. It takes in two main arguments:\n\n\nobject: This is the linear model object which contains the coefficients \\(b_0\\), …, \\(b_p\\). In the graduate and poverty example, this object was m1. We had m1 through m4 in the ZAGAT example.\n\nnewdata: This is a data frame containing the new observation(s). This data frame must at least contain each of the predictor variables used in the column, with a value of these variables for each observation.\n\nExample: ZAGAT food rating\nHere, we will use m2 from the ZAGAT example. This model used \\(food\\) and \\(geo\\) in an attempt to predict price at a restaurant.\nFirst, let’s make a new data frame with a couple of new, made-up observations.\n\nrestaurants &lt;- data.frame(\n  food = c(25, 17),\n  geo = c(\"east\", \"west\"))\n\nOne of these restaurants is located in east Manhattan and has a food score of 25/30, while the other one is in west Manhattan and has a food score of 17/30.\nNow, we can use this data frame alongside our m2 model object to make predictions for the prices.\n\npredict(object = m2, newdata = restaurants)\n\n       1        2 \n55.89738 31.44043 \n\n\nWe are predicted to have to pay roughly \\(\\$56\\) at the first restaurant and roughly \\(\\$31\\) at the second."
  },
  {
    "objectID": "6-prediction/02-improving-predictions/notes.html#summary",
    "href": "6-prediction/02-improving-predictions/notes.html#summary",
    "title": "Evaluating and Improving Predictions",
    "section": "Summary",
    "text": "Summary\nIn this lecture we learned how to evaluate and improve out predictions. While there are many metrics to measure the explanatory power of a model, one of the most commonly used is \\(R^2\\), the proportion of the variability of the \\(y\\) that is explained by the model.\nTo improve our predictions - and increase the \\(R^2\\) - we saw three different strategies. If you have additional predictors in your data frame, its easy as pie to add them to your regression model and you are guaranteed to increase your \\(R^2\\).\nA second strategy is capture non-linear structure by creating new variables that are simple transformations of the existing variable. The third approach, also targeting non-linear structure, is to replace a single predictor with a polynomial."
  },
  {
    "objectID": "6-prediction/02-improving-predictions/notes.html#footnotes",
    "href": "6-prediction/02-improving-predictions/notes.html#footnotes",
    "title": "Evaluating and Improving Predictions",
    "section": "Footnotes",
    "text": "Footnotes\n\nWe call this concave or sometimes diminishing marginal returns.↩︎"
  },
  {
    "objectID": "6-prediction/02-improving-predictions/ps.html",
    "href": "6-prediction/02-improving-predictions/ps.html",
    "title": "Evaluating and Improving Predictions",
    "section": "",
    "text": "Today’s data is on house pricing in Los Angeles! We have access to data on all of the houses that sold in four cities around west LA in the course of a single month. Our goal is to predict the price of a home based on a few attributes of that home. Check Ed for a link to the data.\nWhere applicable, answer the following questions using R code and write the code you used in the space below.\n\nCreate a scatterplot examining the relationship between square footage and price. Write the code you used below.\n\n\n\n\n\n\nAdd two columns onto the LA dataset which measure log price and log square footage. Save the new dataset back into itself.\n\n\n\n\n\n\nCreate another scatterplot examining the relationship between log square footage and log price. Rather than writing the code you used, explain in at least two sentences whether the logged variables or the regular variables are more suitable for a linear model.\n\n\n\n\n\n\nFit two linear models\n\n\none which predicts price with square footage\none which predicts log price with log square footage\n\n\nReport the \\(R^2s\\) for both models, and in a sentence, state whether the results you have line up with your explanation in the last question.\n\n\n\n\n\n\n\nRecently the University of California purchased a new house to serve as the residence of the university President and to host university functions. The address of the house is 2821 Claremont Blvd in Berkeley.\n\nUse your linear model to predict the sale price in log USD of this house. (hint: the internet is helpful!). Then, find this price in regular USD.\n\n\n\n\nWas your model an under- or over-estimate? Why do you think this is?"
  },
  {
    "objectID": "6-prediction/05-logistic-regression/notes.html",
    "href": "6-prediction/05-logistic-regression/notes.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "The framework that we used to build a predictive model for regression followed four distinct steps:\nWe used this process to build our first simple linear regression model to predict high school graduation rates but the same four steps are used by Zillow to build ZestimateTM, their deep learning model to predict house price.\nThese are also the same four steps that we will use as we shift in these notes to the task of classification. In a classification task, we seek to predict a response variable that is categorical, very often a two-level categorical variable. Classification models are everywhere: they help doctors determine whether or not a patient has a disease, whether or not an image contains a particular object (say, a person or a cat), and whether or not customer will purchase an item."
  },
  {
    "objectID": "6-prediction/05-logistic-regression/notes.html#example-spam-filters",
    "href": "6-prediction/05-logistic-regression/notes.html#example-spam-filters",
    "title": "Logistic Regression",
    "section": "Example: Spam filters",
    "text": "Example: Spam filters\nEmail spam, also referred to as junk email or simply spam, is unsolicited messages sent in bulk by email (spamming). The name comes from a Monty Python sketch in which the name of the canned pork product Spam is ubiquitous, unavoidable, and repetitive1. A spam filter is a classification model that determines whether or not a message is spam based on properties of that message. Every mainstream email client, including Gmail, has a spam filter built in to ensure that the messages that get through to the user are genuine messages.\nThe Data\nIn order to build a spam filter, we’ll need a data set to train our model on. This data set must have as the unit of observation a single email message, record whether or not the message was spam (the response), and record various features of the message that are associated with being spam (the predictors). Such a data set, with nearly 4000 observations, can be found in the email data frame in the openintro library.\n\nlibrary(openintro)\nemail\n\n# A tibble: 3,921 × 21\n   spam  to_multiple from     cc sent_email time                image attach\n   &lt;fct&gt; &lt;fct&gt;       &lt;fct&gt; &lt;int&gt; &lt;fct&gt;      &lt;dttm&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n 1 0     0           1         0 0          2011-12-31 22:16:41     0      0\n 2 0     0           1         0 0          2011-12-31 23:03:59     0      0\n 3 0     0           1         0 0          2012-01-01 08:00:32     0      0\n 4 0     0           1         0 0          2012-01-01 01:09:49     0      0\n 5 0     0           1         0 0          2012-01-01 02:00:01     0      0\n 6 0     0           1         0 0          2012-01-01 02:04:46     0      0\n 7 0     1           1         0 1          2012-01-01 09:55:06     0      0\n 8 0     1           1         1 1          2012-01-01 10:45:21     1      1\n 9 0     0           1         0 0          2012-01-01 13:08:59     0      0\n10 0     0           1         0 0          2012-01-01 10:12:00     0      0\n# ℹ 3,911 more rows\n# ℹ 13 more variables: dollar &lt;dbl&gt;, winner &lt;fct&gt;, inherit &lt;dbl&gt;, viagra &lt;dbl&gt;,\n#   password &lt;dbl&gt;, num_char &lt;dbl&gt;, line_breaks &lt;int&gt;, format &lt;fct&gt;,\n#   re_subj &lt;fct&gt;, exclaim_subj &lt;dbl&gt;, urgent_subj &lt;fct&gt;, exclaim_mess &lt;dbl&gt;,\n#   number &lt;fct&gt;\n\n\nWe see in the left most column the response variable, spam, coded as 0 when a message is not spam and 1 when it is. The first predictor, to_multiple, is a 1 if the message was sent to multiple people and 0 otherwise. cc records the number of people who are cc’ed on the email. winner (listed as a variable below the glimpse of the dataframe) records whether or not the word “winner” showed up in the message. The remaining predictors you may be able to intuit by their names, but you can also read the help file that describes each one.\nThese variables seem like they might be useful in predicting whether or not an email is spam, but take a moment to consider: how were we able to get our hands on data like this?\nThis particular data set arose from selecting a single email account, saving every single messages that comes in to that address over the course of a month, processing each message to create values for the predictor variables, then visually classifying whether or not the message is spam. That’s to say: this data represents a human’s best effort to classify spam2. Can we build a model that will be able to identify the features that mark a message as spam to be able to automatically classify future messages?\n\n\n\nExploratory Data Analysis\nLet’s see how well a few tempting predictors work at separating spam from not spam by performing some exploratory data analysis. We might expect messages containing the word “winner” to be more likely to be spam than those that do not. A stacked, normalized bar chart can answer that question.\n\n\n\n\n\n\n\n\nIndeed, it looks like around 30% of emails with “winner” were spam, compared to roughly 10% of those without. At this point, we could consider a very simple spam filter: if the message contains “winner”, then classify it as spam.\nAlthough this is tempting, it is still a pretty weak classifier. Most of the messages with “winner” are not spam, so calling them spam will result in most of them being misclassified.\nSo if “winner” isn’t the silver bullet predictor we need, let’s try another: num_char. This variable records the total number of characters present in the message – how long it is. We probably have no prior sense of whether spam would be more likely to consist of short or long emails, so let’s visualize them. This predictor is numerical continuous, so let’s overlay two density plots to get a sense of the distribution between spam and not spam.\n\n\n\n\n\n\n\n\nThe original plot on the left is very difficult to read because this variable is heavily right-skewed: there are a small number of very long messages that obscure much of the data in the plot. On the right is a more useful visualization, one of the log-transformed version of the same variable.\nWe see a reasonable separation here: spam messages tend to be shorter than non-spam messages. We could consider another very simple spam filter: if the log number of characters is less than 1.2, classify it as spam.\nHowever, this simple filter suffers from the same problem as the first. Although these density plots have the same area, there are in fact far fewer overall instances of spam than not-spam. That means that there are far more not-spam messages with a log number of characters less than 1.2 than there are spam message. This filter, like the first, would misclassify much of the training data.\nWhat we need is a more general framework to fold the strength of these two variables - as well as many of the other ones in the data set - into a model that can produce a single, more accurate prediction. We will start from the framework of multiple linear regression and move to something that can be used for classification."
  },
  {
    "objectID": "6-prediction/05-logistic-regression/notes.html#from-linear-to-logistic-regression",
    "href": "6-prediction/05-logistic-regression/notes.html#from-linear-to-logistic-regression",
    "title": "Logistic Regression",
    "section": "From Linear to Logistic Regression",
    "text": "From Linear to Logistic Regression\nLet’s start by taking our existing simple linear regression model from the past few notes and applying it in this classification setting. We can visualize the relationship between log_num_char and spam using an ordinary scatter plot.\n\n\n\n\nThis is a strange looking scatter plot - the dots can only take y values of 0 or 1 - but it does capture the overall trend observed in the density plots, that spam messages are longer. Since we have a scatter plot, we can fit a simple linear regression model using the method of least squares (in gold).\n\\[ \\hat{y} = b_0 + b_1 x \\]\nWhile this is doable, it leaves us with a bit of a conundrum. For a low value of log_num_char it’s possible that we would predict \\(\\hat{y} = 1\\) and for a high value it’s possible that we’d predict \\(\\hat{y} = 0\\). But what if log_num_char is somewhere in the middle? What does it mean if we predict that the value of spam is .71?\nOne approach to resolving this is to treat our prediction not as a value of \\(y\\), but as a estimate of the probability, \\(\\hat{p}\\), that \\(y = 1\\). We can rewrite the model as:\n\\[ \\hat{p} = b_0 + b_1 x \\]\nThis resolves the conundrum of how to think about a prediction of .71. That is now the model’s determination of the probability that the message is spam. This tweak solves one problem, but it introduces another. How do we interpret predictions at very high values of log_num_char, where \\(\\hat{p}\\) is negative? Surely a probability cannot be negative!\nWe can fix this by changing the mathematical form of the model used to predict the response. Instead of it being a line, we can use an alternative function that prevents predictions greater than 1 and less than zero. The most commonly used function is called the standard logistic function:\n\\[ f(z) = \\frac{1}{1 + e^{-z}}\\]\n\\(z\\) can be any number of the real number line. As \\(z\\) gets large, \\(f(z)\\) approaches 1; as \\(z\\) is negative, \\(f(z)\\) approaches 0; when \\(z\\) is 0, \\(f(z)\\) is .5.\nThis is a very clever idea. It allows us to combine all of the information from our predictors into a single numerical score, \\(z\\). We can obtain \\(z\\) through a form that is familiar to us: \\(b_0 + b_1 x_1 + \\ldots + b_p x_p\\). This score can then be sent through the logistic function to estimate the probability that \\(y = 1\\). This method is called logistic regression.\n\n\nLogistic Regression (for prediction)\n\nA model to predict the probability that 0-1 response variable \\(y\\) is 1 using the inverse logit of a linear combination of predictors \\(x_1, x_2, \\ldots, x_p\\).\n\\[ \\hat{p} = \\frac{1}{1 + e^{-\\left(b_0 + b_1 x_1 + \\ldots + b_p x_p\\right)}} \\]\nCan be used as a classification model by setting up a rule of the form: if \\(\\hat{p}_i\\) &gt; threshold, then \\(\\hat{y}_i = 1\\).\n\n\nWe can visualize the approach that logistic regression takes by sketching the predictions as a green s-shaped curve on top of our scatter plot."
  },
  {
    "objectID": "6-prediction/05-logistic-regression/notes.html#classification-errors",
    "href": "6-prediction/05-logistic-regression/notes.html#classification-errors",
    "title": "Logistic Regression",
    "section": "Classification errors",
    "text": "Classification errors\nNow that we have a model archetype to move forward with, we are almost ready to build it and see how it does. First though, we should first consider what happens when our predictions are wrong. Hopefully, we will classify most of our e-mails correctly, but we will fail at least some of the time. We might predict that an e-mail is spam when it is not. In other cases, we might predict that an e-mail is not spam when it actually is. Therefore, there are two types of error we should consider.\nRecall that based on the email dataset, 1 refers to an e-mail which is truly spam. In a binary classification problem, we often call 1 a positive result. Likewise, 0 refers to an e-mail which is genuine. We call 0 a negative result. This sets up formal definitions for the two types of errors mentioned above.\n\nFalse Positives\n\nPredicting a 1 that is in fact a 0\n\nFalse Negatives\n\nPredicting a 0 that is in fact a 1\n\n\nMisclassification rate\nWhile thinking about failing is not exciting, it does give us a way to think about how well our classification model is doing. We would like the number of misclassifications over a large number of e-mails to be as small as possible. This fraction can be quantified as the misclassification rate or \\(\\text{MCR}\\) for short.\nMisclassification Rate\n\\[\\text{MCR} = \\frac{\\text{no. FP} + \\text{no. FN}}{\\text{no. of predictions}} = \\frac{\\text{no. of misclassifications}}{\\text{no. of predictions}}\\]\nJust like in a multiple linear regression context, we can fit a suite of different models on the training set and evaluate their performance on the testing set. Now, our model type of choice is logistic regression and our evaluation metric is \\(\\text{MCR}\\)."
  },
  {
    "objectID": "6-prediction/05-logistic-regression/notes.html#the-ideas-in-code",
    "href": "6-prediction/05-logistic-regression/notes.html#the-ideas-in-code",
    "title": "Logistic Regression",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nFitting Logistic Regression with glm()\n\nThe computational machinery for fitting logistic regression looks almost identical to what we used for linear least squares regression. The primary function we’ll use is glm(). We will also use a different broom function called tidy() to display the coefficients \\(b_0\\) and \\(b_1\\). tidy() can also be used in a linear regression context as well.\n\nm1 &lt;- glm(spam ~ log(num_char), data = email, family = \"binomial\")\nm1 |&gt;\n  tidy() |&gt;\n  select(term,estimate,std.error)\n\n# A tibble: 2 × 3\n  term          estimate std.error\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     -1.72     0.0606\n2 log(num_char)   -0.544    0.0365\n\n\nThese coefficients are a bit more challenging to interpret since they’re no longer linearly related to the response. The sign of the coefficient for log(num_char), however, is informative: it tells us that messages with more characters will be predicted to have a lower probability of being spam.\nEvaluating model performance\nCalculating \\(\\text{MCR}\\)\n\nLet’s take a look at the predictions that this model makes back into the data set that it was trained on. When using the predict() function on logistic regression models, there are several different types of predictions that it can return, so be sure to use the additional argument type and supply it with \"response\". This will return \\(\\hat{p}_i\\) (probability of 1) for each observation.\nWe can then move from values of \\(\\hat{p}_i\\) to values of \\(\\hat{y}_i\\). In general, our rule will be to check and see whether each value of \\(\\hat{p}_i\\) is greater than .5. If so, we will assign the value of 1; otherwise, we will assign it 0. This sounds similar to creating a logical variable, but assigning something different than TRUE and FALSE to each observation in the dataset. We will use a function called ifelse(), therefore, which will allow us to assign we want (1 and 0).\nThe following block of code completes both of these two steps and saves the results back into the email data frame.\n\nemail &lt;- email |&gt;\n  mutate(p_hat_m1 = predict(object = m1, \n                            newdata = email, \n                            type = \"response\"),\n         y_hat_m1 = ifelse(test = p_hat_m1 &gt; .5, \n                           yes = 1, \n                           no = 0))\n\nWe are now ready to calculate \\(\\text{MCR}\\). First, we can find all of the emails for which the y and y_hat_m1 don’t match. Then, we can take the proportion of those e-mails out of all e-mails in the dataset.\n\nemail |&gt;\n  mutate(misclassified = (spam != y_hat_m1)) |&gt;\n  summarise(MCR = mean(misclassified))\n\n# A tibble: 1 × 1\n     MCR\n   &lt;dbl&gt;\n1 0.0956\n\n\nOverall, we are misclassifying around 10 percent of all e-mails (both spam and genuine), which doesn’t look like a bad start, but we might want to take a little deeper of a dive to see how the model is misclassifying observations.\nFalse positives versus false negatives\nIndeed, we can see if the model is failing at classifiying one type of e-mail more than another. To do this, we can find the number of false positives and false negatives. We can group our observations by their actual class versus their predicted class. There are four possibilities, so there will be four groups.\n\nemail |&gt;\n  group_by(spam, y_hat_m1) |&gt;\n  summarise(n = n())\n\n# A tibble: 4 × 3\n# Groups:   spam [2]\n  spam  y_hat_m1     n\n  &lt;fct&gt;    &lt;dbl&gt; &lt;int&gt;\n1 0            0  3541\n2 0            1    13\n3 1            0   362\n4 1            1     5\n\n\nWe actually see that the model is doing great at predicting correctly when the e-mail is genuine (few false positives), but doing horribly at detecting spam (many, many false negatives). Only \\(5\\) out of the \\(367\\) spam e-mails are being classified correctly. Essentially all of our mistakes are in the form of false negatives! We also can see here that there are way more genuine e-mails than spam e-mails in the dataset, so our misclassification rate is being inflated as a result. Clearly, more than just the length of an e-mail is necessary to help us detect spam.\nTraining and testing sets\nOne final note: this \\(\\text{MCR}\\) was calculated over all of the data. The best way to evaluate the performance of the model is to split the data into training and testing sets, fit the model on the training set and evaluate it on the testing set. We can calculate both training and testing versions of \\(\\text{MCR}\\) and compare them to see if the model is doing well on e-mails it hasn’t yet seen."
  },
  {
    "objectID": "6-prediction/05-logistic-regression/notes.html#summary",
    "href": "6-prediction/05-logistic-regression/notes.html#summary",
    "title": "Logistic Regression",
    "section": "Summary",
    "text": "Summary\nIn these notes we introduced the concept of classification using a logistic regression model. Logistic regression uses the logistic function to transform predictions into a probability that the response is 1. These probabilities can be used to classify y as 0 or 1 by checked to see if they exceed a threshold (often .5).\nWe then went through the process of fitting logistic regression to help us classify spam e-mails, and evaluated our results using the misclassification rate."
  },
  {
    "objectID": "6-prediction/05-logistic-regression/notes.html#footnotes",
    "href": "6-prediction/05-logistic-regression/notes.html#footnotes",
    "title": "Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\nDefinition from Wikipedia, along with the image, by freezelight/flickr.↩︎\nThis data collection and manual processing was done by a graduate student in statistics at UCLA. One of the many humdrum but valuable tasks asked of graduates students . . .↩︎"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Prerequisites",
    "section": "",
    "text": "Prerequisites\nInstall Docker.\nIf you are running on a computer with Apple Silicon (using a new “M” processor), - Update macOS to Venture 13 or newer. - Install Apple’s Rosetta emulation by running softwareupdate --install-rosetta. - In Docker Desktop, enable Settings &gt; Features in development &gt; Use Rosetta for x86/amd64 emulation on Apple Silicon.\nIf you are running Microsoft Windows, - Install WSL. - In a Command or Power Shell window, run wsl --install -d Ubuntu. - In an Ubuntu window, run apt update, then apt install make. - Run exec ssh-agent bash to start your SSH agent.\nCheckout this repository into a new working directory.\n\n\nRun the Container\nIn a terminal window, change into the working directory and run make up. This initializes your environment by creating the file .env. It then runs docker compose.\nWhen your environment changes, for example if you log out and back in, or if you manually copy your working directory to another platform, run make clean or manually delete the .env file. Then run make up.\n\n\nBuild a Custom Image (WIP)\nThe container is normally built automatically by a CI process on GitHub Actions. If you need to alter the image, for example if you want to locally test a new library addition, you can do the following:\n\nCheckout the https://github.com/stat20/stat20-docker repository.\nMake any changes\nRun docker build -t somename:sometag .. If on an alternative hardware platform like Apple’s ARM (modern Apple computers), you can run docker build -t somename:sometag --platform linux/amd64 ..\nCreate a new file names docker-compose.override.yml with the following contents:\n\nversion: \"3.8\"\nservices:\n  stat20:\n    image: somename:sometag"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Staff\n\nInstructor of Record: Jeremy Eli Sanchez (jeremysanchez@berkeley.edu)\nTutors: Evelyn Cheng, Christopher Lee, Emma Wu\n\n\n\nLecture time and location\n\nMondays and Wednesdays: 2-4pm, Wheeler 212\nThursdays: 2-4pm, Cory 77\n\n\n\nLab time and location\n\nMondays: 4-5:30pm, Wheeler 212\nThursdays: 4-5:30pm, Cory 77\n\nThis period can be treated as extra time to work on your assignments. I will be there to help you!\n\n\nCourse Culture\nStudents taking Stat 20 come from a wide range of backgrounds. We hope to foster an inclusive and supportive learning environment based on curiosity rather than competition. All members of the course community—the instructor, students, tutors, and readers—are expected to treat each other with courtesy and respect.\nYou will be interacting with course staff and fellow students in several different environments: in class, over the discussion forum, and in office hours. Some of these will be in person, some of them will be online, but the same expectations hold: be kind, be respectful, be professional.\nIf you are concerned about classroom environment issues created by other students or course staff, please come talk to us about it.\n\n\nMode of Instruction\nThis course is mostly structured as a flipped class, meaning that you will first be encountering new concepts in statistics and data science outside of class. Class time is dedicated to expanding on the work you’ve done outside of class by working through questions solo, in groups, and as a class. On Mondays and during select lectures, we will revert to a more traditional lecturing style.\n\n\nBefore class\n\nMondays\nFinish up your lab or study for your quiz! You can read more about this below.\n\nWednesdays/Thursdays\nIt is your responsibility to become familiar with the topics that appear in the course notes and to work through the reading questions on Gradescope by 12 pm Wedenesday (for Weds class) and 12pm Thursday (for Thurs class).\nYou’re encouraged to experiment to find the method that works best for you: downloading the notes as a pdf and making notes on them, asking and answering questions over the class forum, etc.\n\n\n\n\nDuring class\n\nMondays: traditional lecture\nThe instructor will walk through the material in a regular lecture format using the iPad, mixing in practice questions so you can work on them in groups. Once we cover everything we need to, we will spend the remainder of time working through components of Problem Sets and Labs.\nWe will be conducting the whole of the probability unit in this format.\n\n\nWednesdays/Thursdays: flipped classroom\nClass time will be spent on a range of activities, but the most common will be answering concept questions (using Poll Everywhere) that check your understanding of the notes in the first half of class, and then working through components of your Problem Sets and Labs in the second half of class. We will not be walking through the material like on Mondays, so it’s imperative that you complete the reading and the reading questions before coming to class.\n\n\n\n\n\nHelp outside of class\nAsking for help does not send a signal that you are behind or need extra help. On the contrary, asking for help early and often tends to co-occur with success in the course. To this end, we have a few options available for you when this!\n\nInstructor OH\nThe instructor will offer office hours twice a week. Instructors are happy to chat about the course material, statistics in general, careers in statistics, and whatever other statistics or data science topics are on your mind!\n\n\nGroup tutoring\nTutors will offer one group tutoring session each week. This is an opportunity to finish up any assignments that you’ve started in class or review any topics that are confusing for you. Group tutoring is a great place to go to meet other students and collaborate on assignments with tutors on hand to help you get unstuck.\n\n\n\nMaterials\nThe primary materials for the course are the lecture notes, which will be posted to the course website in advance of class. We’ll teach you everything you need to know!\n\n\nRStudio\nThe software that we’ll be using for our data analysis is the free and open-source language called R that we’ll be interacting with via software called RStudio. As a Berkeley student, you have your own version of RStudio waiting you for at: http://stat20.datahub.berkeley.edu. Most students taking Stat 20 have no experience programming; we’ll teach you everything you need to know!\n\n\n\nCourse communication\n\nbCourses\nWe will use bCourses to disseminate announcements for the entire class, such as final exam information.\n\n\nDiscussion forum\nThe official discussion forum for the class will be hosted on Ed. Ed is a forum to ask and answer questions with your fellow students and course staff. It’s an indispensable resource for learning from your peers and seeking help from tutors and instructors.\nIf you have a question for staff, create a new post and mark it as “private” and it will go only to course staff. This is the best option to contact us if you have a personal concern. If your question does not include personal information and can be answered by other students, make sure it is public.\n\n\nCourse website\nAll of the assignments will be posted to the course website at https://stat20.org. This also holds the course notes, the syllabus, and links to Gradescope, Ed, and RStudio.\n\n\n\nAssignments, Exams, and Grading\nYou will be turning in your assignments on a platform called Gradescope. Generally, you will have:\n\nReading Questions due Wednesdays at 12pm and Thursdays at 12pm\nParticipation you can complete in class on Wednesday and Thursday\nProblem Sets due Thursdays at 11:59pm\nLabs due Mondays at 12pm or a Quiz taken on Monday.\n\nGradescope is the platform where your assignments will be graded, so you can return there to get feedback on your work. You are welcome to file a regrade request if you notice that we made an error in applying the rubric to your work. You will typically have a week after the release of grades to file a request.\n\n\nMore on the assignment types\n\nReading Questions\nReading questions serve to check your understanding and engagement while going through the lecture notes prior to class. They will be due at 12pm on Wednesdays and Thursdays, so before the lectures on those days. There will be a handful of questions per lecture note. These questions be a mix of multiple choice, short answer, and coding questions. You can find them directly on Gradescope.\n\n\nParticipation\nOn Wednesdays and Thursdays, we will be using the service PollEverywhere to record your participation. You will receive full credit as long as you participate in the poll questions on a given day.\n\n\nProblem Sets\nDuring class, we will give you a second engagement with the day’s material in the form of a worksheet. These worksheets will run like traditional homework problems and drill the concepts in the reading notes. The primary purpose of the problem sets are meant to help prepare you for the quizzes and the final exam. They will generally be due on Thursdays at 11:59pm, so that solutions can be released on Saturday.\nThese problem sets are graded on what we call earnest engagement. You will not be graded on accuracy, but you will be expected to show effort on the assignment. You can either receive full credit or no credit. The best way to achieve full credit is to explain your work wherever possible and attempt a fair number of the problems.\nYou may submit as a group of two or alone.\n\n\nAssessments\nThese are the primary assignments in the course. Depending on the topic at hand, you will have either a lab or a quiz which will be due/taken every Monday.\n\nLabs\nLabs are week-long assignments designed to apply the concepts from the lecture notes in the cause of doing an analysis of real data. This will involve both writing code and communicating your thoughts and findings in English. We’ll be working through some problems from the labs in class, but you may have to complete them on your own outside of class time. Some labs will be turned in individually; some will be turned in as groups.\nLabs are individual assignments and are to be submitted as PDF files. These PDFs will be generated by rendering Quarto Documents (.qmd files) to HTML and then exporting the HTML into a PDF. Don’t worry if you’re not familiar with the Quarto Document as we will teach you about it!\n\n\nQuizzes\nQuizzes reinforce the most important concepts from the lecture notes and provide you the opportunity to work through misunderstandings of concepts with peers and the instructor. They also will prepare you for the final exam.\nThere is both an individual and group component to the quiz.\nThe individual component will last ~25 minutes. You are allowed one, A4, one-sided handwritten sheet of notes. The group component will take place immediately after the individual component has been completed and will last ~15 minutes. Groups must be of size 2-3. Your grade for a given quiz will be the average of your group and individual quiz scores. The group component is meant to and overwhelmingly does help students raise their grades; one way to take advantage of this component is to work with a group that you have worked and studied with in class.\nThe current quiz dates are Monday, June 24th, Monday, July 8th and Monday, July 15th.\n\n\n\n\n\n\nFinal Exam\nThe final exam is cumulative (covers all course topics). The time, date, and location is Wednesday, August 7 from 8-11am in Evans 10.\n\n\nGrading\nYour final grade in the course will be computed as follows:\n\nReading Questions: 5%\nParticipation: 5%\nProblem Sets: 20%\nAssessments: 49%\nGetting Started Lab: 1%\nEC Assessment: 1%\nFinal: 20%\n\nWhere applicable, within each group, all assignments are weighted equally.\nThe course grade calculator can be used to see your raw course grade. Make a copy of this calculator to modify it. Please refrain from asking staff questions about how bins will be determined. The historical distribution of letter grades shown for the class on Berkeleytime is usually a good reference for what the final grades will look like.\n\n\nPolicies\n\nAccomodations for students with disabilities\nStat 20 is a course that is designed to allow all students to succeed. If you have letters of accommodations from the Disabled Students’ Program, please share them with your instructor as soon as possible, and we will work out the necessary arrangements.\n\n\nLate Work\nUnfortunately, with a class of this size, we are unable to keep track of and grade submissions of labs, reading questions, and problem sets that come in very late. If you narrowly miss the standard deadline, you will still be able to submit within an hour for a small penalty (5% reduction in score). If you don’t submit within an hour you can still submit for a larger penalty (30% reduction).\n\n\nDrops\nIn order to provide more flexibility around emergencies that might arise for you throughout the semester (for example, missing a quiz due to COVID), we will apply for everyone:\n\none emergency drop for quizzes\n\none emergency drop for labs\ntwo emergency drops for problem sets\ntwo emergency drops for participation\ntwo emergency drops for reading questions\n\nThis means, for example, that we will drop your lowest quiz score before computing your average score across all quizzes.\nThe calculator linked in an above section does not account for drops, so if you would like, you can readjust the cells in your own copy of the calculator to reflect your particular circumstance!\n\n\n\nCollaboration policy\nYou are encouraged to collaborate with your fellow students on problem sets and labs, but the work you turn in should reflect your own understanding and all of your collaborators must be cited. The individual component of quizzes, reading questions, and exams must reflect only your work.\nResearchers don’t use one another’s research without permission; scholars and students always use proper citations in papers; professors may not circulate or publish student papers without the writer’s permission; and students may not circulate or post non-public materials (quizzes, exams, rubrics-any private class materials) from their class without the written permission of the instructor.\nThe general rule: you must not submit assignments that reflect the work of others unless they are a cited collaborator.\nThe following examples of collaboration are allowed and in fact encouraged!\n\nDiscussing how to solve a problem with a classmate.\nShowing your code to a classmate along with an error message or confusing output.\nPosting snippets of your code to the discussion forum when seeking help.\nHelping other students solve questions on the discussion with conceptual pointers or snippets of code that doesn’t whole hog give away the answer.\nGoogling the text of an error message.\nCopying small snippets of code from answers on Stack Overflow.\n\nThe following examples are not allowed:\n\nLeaving a representation of your assignment (the text, a screenshot) where students (current and future) can access it. Examples of this include websites like course hero, on a group text chain, over discord/slack, or in a file passed on to future students.\nAccessing and submitting solutions to assignments from other students distributed as above. This includes copying written answers from other students and slightly modifying the language to differentiate it.\nGoogling for complete problem solutions.\nWorking on the reading questions or individual quizzes in collaboration with other people or resources. These assignments must reflect individual work.\nSubmitting work on an quiz or final that reflects consultation with outside resources or other people. These assessments must reflect individual work.\n\nIf you have questions about the boundaries of the policy, please ask. We’re always happy to clarify.\n\nViolations of the collaboration policy\nThe integrity of our course depends on our ability to ensure that students do not violate the collaboration policy. We take this responsibility seriously and forward cases of academic misconduct to the Center for Student Conduct.\nStudents determined to have violated the academic misconduct policy by the Center for Student Conduct will receive a grade penalty in the course and a sanction from the university which is generally: (i) First violation: Non-Reportable Warning and educational intervention, (ii) Second violation: Suspension/Disciplinary Probation and educational interventions, (iii) Third violation: Dismissal.\nAnd again, if you have questions about the boundaries of the collaboration policy, please ask!\n\n\n\nFrequently Asked Questions\n\nWhat should I do if I’m on the waitlist?\nAttend both lecture and section (remember, we are teaching it as one two hour class), and submit all assignments on time. Visit your instructor on the first day of class so you can be added to the course Ed and Gradescope.\nAre class sessions recorded?\nNo. Class sessions feature a mix of group problem solving, activities, and discussion and don’t lend themselves to recording. The course notes are the main reference source for the course. Any materials used during the class session will be posted to the course website.\nIs attendance required?\nNo, but it is difficult to succeed in this course if you are not regularly attending class. Class sessions are designed to be an effective and efficient format to make progress on important assignments. Plus, it’s a great way to meet your fellow students and learn from one another.\nIf you can’t attend due to a religious observance, athletic competition, or something similarly important, don’t worry. Just reach out to us via a private Ed post, and we can let you know what to keep tabs on during the time you’re away.\nAre time conflicts allowed?\n\nStat 20 does not allow students to enroll with time conflicts.\n\nWhat if I join the class late?\nIf you join the class within the first two weeks, read the syllabus and lecture notes, take a look at Gradescope to get a sense of any assignments that may have already passed, and visit office hours to check that you’re up to date with things. The first two weeks of material are very important so you must be able to make up some assignments.\nAfter two weeks into the semester, you’ll have too much material that you’ll need to make up, so you will have to wait to a subsequent semester to take Stat 20.\n\n\n\nDon’t come to class if you’re sick!\nMaintaining your health and that of the Berkeley community is of primary importance to course staff, so if you are feeling ill or have been exposed to illness, whether it’s COVID-19 or something else, please do not come to class. All of the materials used in class will be posted to the course website. You’re encouraged to reach out to fellow students to discuss the class materials or stop by group tutoring or office hours to chat with a tutor or the instructor.\n\n\nCampus Resources\nIf you ever need someone to talk to about anything that you’re going through, please feel to reach out to the instructors. For some topics, the tutors might be an even better resource because they are students just like you. Tutors can also tell you what being an Academic Student Employee (ASE) is like.\nWith regards to reports of sexual misconduct/violence/assault, you may speak with us as well, but know that we will need to report our discussion to the Title IX officer. This is detailed below.\n\nAs UC employees, the instructors (and tutors) are “Responsible Employees” and are therefore required to report incidents of sexual violence, sexual harassment, or other conduct prohibited by University policy to the Title IX officer. We cannot keep reports of sexual harassment or sexual violence confidential, but the Title IX officer will consider requests for confidentiality. Note that there are confidential resources available to you through UCB’s PATH to Care Center, which serves survivors of sexual violence and sexual harassment; call their 24/7 Care Line at 510-643-2005.\n\nBelow are some campus resources that may be helpful for you:\n\nBerkeley Supportal- great place to start\nDisabled Students’ Program\nUniversity Health Services\nUCB Path to Care\nStudent Learning Center\nBerkeley International Office\nOmbuds Office for Students and Postdoctoral Appointees\nGender Equity Resource Center\nCenter for Educational Justice & Community Engagement\nUHS Counseling and Psychological Services (CAPS)\nCampus Academic Accommodations Hub\nASUC Student Advocate’s Office\nBasic Needs Center\nASUC Mental Health Resources Guide"
  },
  {
    "objectID": "glossary-fns.html",
    "href": "glossary-fns.html",
    "title": "Functions",
    "section": "",
    "text": "Questions and Data"
  },
  {
    "objectID": "3-probability/labs/04-elections/lab.html",
    "href": "3-probability/labs/04-elections/lab.html",
    "title": "Lab 3: Elections",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "3-probability/labs/04-elections/lab.html#part-i-understanding-the-context-of-the-data",
    "href": "3-probability/labs/04-elections/lab.html#part-i-understanding-the-context-of-the-data",
    "title": "Lab 3: Elections",
    "section": "Part I: Understanding the Context of the Data",
    "text": "Part I: Understanding the Context of the Data\nPlease record your answers on the worksheet below and upload it to Gradescope.\n\nPart 1: Understanding the Context of the Data\n\nTo help answer this worksheet, consult this snapshot of the original dataset found here."
  },
  {
    "objectID": "3-probability/labs/04-elections/lab.html#part-ii-computing-on-the-data",
    "href": "3-probability/labs/04-elections/lab.html#part-ii-computing-on-the-data",
    "title": "Lab 3: Elections",
    "section": "Part II: Computing on the Data",
    "text": "Part II: Computing on the Data\nQuestion 1\nWhat is the empirical distribution of the vote counts for Ahmadinejad? Answer with:\n\na plot (label your axes and provide a title),\nnumerical summaries of center and spread,\nand a written interpretation.\nQuestion 2\nCreate two vectors:\n\none with the range of values that the Benford’s Law probability distribution can take\nand the second with the corresponding probabilities for each value.\nQuestion 3\nWhat might 366 draws (the amount of rows in the iran dataframe) from \\(X \\sim Benford()\\) look like? Find out by sampling from the \\(Benford\\) probability distribution. Create a plot of the resulting empirical distribution that you collect. Label your axes and title your plot Benford’s Law Simulation.\nQuestion 4\nWhat do the first digit empirical distributions look like for the four candidates in the Iranian presidential election?\n\nMake one plot for each distribution and title them by candidate name.\nCombine the four plots into a single visualization using the patchwork library.\n\nInside the stat20data package there is a function called get_first() that pulls off the first digit of every element in a vector. This will be helpful when creating your plots.\nQuestion 5\nHow do the observed first digit distributions of Question 4 compare to the one you created in Question 3 by sampling from Benford’s Law? Which candidate has a first-digit distribution that is:\n\nmost similar to\nmost different\n\nfrom the sampled one?\nQuestion 6\nBelow are two visualizations using data from the last two presidential elections in California. The last two Democratic candidates for president were Joe Biden in 2020 and Hillary Clinton in 2016.\n\n\n\n\n\n\n\n\nBased on these results and the results from Question 5, to what extent to you believe the 2009 Iranian election was fraudulent? Explain in three to four sentences."
  },
  {
    "objectID": "3-probability/labs/04-elections/lab-context.html",
    "href": "3-probability/labs/04-elections/lab-context.html",
    "title": "Stat 20",
    "section": "",
    "text": "What is the unit of observation in the dataset shown in the screengrab?\n\n\n\n\nSketch a plot of the distribution of the vote counts for Ahmadinejad. Label your axes and title your plot with a claim about the shape and modality of the data. Depict a shape which reflects your expectation of the phenomenon.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSketch a plot of the distribution of the first digit of the vote counts for Ahmadinejad. Label your axes and title your plot with a claim about the shape and modality of the data. Depict a shape which reflects your expectation of the phenomenon.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet \\(X\\) be a random variable which represents the first digit of vote tallies.\n\nIf you had no inclination as to what the probability distribution of first digits of vote counts would look like, what distribution might you assign \\(X\\)?\n\n\nState a distribution (and the values which it can take).\nWrite down the probability mass function of this distribution.\n\n\nSketch a probability histogram which describes the probability distribution of \\(X\\). Label your axes.\n\n\n\n\n\n\n\n\nThen calculate \\(\\mathbb{E}(X)\\) and \\(Var(X)\\) for a random variable \\(X\\) having the distribution you assigned it in the previous question.\n\n\n\n\n\n\n\nOne common theory on how to determine whether an election is fair is as follows:\n\nIn a normally occurring, fair election, the first digit of the vote counts for each voting precinct should follow Benford’s Law. If they do not, that might suggest that vote counts have been manually altered.\n\nBenford’s Law is not any universal, binding statute but actually a probability distribution on the set \\(\\{1,2,3,4,5,6,7,8,9\\}\\). Let \\(Y\\) be a random variable following the Benford’s Law probability distribution. Then:\n\\[\nf(y) = log_{10}(1 + \\frac{1}{y})\n\\]\n\nSketch an (approximate) probability histogram which describes the probability distribution of \\(Y\\). Label your axes.\n\n\n\n\n\n\n\n\nUse the formula to calculate \\(\\mathbb{E}(Y)\\).\n\n\n\n\n\n\nUse the formula to calculate \\(Var(Y)\\)."
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/handout-soln.html",
    "href": "3-probability/02-cond-prob-indep/handout-soln.html",
    "title": "Probability practice (subset of last handout)",
    "section": "",
    "text": "Question 1\nIn the notes, one of the examples is about dice, and sets up a box model for the process of rolling a pair of fair six sided dice and summing the spots. In the notes we make 2 draws at random with replacement from the following box, and sum the draws:\n\\[\\fbox{ \\fbox{1} \\fbox{2} \\fbox{3} \\fbox{4}  \\fbox{5}  \\fbox{6}}\\]\nWhy not make one draw from the following box instead? (The tickets are the possible sums when we roll a pair of six sided dice.)\n\\[\\fbox{ \\fbox{2} \\fbox{3} \\fbox{4}  \\fbox{5}  \\fbox{6}  \\fbox{7}  \\fbox{8}  \\fbox{9}  \\fbox{10}  \\fbox{11}  \\fbox{12}}\\]\nIf we did indeed want to model the result of two die rolls with a box from which we would only draw once, what would the box be? (List all the tickets in the box, which numbers, and how many of each.)\n\nSolution\nThe second box doesn’t work because the tickets won’t be equally likely if it is to represent the outcome space of rolling a pair of dice. The correct box to use would be one that reflects the probabilities of the outcomes. For example the chance of the sum being 2 is 1 in 36, but the chance of the sum being 7 is 6 in 36. The tickets in the box have to reflect these unequal chances. The correct box, from which we can draw one ticket representing the outcome of the sum of spots of a pair of dice would be: \\[\\fbox{ 1 \\fbox{2} 2 \\fbox{3} 3 \\fbox{4} 4 \\fbox{5}  5 \\fbox{6}  6 \\fbox{7}  5 \\fbox{8}  4 \\fbox{9} 3 \\fbox{10} 2 \\fbox{11}  1 \\fbox{12}}\\]\n\n\n\nQuestion 2\nIf we are setting up a box for modeling the number of heads in three tosses of a fair coin, would either of the boxes below work? If not, why not?\n\n\\(\\fbox{\\fbox{0} \\fbox{1}}\\) - Draw three times at random with replacement, and sum the draws.\n\\(\\fbox{\\fbox{0} \\fbox{1} \\fbox{2} \\fbox{3}}\\) - Draw once, the result is the number of heads.\n\n\nSolution\nThe second one does not work, for the same reason as the first one above.\n\n\n\nQuestion 3\nA standard deck of cards has 52 cards, consisting of 4 “suits” (hearts , diamonds , spades \\(\\spadesuit\\), and clubs \\(\\clubsuit\\)). Each suit has 13 cards (Ace, King, Queen, Jack, 2, 3, 4, 5, 6, 7, 8, 9, and 10). Half of the cards are red (hearts and diamonds) and half of the cards are black (spades and clubs). I would like to shuffle the cards, draw five cards with replacement, and count the number of hearts in these five cards. What would be the box model for this set up?\n\n\nQuestion 4\nOne ticket will be drawn at random from each of the two boxes below:\n\\(A: \\fbox{\\fbox{1} \\fbox{2} \\fbox{3}}\\)\n\\(B: \\fbox{ \\fbox{1} \\fbox{2} \\fbox{3} \\fbox{4}}\\)\n\nWhat is the chance the number drawn from \\(A\\) is greater than the one drawn from \\(B\\)?\nWhat is the chance that the number drawn from \\(A\\) is equal to the one drawn from \\(B\\)?\nWhat is the chance the number drawn from \\(A\\) is smaller than the one drawn from \\(B\\)?\n\n\n\nQuestion 5\nI want to estimate the proportion of people in Berkeley who speak at least two languages. I stand at the corner of University and Shattuck and ask each person who goes by how many languages they speak, and keep a count of how many speak at least two. Can I set this up as a box model? If so, how? If not, why not?\n\n\nQuestion 6\nThe company Chegg wants to justify its business model and decides to conduct an opinion poll. It sends out a survey to its 7.8 million subscribers (as of 2021) asking if they have ever cheated on an assignment using Chegg. It gets back 1.5 million responses of which 87% of the respondents say that they have never used Chegg to cheat on an assignment.\n\nCould this survey be set up as a box model? If so, what tickets would go in the box and how many would we draw? If not, why not?\nIs this sample of 1.5 million a representative sample of high school and college students?"
  },
  {
    "objectID": "3-probability/02-cond-prob-indep/ps.html",
    "href": "3-probability/02-cond-prob-indep/ps.html",
    "title": "Computing Probabilities",
    "section": "",
    "text": "Consider picking numbers from the following box.\nLet \\(A\\) be the event that the first pick yields an even number; \\(B\\) be the event that the second pick is greater than or equal to one.\n\nPick two numbers without replacement. Find \\(\\text{P}(B|\\text{first pick is 0})\\).\n\n\n\n\nPick two numbers without replacement. Find \\(\\text{P}(B|\\text{first pick is 2})\\).\n\n\n\n\nPick two numbers with replacement. Find \\(\\text{P}(B|A)\\).\n\n\n\nConsider a fair, eight-sided die.\n\nI roll the die four times. What is the probability that I roll the same number on all four rolls?\n\n\n\n\n\n\nI roll the die twice. What is the probability that the rolls are different?\n\n\n\n\n\n\nMy dog Bella has two toys that she loves: an orange ball, and a thick rope. Each time she picks out a toy, she chooses it independently of all the other times (like a coin toss). That day, she was busy, so went to her toys three times.\nDefine the events \\(A\\) and \\(B\\) where:\n\\(A\\) is the event that she picked the rope at most one time;\n\\(B\\) is the event that the toys she picked that day included both the rope and the ball.\n\nAre \\(A\\) and \\(B\\) independent?\n\nAn American roulette wheel has 38 pockets, of which 18 are red, 18 black, and 2 are green. In each round, the wheel is spun and a white ball lands in one of these 38 pockets.\n\nWhat is the probability of getting at the ball landing in a green pocket at least once in 5 spins of the wheel?\n\n\n\n\n\nA European roulette wheel has 37 pockets, of which 18 are red, 18 black, and only 1 green. The roulette wheel is numbered 0 through 36.\n\nWrite R code to simulate three spins of this wheel.\n\n\n\n\nNow imagine that after each of the three spins, a pocket disappears. Simulate three spins of this magic wheel.\n\n\n\nWe will now perform our first simulation of the year! For the following questions, consider the European roulette wheel of Question 7 and ensure your Quarto document will present the same results each time it is rendered. Write your code in the spaces below.\n\nCreate three vectors: one which contains 100 simulated spins of the European roulette wheel (call this one_hundred), one which contains 1,000 such spins (call this one_thousand), and another which contains 10,000 such spins (call this ten_thousand).\n\n\n\n\n\n\nCreate a new vector that returns TRUE/FALSE values for each element in one_hundred, where TRUE means that the number spun is greater than 18, and save it. Repeat these steps for the one_thousand and ten_thousand vectors.\n\n\n\n\n\n\nFind the proportion of numbers spun in each simulation that were greater than 18 (write the code and the proportion). Hint: how can you take a proportion of a logical vector?\n\n\n\n\n\n\nComment on how the proportions changed with respect to the true probability of spinning a number greater than 18 as the number of spins increased."
  },
  {
    "objectID": "3-probability/03-probability-dsns/slides.html",
    "href": "3-probability/03-probability-dsns/slides.html",
    "title": "Probability Distributions",
    "section": "",
    "text": "Announcements\nProbability refresher (last week)\nBreak\nConcept questions: Probability Distributions\nPS 8: Probability Distributions"
  },
  {
    "objectID": "3-probability/03-probability-dsns/slides.html#agenda",
    "href": "3-probability/03-probability-dsns/slides.html#agenda",
    "title": "Probability Distributions",
    "section": "",
    "text": "Announcements\nProbability refresher (last week)\nBreak\nConcept questions: Probability Distributions\nPS 8: Probability Distributions"
  },
  {
    "objectID": "3-probability/03-probability-dsns/slides.html#announcements",
    "href": "3-probability/03-probability-dsns/slides.html#announcements",
    "title": "Probability Distributions",
    "section": "Announcements",
    "text": "Announcements\n\nNo lab today\n\n. . .\n\nProblem Set 8 (paper max. 3) due next Tuesday at 9am\n\n. . .\n\nRQ: Random Variables due Wednesday 11:59pm\n\n\nYou may need to review the concepts as I said in slack, depending on how they do in the pe refresher"
  },
  {
    "objectID": "3-probability/03-probability-dsns/slides.html#section",
    "href": "3-probability/03-probability-dsns/slides.html#section",
    "title": "Probability Distributions",
    "section": "",
    "text": "Codecountdown::countdown(2, top = 0)\n\n\n\n−&plus;\n\n02:00\n\n\n\nConsider the box of tickets shown below.\n\n\n\n\nThe plots below show:\n\nThe probability histogram for the value of a ticket drawn at random from the box\nAn empirical histogram for which the data were generated by drawing 10 tickets from the box with replacement\nAn empirical histogram for which the data were generated by drawing 100 tickets from the box with replacement\nAn empirical histogram generated by 20 draws from a different box.\n\n\nCodebox &lt;-  c(rep(0,4), 3,3,5,6,7,7)\nset.seed(12345)\n\np1 &lt;- data.frame(box) %&gt;%\n   group_by(box) %&gt;% \n   summarise(probs = n()/10) %&gt;% \n  ggplot(aes(x=factor(box), y = probs)) +\n  geom_col(width = 0.98, fill = \"deeppink3\") + \n  theme_bw() + \n  xlab(\"box tickets\") +\n  ylab(\"\") + \n  annotate(\"text\", x = 3, y = 0.35, label = \"(B)\", size = 5) +\n  ylim(c(0,0.4))\n\nsamp1 &lt;- sample(box,10, replace = TRUE )\n\np2 &lt;- data.frame(samp1) %&gt;%\n  group_by(samp1) %&gt;% \n  summarise(probs = n()/10) %&gt;% \n  ggplot(aes(x=factor(samp1), y = probs)) +\n  geom_col(width = 0.98, fill = \"darkorchid4\") + \n  theme_bw() + \n  xlab(\"box tickets\") +\n  ylab(\"\") + \n  annotate(\"text\", x = 3, y = 0.35, label = \"(D)\", size = 5) +\n  ylim(c(0,0.4))\n\nsamp2 &lt;- sample(box,100, replace = TRUE )\n\np3 &lt;- data.frame(samp2) %&gt;%\n  group_by(samp2) %&gt;% \n  summarise(probs = n()/100) %&gt;% \n  ggplot(aes(x=factor(samp2), y = probs)) +\n  geom_col(width = 0.98, fill = \"deepskyblue3\") + \n  theme_bw() + \n  xlab(\"box tickets\") +\n  ylab(\"\") + \n  annotate(\"text\", x = 3, y = 0.35, label = \"(C)\", size = 5) +\n  ylim(c(0,0.4))\n\nbox2 &lt;- c(0,0,0,3,5,5,6,6,7,7)\n\nset.seed(12345)\nsamp3 &lt;- sample(box2,20, replace = TRUE )\n\np4 &lt;- data.frame(samp3) %&gt;%\n  group_by(samp3) %&gt;% \n  summarise(probs = n()/20) %&gt;% \n  ggplot(aes(x=factor(samp3), y = probs)) +\n  geom_col(width = 0.98, fill = \"darkorange3\") + \n  theme_bw() + \n  xlab(\"box tickets\") +\n  ylab(\"\") + \n  annotate(\"text\", x = 2, y = 0.35, label = \"(A)\", size = 5) +\n  ylim(c(0,0.4))\n\n(p4 + p1)/(p3 + p2)\n\n\n\n\n\n\n\n\nIdentify which is which by matching the letters to the numbers.\n\n\nSince you have to scroll, maybe draw the box on the board. Scroll to see graphs\n\nB-1 the prob dsn of tickets\nD-2 emp dsn of 10 draws\nC-3 emp dsn of 100 draws\nA-4 emp dsn from different box"
  },
  {
    "objectID": "3-probability/03-probability-dsns/slides.html#section-1",
    "href": "3-probability/03-probability-dsns/slides.html#section-1",
    "title": "Probability Distributions",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nDoes the quantity described below have a probability distribution that is binomial (if so, what are \\(n\\) and \\(p\\)?), hypergeometric (if so , what are \\(N\\), \\(G\\), and \\(n\\)?), or neither (why)?\n\nRoll a fair ten-sided die 20 times. We count the number of times we roll a multiple of 3.\n\n\nBinomial, hypergeometric, or neither?\n\n\nBinomial(20, 3/10). The pollev will have 4 choices, and then figuring out the parameters can be a classroom discussion. The fourth choice is “Need more info”"
  },
  {
    "objectID": "3-probability/03-probability-dsns/slides.html#section-2",
    "href": "3-probability/03-probability-dsns/slides.html#section-2",
    "title": "Probability Distributions",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nDoes the quantity described below have a probability distribution that is binomial (if so, what are \\(n\\) and \\(p\\)?), hypergeometric (if so , what are \\(N\\), \\(G\\), and \\(n\\)?), or neither (why)?\n\nYouGov surveyed 1,500 adult US citizens last December and counted the number of respondents who had read at least one book in 2023. The population of the US is about 335 million.\n\nBinomial, hypergeometric, or neither?\n\n\nHypergeometric(\\(N = 335\\) mill, \\(n = 1500\\), G), where G is not known, being the number of people who have read at least one book . Can remark that we could use the binomial to model this rather than HG, and that we will discuss sample surveys in this unit. Disturbingly, only 54% of respondents had read at least one book. Here is the link: https://today.yougov.com/entertainment/articles/48239-54-percent-of-americans-read-a-book-this-year"
  },
  {
    "objectID": "3-probability/03-probability-dsns/slides.html#section-3",
    "href": "3-probability/03-probability-dsns/slides.html#section-3",
    "title": "Probability Distributions",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nAre we describing a probability distribution below that is binomial? Or is it hypergeometric? Or do neither apply?\n\nA six-sided die is tossed twice, and we check if the sum of the spots is 8 or not.\n\n\nBinomial, hypergeometric, or neither?\n\n\nBinomial, with \\(n = 1, p = 5/36\\) or Bernoulli with \\(p = 5/36\\)."
  },
  {
    "objectID": "3-probability/03-probability-dsns/slides.html#section-4",
    "href": "3-probability/03-probability-dsns/slides.html#section-4",
    "title": "Probability Distributions",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nAre we describing a probability distribution below that is binomial? Or is it hypergeometric? Or do neither apply?\nA bag that has 6 pieces of fruit: 2 mangoes, 3 apples, and 1 orange. I reach into the bag and draw out one fruit at a time, selecting each fruit at random (so they are equally likely on each draw). I count the number of draws until and including the first time I draw a apple.\n\nBinomial, hypergeometric, or neither?\n\n\nNeither. It would be useful at this point to write out what is going on, and point out that they are using the multiplication rule and a conditional probability."
  },
  {
    "objectID": "3-probability/04-random-variables/notes.html",
    "href": "3-probability/04-random-variables/notes.html",
    "title": "Random Variables",
    "section": "",
    "text": "In the last chapter, we saw that if we want to simulate tossing a fair coin \\(10\\) times, and compute the proportion of times that the coin lands heads, we could use the function sample() to sample from (0, 1), where the outcome “Heads” is represented by the number 1 and the outcome “Tails” is represented by the number 0. Our code might look like:\n[1] 0 1 0 0 1 0 0 0 1 1\n\n\n[1] 0.4\nWe used the same idea - of assigning numbers to outcomes when we looked at the probability distribution of the number of heads in three tosses, and when we defined the Bernoulli, binomial, and hypergeometric distributions."
  },
  {
    "objectID": "3-probability/04-random-variables/notes.html#random-variables",
    "href": "3-probability/04-random-variables/notes.html#random-variables",
    "title": "Random Variables",
    "section": "Random variables",
    "text": "Random variables\nIn each of the above scenarios, we had an outcome space \\(\\Omega\\), and then defined a function that assigned a real number to each possible outcome in \\(\\Omega\\). In our simulation above, if \\(\\Omega\\) is the set of outcomes \\(\\{\\text{``Heads'', ``Tails''}\\}\\), we assigned the outcome \\(\\text{``Heads''}\\) to the real number \\(1\\), and the outcome \\(\\text{``Tails''}\\) to the real number \\(0\\). By sampling over and over again from (0,1), we got a sequence of \\(0\\)’s and \\(1\\)’s that was randomly generated by our sampling. Once we had numbers (instead of sequences of heads and tails), we were able to do operations using these numbers, such as compute the proportion of times we sampled \\(1\\), or represent the probabilities as a histogram. Moving from non-numeric outcomes in an outcome space \\(\\Omega\\) to numbers on the real line is enormously useful.\nIn mathematical notation:\n\\[ X : \\Omega \\rightarrow \\mathbb{R}\\]\n\\(X\\) is called a random variable: variable, because it takes different values on the real line, and random, because it inherits the randomness from the generating process (in this case, the process is tossing a coin).\n\n\nRandom variable\n\nA random variable is a function that associates real numbers with outcomes from a random experiment where \\(\\Omega\\) is the associated outcome space.\n\n\n\nThe values on the real line that are determined by \\(X\\) have probabilities coming from the probability distribution on \\(\\Omega\\). The range of the random variable \\(X\\) is the set of all the possible values that \\(X\\) can take. We usually denote random variables by \\(X, Y, \\ldots\\) or capital letters towards the end of the alphabet. We write statements about the values \\(X\\) takes, such as \\(X = 1\\) or \\(X = 0\\). Note that \\(X = 1\\) is an event in \\(\\Omega\\) consisting of all the outcomes that are mapped to the number \\(1\\) on the real line. The probability of such events is written as \\(P(X = x)\\), where \\(x\\) is a real number.\nNote that we are just formalizing the association of outcomes in \\(\\Omega\\) with numbers - an association we have seen before, while defining probability distributions. Earlier we defined probability distributions as how the total probability of \\(1\\) or \\(100\\)% was distributed among all possible outcomes of the random experiment. Now we can extend this definition to the associated real numbers as defined by \\(X\\).\n\n\nProbability distribution of a random variable \\(X\\)\n\nThe set of possible values of \\(X\\), along with the associated probabilities, is called the probability distribution for the random variable \\(X\\).\n\n\n\nFor example, consider the familiar example of the outcome space of three tosses of a fair coin. We can define the random variable \\(X\\) to be the number of heads, and represent it as in the picture below:\n\n\n\n\nJust as with data types, random variables can be classified as discrete or continuous:\n\n\nDiscrete and continuous random variables\n\nDiscrete random variables are restricted to take particular values in an interval, they cannot take just any value, as opposed to continuous random variables which can take any value in some specified interval."
  },
  {
    "objectID": "3-probability/04-random-variables/notes.html#examples-of-random-variables",
    "href": "3-probability/04-random-variables/notes.html#examples-of-random-variables",
    "title": "Random Variables",
    "section": "Examples of random variables",
    "text": "Examples of random variables\n\nDiscrete random variables\n\nThe number of heads in \\(3\\) tosses of a fair coin: The assignment is similar to the outcomes from a single toss, except now we have the possible outcome from tossing a coin three times. For example, the outcome \\(HHH\\) is assigned the number 3, the outcomes \\(HHT, HTH, THH\\) are all assigned the number 2 etc. Note that even though we should write \\(X(HHH) = 3\\), we don’t. The practice is to ignore the outcome space and just write \\(X = 3\\).\nThe number of tosses until the coin lands heads for the first time: If \\(X\\) is the random variable representing the number of tosses until a coin lands heads, the smallest value \\(X\\) can take is 1 (you need at least 1 toss), and there is no upper bound, since in theory, one could keep tossing the coin forever and it could land tails every single time.\nThe number of people that arrive at an ATM in a day\nThe number of people in the United States who will have read at least one book in 2024\nThe number of typos in the Stat 20 notes\n\nContinuous random variables\nIn all of the following, we do not restrict the value taken by the random variable.\n\nTime between consecutive people arriving at an ATM\nPrice of a stock\nHeight of a randomly selected stat 20 student\nThe weight of a randomly selected newborn baby in California\nThe amount of rain that falls each March in the Western United States"
  },
  {
    "objectID": "3-probability/04-random-variables/notes.html#example-making-bets-on-red-in-roulette",
    "href": "3-probability/04-random-variables/notes.html#example-making-bets-on-red-in-roulette",
    "title": "Random Variables",
    "section": "Example: Making bets on red in Roulette",
    "text": "Example: Making bets on red in Roulette\n\n\n\n\nRecall that American roulette wheels have \\(38\\) numbered slots, numbered from \\(1\\) to \\(36\\), of which \\(18\\) are colored red, and \\(18\\) black. There are two green slots numbered \\(0\\) and a \\(00\\). As the wheel spins, a ball is sent spinning in the opposite direction. When the wheel slows the ball will land in one of the numbered slots. Players can make various bets on where the ball lands, such as betting on whether the ball will land in a red slot or a black slot. If a player bets one dollar on red, and the ball lands on red, then they win a dollar, in addition to getting their stake of one dollar back. If the ball does not land on red, then they lose their dollar to the casino. Suppose a player bets six times on six consecutive spins, betting on red each time. Their net gain can be defined as the amount they won minus the amount they lost. Is net gain a random variable? What are its possible values (write down how much the player can win or lose in one spin of the wheel, then two, and so on)?\n\nCheck your answer\nYes, net gain is a random variable, and its possible values are: \\(-6, -4, -2, 0, 2, 4, 6\\). (Why?)"
  },
  {
    "objectID": "3-probability/04-random-variables/notes.html#the-probability-distribution-of-a-discrete-random-variable-x",
    "href": "3-probability/04-random-variables/notes.html#the-probability-distribution-of-a-discrete-random-variable-x",
    "title": "Random Variables",
    "section": "The probability distribution of a discrete random variable \\(X\\)\n",
    "text": "The probability distribution of a discrete random variable \\(X\\)\n\nThe list of probabilities associated with each of its values is called the probability distribution of the random variable \\(X\\). We can list the values and corresponding probability in a table. This table is called the distribution table of the random variable. For example, let \\(X\\) be the number of heads in \\(3\\) tosses of a fair coin. The probability distribution table for \\(X\\) is shown below. The first column should have the possible values that \\(X\\) can take, denoted by \\(x\\), and the second column should have \\(P(X = x)\\). Make sure that the probabilities add up to 1! \\(\\displaystyle \\sum_x P(X = x) = 1\\).\n\n\n\\(x\\)\n\\(P(X = x)\\)\n\n\n\n\\(0\\)\n\\(\\displaystyle \\frac{1}{8}\\)\n\n\n\\(1\\)\n\\(\\displaystyle \\frac{3}{8}\\)\n\n\n\\(2\\)\n\\(\\displaystyle \\frac{3}{8}\\)\n\n\n\\(3\\)\n\\(\\displaystyle \\frac{1}{8}\\)\n\n\n\nThe probability mass function or pmf of a discrete random variable\n\n\nProbability mass function (pmf) of a discrete random variable \\(X\\)\n\nThe pmf of a discrete random variable \\(X\\) is defined to be the function \\(f(x) = P(X = x)\\).\n\n\n\nWe can write down the definition of the function \\(f(x)\\) and it gives the same information as in the table:\n\\[\nf(x) = \\begin{cases}\n          \\frac{1}{8}, \\; x = 0, 3 \\\\\n          \\frac{3}{8}, \\; x = 1, 2\n  \\end{cases}\n\\]\nWe see here that \\(f(x) &gt; 0\\) for only \\(4\\) real numbers, and is \\(0\\) otherwise. We can think of the total probability mass as \\(1\\), and \\(f(x)\\) describes how this mass of \\(1\\) is distributed among the real numbers. It is often easier and more compact to define the probability distribution of \\(X\\) using \\(f\\) rather than the table.\nLet’s revisit the special distributions that we have seen so far."
  },
  {
    "objectID": "3-probability/04-random-variables/notes.html#some-special-discrete-random-variables-and-their-distributions",
    "href": "3-probability/04-random-variables/notes.html#some-special-discrete-random-variables-and-their-distributions",
    "title": "Random Variables",
    "section": "Some special discrete random variables and their distributions",
    "text": "Some special discrete random variables and their distributions\nFor each of the named distributions that we defined earlier, we can define a random variable with that probability distribution.\nThe Discrete Uniform Distribution\nLet \\(X\\) take the values \\(1, 2, 3, \\ldots, n\\) with \\(P(X = k) = \\displaystyle \\frac{1}{n}\\) for each of the \\(k\\) from \\(1\\) to \\(n\\). We call \\(X\\) a discrete uniform random variable with \\(P(X = k) = \\displaystyle \\frac{1}{n}\\) for \\(1 \\le k \\le n\\). Recall that \\(n\\) is the parameter of the discrete uniform distribution, and we write \\(X \\sim\\) Discrete Uniform\\((n)\\).\nExample: Rolling a pair of dice and summing the spots\nSuppose we roll a pair of dice and sum the spots, and let \\(X\\) be the sum. Is \\(X\\) a discrete uniform random variable?\n\nCheck your answer\nNo. \\(X\\) takes discrete values: \\(2, 3, 4, \\ldots, 12\\), but these are not equally likely.\nThe Bernoulli Distribution\nRecall that the Bernoulli distribution describes the probabilities associated with random binary outcomes that we designate as success and failure, where \\(p\\) is the probability of a success. We can define \\(X\\) be a random variable that takes the value \\(1\\) with probability \\(p\\) and the value \\(0\\) with probability \\(1-p\\), then \\(X\\) is called a Bernoulli random variable, and it indicates whether the outcome of the random experiment or trial was a sucess or not. We say that \\(X\\) is Bernoulli with parameter \\(p\\), and write \\(X \\sim\\) Bernoulli\\((p)\\). Below are the same probability histograms that we have seen in the last chapter, but now they describe the probability mass function of \\(X\\).\n\n\n\n\nThe Binomial Distribution\nRecall that the binomial distribution describes the probabilities of the total number of successes in \\(n\\) independent Bernoulli trials. We let \\(X\\) be this total number of successes (think tossing a coin \\(n\\) times, and counting the number of heads). Then we say that \\(X\\) has the binomial distribution with parameters \\(n\\) and \\(p\\), and write \\(X \\sim Bin(n,p)\\), where \\(X\\) takes the values in \\(\\{0, 1, 2, \\ldots, n\\}\\), and \\[P(X = k ) = \\binom{n}{k} p^k (1-p)^{n-k}. \\] Recall that the binomial coefficient 1 \\(\\displaystyle \\binom{n}{k} = \\frac{n!}{k!(n-k)!}.\\)\nExample: Proportion of likely voters in California that prefer Adam Schiff\nThe top four candidates in the California Senate race will have their final debate on February 20, before the primary elections on March 5, 2024. According to the latest California Elections and Policy Poll, conducted January 21-29, 2024 2, Adam Schiff is currently the favorite candidate in the California Senate primary election, preferred by \\(25\\%\\) of likely voters. The primary elections will decide the top two candidates (regardless of political affiliation) who will compete in the general election.\n\nSuppose we survey \\(10\\) California residents sampling them with replacement from a database of voters, what is the probability that more than \\(2\\) of the individuals in our sample will prefer Adam Schiff over the other candidates for CA Senator?\nIn this example, since we are counting the number of voters in our sample that prefer Adam Schiff, such a voter would count as a “success”, because a success is whatever outcome we are counting (regardless of whom we may prefer). We can now set up our random variable \\(X\\):\nLet \\(X\\) be the number of voters in our sample of ten voters who like Adam Schiff best among all the Senatorial candidates in California.\nThen \\(X \\sim Bin(10, p = 0.25)\\). (Why are these the parameters?) Using the complement rule,\n\\(P(X &gt; 2) = 1 - P(X \\le 2) = 1 - \\left(P(X = 0) + P(X = 1) + P(X = 2)\\right)\\).\nNote that since these events are mutually exclusive, we can use the addition rule. This gives us:\n\\[\n\\begin{aligned}\nP(X &gt; 2) &= 1 - \\left(\\binom{10}{0}(0.25)^0 (0.75)^{10} + \\binom{10}{1}(0.25)^1 (0.75)^9 + \\binom{10}{2}(0.25)^2 (0.75)^8 \\right)\\\\\n& \\approx 0.474\\\\\n\\end{aligned}\n\\] (The symbol \\(\\approx\\) denotes approximately equal.) \nAs stated earlier, we can define events by the values taken by random variables. For example, let \\(X \\sim Bin(10,0.4)\\). In words \\(X\\) counts the number of successes in 10 trials. Given this \\(X\\), what are the following events in words?\n\n\\(X = 5\\)\n\\(X \\le 5\\)\n\\(3 \\le X \\le 8\\)\n\nWhat are these events in words?\n\nCheck your answer\n\\(X\\) is the number of successes in ten trials, where the probability of success in each trial is 40%. \\(X = 5\\) is the event that we see exactly five successes in the ten trials, while \\(X \\le 5\\) is the event of seeing at most five successes in ten trials. The last event, \\(3 \\le X \\le 8\\) is the event of at least three successes, but not more than eight, in ten trials.\n\nThe Hypergeometric Distribution\nIn the example above, we sampled \\(10\\) California residents with replacement from a database of voters. Usually we always sample without replacement, so in a situation such as in the example, where we are define a random variable \\(X\\) to be the number of successes in a simple random sample of \\(n\\) draws from a population of size \\(N\\), then \\(X\\) will have the hypergeometric distribution with parameters \\(\\left(N, G, n\\right)\\) where \\(G\\) is the total number of successes in our population. We write this as \\(X \\sim HG(N, G, n)\\). If we let \\(X\\) be the number of successes in \\(n\\) draws, then we have that \\[ P(X = k) = \\frac{\\binom{G}{k} \\times \\binom{N-G}{n-k}}{\\binom{N}{n}} \\] where \\(N\\) is the size of the population, \\(G\\) is the total number of successes in the population, and \\(n\\) is the sample size (so \\(k\\) can take the values \\(0, 1, \\ldots, n\\) or \\(0, 1, \\ldots, G\\), if the number of successes in the population is smaller than the sample size.)\nExample: Gender discrimination at a large supermarket?\nA large supermarket chain in Florida (with 1,000 employees) occasionally selects employees to receive management training. A group of women there claimed that female employees were passed over for this training in favor of their male colleagues. The company denied this claim. (A similar complaint of gender bias was made about promotions and pay for the 1.6 million women who work or who have worked for Wal-Mart. The Supreme Court heard the case in 2011 and ruled in favor of Wal-Mart, in that it rejected the effort to sue Wal-Mart.)3 If we set this up as a probability problem, we might ask the question of how many women have been selected for executive training in the last 10 years. In order to do the math, we can simplify the problem. Suppose no women had ever been selected in 10 years of annually selecting one employee for training. Further, suppose that the number of men and women were equal, and suppose the company claims that it draws employees at random for the training, from the 1,000 eligible employees. If \\(X\\) is the number of women that have been picked for training in the past 10 years, what is \\(P(X = 0)\\)?\nNote that we are picking a sample of size 10 without replacment and counting the number of women in our sample and therefore, a “success” will be selecting a woman. Modeling \\(X\\) as a hypergeometric random variable, we have that \\(N = 1,000\\), there are 1,000 employees, and since half are women, we have \\(G = N-G = 500\\). We want to compute the probability that out of the \\(10\\) employees that were selected at random from the \\(1,000\\) employees, none are women. This gives us, using the formula above:\n\\[P(X = 0) = \\frac{\\binom{500}{0} \\times \\binom{500}{10}}{\\binom{1000}{10}} \\approx 0.0009\\] We will see later how to compute this probability in R.\n\nThe Poisson Distribution\nThere are three very important distributions that we see over and over again in many situations. One of them is the binomial distribution, which we have discussed above. The reason this distribution is so ubiquitous is that we use it for classifying things into binary outcomes and counting the number of “successes”. The second important discrete distribution is used to model many different things - from the number of people arriving at an ATM in a given period of time, to the frequency with which officers in the Prussian army were accidentally kicked to death by their horses4. This distribution is called the Poisson distribution after a French mathematician, Siméon-Denis Poisson, who developed the theory in the nineteenth century.Interestingly, he was not the first French mathematician to develop this theory. That honor belonged to the seventeenth century mathematician who was a contemporary (and friend) of Isaac Newton, Abraham de Moivre.5 (The third important distribution is called the Normal distribution, also first discovered by de Moivre, which we will introduce later in the course.)\nThe Poisson distribution appears in situations when we have a very large number of trials in which we are checking the occurrence or not of a particular event which has a very low probability. That is, we have a very large number \\(n\\) of Bernoulli trials, which have a very small \\(p\\) or probability of success, such that the product \\(np\\) is not too small or large. We call the number of successes \\(X\\), and it counts the occurrence of events whose counts tend to be small. Note that \\(X \\sim Bin(n, p)\\), and \\[P(X = 0) = \\binom{n}{0}\\times p^0\\times (1-p)^n = (1-p)^n.\\] For large \\(n\\), it turns out that \\((1-p)^n \\approx e^{-\\lambda}\\), where \\(\\lambda = np\\). We won’t derive the distribution here, but we will use the Poisson distribution for random variables that count the number of occurrences of events in a given period of time in when the events result from a very large number of independent trials. The independence of the trials means that the probability of success does not change over time.\n\n\nPoisson distribution\n\nWe say that such a random variable \\(X\\) has the Poisson distribution with parameter \\(\\lambda\\), and write it as \\(X \\sim Poisson(\\lambda)\\), if \\[ P(X = k) = e^{-\\lambda} \\frac{\\lambda^k}{k!},\\] where \\(k = 0, 1, 2, \\ldots\\). That is, the possible values of \\(X\\) are non-negative integers, and since \\(k!\\) grows much faster than \\(\\lambda^k\\), the probability that \\(X\\) takes large values is very small. The parameter \\(\\lambda\\) is called the rate of the distribution, and represents how many “successes” we expect in the given time unit.\n\n\n\nExample: The number of soldiers kicked to death by their horses each year in each corps in the Prussian army\nThis example was made famous by Ladislaus Bortkiewicz in 1898 when he discussed how a Poisson distribution fit the data he obtained from 14 corps in the Prussian cavalry over a period of 20 years. Let’s look at the empirical histogram of the data. Note that death by horse-kicks was quite rare with less than 1 death per corps per year, over the 280 observations. Bortkiewicz recorded the number of deaths per corps per year, and the majority of years had no deaths in any of the corps. Here is the empirical histogram of the data.\n\n\n\n\n\n\n\n\nThis is the shape we want when we look at the distribution of a Poisson random variable, where a very low number of events has a much higher probability than larger numbers, so we have a right-skewed distribution."
  },
  {
    "objectID": "3-probability/04-random-variables/notes.html#binomial-vs-hypergeometric-distributions",
    "href": "3-probability/04-random-variables/notes.html#binomial-vs-hypergeometric-distributions",
    "title": "Random Variables",
    "section": "Binomial vs Hypergeometric distributions",
    "text": "Binomial vs Hypergeometric distributions\nBoth the binomial and the hypergeometric distributions deal with counting the number of successes in a fixed number of trials with binary outcomes. The difference is that for a binomial random variable, the probability of a success stays the same for each trial, and for a hypergeometric random variable, the probability changes with each trial. If we use a box of tickets to describe these random variables, both distributions can be modeled by sampling from boxes with each ticket marked with \\(0\\) or \\(1\\), but for the binomial distribution, we sample \\(n\\) times with replacement and count the number of successes by summing the draws; and for the hypergeometric distribution, we sample \\(n\\) times without replacement, and count the number of successes by summing the draws.\nNote that when the sample size is small relative to the population size, there is not much difference between the probabilities if we use a binomial distribution vs using a hypergeometric distribution. Let’s look at the gender discrimination example again, and pretend that we are sampling with replacement. Now we can use the binomial distribution to compute the chance of never picking a woman. Let \\(X\\) be defined as before, as the number of women selected in \\(10\\) trials.\n\\[\nP(X = 0) = \\binom{10}{0}\\times\\left(\\frac{1}{2}\\right)^0\\times\\left(\\frac{1}{2}\\right)^{10} = 0.0009765625 \\approx 0.00098\n\\] \nRecall that the probability using the hypergeometric distribution was 0.0009331878. You can see that the values are very close to each other. This is an illustration of the fact that we can use a binomial random variable to approximate a hypergeometric random variable if the sample size \\(n\\) is very small compared to the population size \\(N\\).\nNow we will define another important quantity related to random variables. This quantity called the cumulative distribution function is another way to describe the probability distribution of the random variable."
  },
  {
    "objectID": "3-probability/04-random-variables/notes.html#the-cumulative-distribution-function-fx",
    "href": "3-probability/04-random-variables/notes.html#the-cumulative-distribution-function-fx",
    "title": "Random Variables",
    "section": "The cumulative distribution function \\(F(x)\\)\n",
    "text": "The cumulative distribution function \\(F(x)\\)\n\n\n\nCumulative distribution function (cdf) \\(F(x)\\)\n\nThe cumulative distribution function of a random variable \\(X\\) is defined for every real number, and gives, for each \\(x\\), the amount of probability or mass that has been accumulated up to (and including) the point \\(x\\), that is, \\(F(x) = P(X \\le x)\\).\n\n\n\nWe usually abbreviate this function to cdf. It is a very important function since it also describes the probability distribution of \\(X\\). In order to compute \\(F(x)\\) for any real number \\(x\\), we just add up all the probability so far: \\[\nF(x) = \\sum_{y \\le x} f(y)\n\\]\nFor example, if \\(X\\) is the number of heads in \\(3\\) tosses of a fair coin, recall that:\n\\[ f(x) = \\begin{cases}\n  \\displaystyle \\frac{1}{8}, \\; x = 0, 3 \\\\\n  \\displaystyle \\frac{3}{8}, \\; x = 1, 2\n  \\end{cases} \\]\nIn this case, \\(F(x) = P(X\\le x) = 0\\) for all \\(x &lt; 0\\) since the first positive probability is at \\(0\\). Then, \\(F(0) = P(X \\le 0) = 1/8\\) after which it stays at \\(1/8\\) until \\(x = 1\\). Look at the graph below:\n\n\n\n\nNotice that \\(F(x)\\) is a step function, and right continuous. The jumps are at exactly the values for which \\(f(x) &gt; 0\\). We can get \\(F(x)\\) from \\(f(x)\\) by adding the values of \\(f\\) up to and including \\(x\\), and we can get \\(f(x)\\) from \\(F(x)\\) by looking at the size of the jumps.\nExample: Writing down the cdf of a Bernoulli random variable\nSuppose $X $ Bernoulli\\((0.5)\\). Then we know that \\(P(X = 0) = P(X=1) = 0.5\\). The cdf of \\(X\\), \\(F(x)\\) gives us the total probability so far up to and including \\(x\\). For example, if \\(x = -3\\), \\(F(x) =0\\) since the first time there is any positive probability for \\(X\\) is at \\(0\\). At \\(x = 0\\), \\(F(x) = 0.5\\), and it stays there until it gets to \\(x =1\\), when it “accumulates” another \\(0.5\\) of probability. Here is the figure:\n Notice where the function is open and closed (\\(\\circ\\) vs \\(\\bullet\\)).\nExercise: Drawing the graph of the cdf\nLet \\(X\\) be the random variable defined by the distribution table below. Find the cdf of \\(X\\), and draw the graph, making sure to define \\(F(x)\\) for all real numbers \\(x\\). Before you do that, you will have to determine the value of \\(f(x)\\) for \\(x = 4\\).\n\n\n\n\\(x\\)\n\\(P(X = x)\\)\n\n\n\n\\(-1\\)\n\\(0.2\\)\n\n\n\\(1\\)\n\\(0.3\\)\n\n\n\\(2\\)\n\\(0.4\\)\n\n\n\\(4\\)\n??\n\n\n\n\n\nCheck your answer\nSince \\(\\displaystyle \\sum_x P(X = x) = \\sum_x f(x) = 1\\), \\(f(4) = 1-(0.2+0.3+0.4) = 0.1.\\) Therefore \\(F(x)\\) is as shown below."
  },
  {
    "objectID": "3-probability/04-random-variables/notes.html#ideas-in-code",
    "href": "3-probability/04-random-variables/notes.html#ideas-in-code",
    "title": "Random Variables",
    "section": "Ideas in code",
    "text": "Ideas in code\nUseful functions\n\nfactorial() and choose()\n\nThe function used to compute the binomial coefficient \\(\\binom{n}{k}\\) is choose(n,k), while the function to compute \\(n!\\) is factorial(n). Here are a couple of examples:\n\ncat(\" 5! is equal to \", factorial(5) )\n\n 5! is equal to  120\n\n\ncat() prints whatever is in the ().\n\nchoose(4,2)\n\n[1] 6\n\n\nWhile these are both functions that are useful if we want to compute binomial probabilities, we won’t use them since R has built in functions that calculate both \\(f(x)\\) and \\(F(x)\\) for many distributions, including the ones that we have listed in these notes.\nAll these all have the similar forms, and we will list the functions and their arguments here. For each distribution, there are four types of functions and they begin with d, p, r, and q, followed by an abbreviation of the name of the distribution. We will describe the first three types of functions in these notes.\nBernoulli\\((p)\\) and Binomial\\((n,p)\\)\n\n\n\ndbinom computes the pmf of \\(X\\), \\(f(k) = P(X = k)\\), for \\(k = 0, 1, \\ldots, n\\).\n\n\nArguments:\n\n\nx: the value of \\(k\\) in \\(f(k)\\)\n\n\nsize: the parameter \\(n\\), the number of trials\n\nprob: the parameter \\(p\\), the probability of success\n\n\n\n\n\npbinom computes the cdf \\(F(x) = P(X \\le x)\\)\n\n\n\nArguments:\n\n\nq: the value of \\(x\\) in \\(F(x)\\)\n\n\nsize: the parameter \\(n\\), the number of trials\n\nprob: the parameter \\(p\\), the probability of success\n\n\n\n\n\nrbinom generates a sample (random numbers) from the Binomial\\((n,p)\\) distribution.\n\n\nArguments:\n\n\nn: the sample size\n\nsize: the parameter \\(n\\), the number of trials\n\nprob: the parameter \\(p\\), the probability of success\n\n\n\nExample\nSuppose we consider \\(n = 3\\), \\(p= 0.5\\), that is, \\(X\\) is the number of successes in 3 independent Bernoulli trials.\n\n# probability that we see exactly 1 success = f(1)\ndbinom(x = 1, size = 3, prob = 0.5)\n\n[1] 0.375\n\n# probability that we see at most 1 success = F(1) = f(0) + f(1)\npbinom(q = 1, size = 3, prob = 0.5 )\n\n[1] 0.5\n\n# check f(0) + f(1)\ndbinom(x = 0, size = 3, prob = 0.5) + dbinom(x = 1, size = 3, prob = 0.5)\n\n[1] 0.5\n\n# generate a sample of size 5 where each element in sample \n# represents number of successes in 3 trials (like number of heads in 3 tosses)\nrbinom(n = 5, size = 3, prob = 0.5)\n\n[1] 1 1 2 1 2\n\n# if we want to generate a sequence of 10 tosses of a fair coin, for example:\nrbinom(n = 10, size = 1, prob = 0.5)\n\n [1] 0 1 1 0 1 1 0 1 0 0\n\n\nExercise\nIn the section on the Binomial distribution above, we had an exercise where \\(X \\sim Bin(10, 0.4)\\). Using the functions defined above, compute:\n\n\\(X = 5\\)\n\\(X \\le 5\\)\n\\(3 \\le X \\le 8\\)\n\n\nCheck your answer\n\n# P(X = 5)\ndbinom(x = 5, size = 10, prob = 0.4)\n\n[1] 0.2006581\n\n# P(X = 5)\npbinom(5, 10, 0.4) - pbinom(4, 10, 0.4)\n\n[1] 0.2006581\n\n# P(X &lt;= 5)\ndbinom(x = 0, size = 10, prob = 0.4) + dbinom(x = 1, size = 10, prob = 0.4) + \n  dbinom(x = 2, size = 10, prob = 0.4) + dbinom(x = 3, size = 10, prob = 0.4) +\n  dbinom(x = 4, size = 10, prob = 0.4) + dbinom(x = 5, size = 10, prob = 0.4)\n\n[1] 0.8337614\n\n# P(X &lt;= 5)\npbinom(5, 10, 0.4)\n\n[1] 0.8337614\n\n# P(3 &lt;= X &lt;= 8)\ndbinom(x = 3, size = 10, prob = 0.4) + dbinom(x = 4, size = 10, prob = 0.4) + \n  dbinom(x = 5, size = 10, prob = 0.4) + dbinom(x = 6, size = 10, prob = 0.4) +\n  dbinom(x = 7, size = 10, prob = 0.4) + dbinom(x = 8, size = 10, prob = 0.4)\n\n[1] 0.8310325\n\n# P(3 &lt;= X &lt;= 8)\npbinom(8, 10, 0.4) - pbinom(2, 10, 0.4)\n\n[1] 0.8310325\n\n\nWhat is going on in the last expression? Why is \\(P(3 &lt;= X &lt;= 8) = F(8) - F(2)\\)?\n\nCheck your answer\n\\(P(3 &lt;= X &lt;= 8)\\) consists of all the probability at the points \\(3, 4, 5, 6, 7, 8\\).\n\\(F(8) = P(X \\le 8)\\) is all the probability up to \\(8\\), including any probability at \\(8\\). We subtract off all the probability up to and including \\(2\\) from \\(F(8)\\) and are left with the probability at the values \\(3\\) up to and including \\(8\\), which is what we want.\nHypergeometric \\((N, G, n)\\)\n\nThe notation is a bit confusing, but just remember that x is usually the number \\(k\\) that you want the probability for, and m + n\\(=N\\) is the total number of successes and failures, or the population size.\n\n\ndhyper computes the pmf of \\(X\\), \\(f(k) = P(X = k)\\), for \\(k = 0, 1, \\ldots, n\\).\n\n\nArguments:\n\n\nx: the value of \\(k\\) in \\(f(k)\\)\n\n\nm: the parameter \\(G\\), the number of successes in the population\n\nn: the value \\(N-G\\), the number of failures in the population\n\nk: the sample size (number of draws \\(n\\), note that \\(0 \\le k \\le m+n\\))\n\n\n\n\n\nphyper computes the cdf \\(F(x) = P(X \\le x)\\)\n\n\n\nArguments:\n\n\nq: the value of \\(x\\) in \\(F(x)\\)\n\n\nm: the parameter \\(G\\), the number of successes in the population\n\nn: the value \\(N-G\\), the number of failures in the population\n\nk: the sample size (number of draws \\(n\\))\n\n\n\n\n\nrhyper generates a sample (random numbers) from the hypergeometric\\((N, G, n)\\) distribution.\n\n\nArguments:\n\n\nnn: the number of random numbers desired\n\nm: the parameter \\(G\\), the number of successes in the population\n\nn: the value \\(N-G\\), the number of failures in the population\n\nk: the sample size (number of draws \\(n\\))\n\n\n\nExample\nSuppose we consider \\(N = 10, G  = 6, n = 3\\), that is, \\(X\\) is the number of successes in 3 draws without replacement from a box that has 6 tickets marked \\(\\fbox{1}\\) and 4 tickets marked \\(\\fbox{0}\\)\n\n# probability that we see exactly 1 success = f(1)\ndhyper(x = 1, m = 6, n = 4, k = 3)\n\n[1] 0.3\n\n# you can compute this by hand as well to check. \n\n# probability that we see at most 1 success = F(1) = f(0) + f(1)\nphyper(q = 1, m = 6, n = 4, k = 3)\n\n[1] 0.3333333\n\n# check f(0) + f(1)\ndhyper(x = 0, m = 6, n = 4, k = 3) + dhyper(x = 1, m = 6, n = 4, k = 3)\n\n[1] 0.3333333\n\n# generate a sample of size 5 where each element in sample \n# represents number of successes in 3 draws\nrhyper(nn = 5, m = 6, n = 4, k = 3)\n\n[1] 2 3 2 1 2\n\n\nPoisson(\\(\\lambda\\))\n\n\ndpois computes the pmf of \\(X\\), \\(f(k) = P(X = k)\\), for \\(k = 0, 1, 2,  \\ldots\\).\n\n\nArguments:\n\n\nx: the value of \\(k\\) in \\(f(k)\\)\n\n\nlambda: the parameter \\(\\lambda\\)\n\n\n\n\n\n\nppois computes the cdf \\(F(x) = P(X \\le x)\\)\n\n\n\nArguments:\n\n\nq: the value of \\(x\\) in \\(F(x)\\)\n\n\nlambda: the parameter \\(\\lambda\\)\n\n\n\n\n\n\nrpois generates a sample (random numbers) from the Poisson(\\(\\lambda\\)) distribution.\n\n\nArguments:\n\n\nn: the desired sample size\n\nlambda: the parameter \\(\\lambda\\)\n\n\n\n\nExample\nSuppose we consider \\(\\lambda = 1\\), that is \\(X \\sim\\) Poisson\\((\\lambda)\\).\n\n# probability that we see exactly 1 event = f(1)\ndpois(x = 1, lambda = 1)\n\n[1] 0.3678794\n\n#check f(1) = exp(-lambda)*lambda = exp(-1)*1\nexp(-1)\n\n[1] 0.3678794\n\n# probability that we see at most 1 success = F(1) = f(0) + f(1)\nppois(q = 1,lambda = 1)\n\n[1] 0.7357589\n\n# check f(0) + f(1)\ndpois(x = 0, lambda = 1) + dpois(x = 1, lambda = 1)\n\n[1] 0.7357589\n\n# generate a sample of size 5 where each element in sample \n# represents a random count from the Poisson(1) distribution\nrpois(n = 5, lambda = 1)\n\n[1] 1 1 1 0 2"
  },
  {
    "objectID": "3-probability/04-random-variables/notes.html#summary",
    "href": "3-probability/04-random-variables/notes.html#summary",
    "title": "Random Variables",
    "section": "Summary",
    "text": "Summary\n\nIn these notes, we defined random variables, and described discrete and continuous random variables.\nFor any random variable, there is an associated probability distribution, and this is described by the probability mass function or pmf \\(f(x)\\).\nWe also defined a function that, for a random variable \\(X\\), and any real number \\(x\\), describes all the probability that is to the left of \\(x\\). This function is called the cumulative distribution function (cdf) of \\(X\\) and is denoted \\(F(x)\\).\nWe looked at some special distributions (discrete uniform, Bernoulli, binomial, hypergeometric, and Poisson)\nWe defined functions in R that can compute the pmf and cdf for the named distributions (except for discrete uniform since it doesn’t need a special function as we can just use sample())."
  },
  {
    "objectID": "3-probability/04-random-variables/notes.html#footnotes",
    "href": "3-probability/04-random-variables/notes.html#footnotes",
    "title": "Random Variables",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://en.wikipedia.org/wiki/Binomial_coefficient↩︎\nhttps://dornsife.usc.edu/news-briefs/wp-content/uploads/sites/182/2024/01/USC-CSU-CEPPoll.pdf↩︎\nhttps://www.latimes.com/world/la-xpm-2011-jun-20-la-naw-wal-mart-court-20110621-story.html↩︎\nhttps://en.wikipedia.org/wiki/Poisson_distribution↩︎\nhttps://en.wikipedia.org/wiki/Poisson_distribution↩︎"
  },
  {
    "objectID": "3-probability/04-random-variables/ps.html",
    "href": "3-probability/04-random-variables/ps.html",
    "title": "Random Variables",
    "section": "",
    "text": "For each of the boxes below, draw one ticket at random, and let \\(X\\) be the value of the ticket that you draw.\n\n\n\nWrite down the pmf, and draw the graph of the cdf of \\(X\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite down the pmf, and draw the graph of the cdf of \\(X\\).\n\n\n\n\n\n\n\n\n\n\n\nSuppose \\(X \\sim\\) Poisson\\((6)\\), where \\(X\\) represents the number of students who earn an A+ in Stat 20, and \\(Y \\sim\\) Poisson\\((5)\\) represents the number of students who earn an F in stat 20. We can assume that these two quantities are independent, since the class is not curved.\n\nWhat is the probability that no one fails Stat 20? After writing the expression for this probability, compute it in R, using an appropriate function, and copy your code here.\n\n\n\n\n\n\n\n\nWhat is the probability that at least 10 students earn an A+ in Stat 20? Write the expression for this probability, and then compute it in R, using an appropriate function, and copy your code here.\n\n\n\nSuppose you are playing roulette in Las Vegas, and you bet on red each time (recall that an American roulette wheel has 18 red, 18 black, and 2 green slots). You play 50 times, and bet on red every single time. Let \\(X\\) be the number of times you win in 50 plays. What is the distribution of \\(X\\)? Make sure to state the parameters of the distribution. What is the probability that you win at least 12 times? Use an appropriate function to compute this probability in R and copy your code here.\n\n\n\n\n\n\n\n\nI tried to use the Hypergeometric distribution to simulate drawing spades (\\(\\spadesuit\\)) from a standard 52-card deck with the following line of code, but I received an error and the code would not run.\n\nrhyper(m = 13, n = 39, k = 60, nn = 1)\nWhy didn’t my code run? What caused the error?\n\n\n\n\n\n\n\nI decided to try again to use the function rhyper() to simulate drawing 5 cards from a standard deck and counting the number of \\(\\spadesuit\\)s.\n\nrhyper(m = 13, n = 52, k = 5, nn = 1)\nThis code runs, but is it correct? Explain your answer clearly.\n\n\n\n\n\n\n\nHow would you simulate tossing a coin 10 times and counting the number of heads? Let \\(X\\) be the number of heads in 10 tosses. What is the distribution of \\(X\\)? Write code to simulate 100 values from this distribution, and plot the empirical histogram for \\(X\\). Copy the code here."
  },
  {
    "objectID": "3-probability/06-normal-approx/notes.html",
    "href": "3-probability/06-normal-approx/notes.html",
    "title": "Continuous Distributions and Normal Approximations",
    "section": "",
    "text": "Imagine that a regular patron of a bar has hit the bottle rather hard one evening. When the bar closes for the night, they come out to weave their way home. Home is very near, in fact, just straight down the road. If our inebriate walks straight in the direction of their home, they can be there very soon. The only problem is - they can’t walk straight. Every minute, they move in a random direction: backwards or forwards with equal probability. Where will they be after \\(n\\) minutes? This is the famous “drunkard’s walk” problem1.\nEach time step (say, each minute), they go backwards or forwards with equal probability, so it is as if they are walking on the real line, and each minute they go either forward \\((+1)\\) or backwards \\((+1)\\), each with probability \\(\\displaystyle \\frac{1}{2}\\). Where will our tipsy traveler be after \\(n\\) steps?\nHere is a plot of a simulation of our itinerant inebriate’s path for \\(n = 30\\):\nNote that the graph is spread out to show the number of steps taken, but the walker is just walking up and down the \\(y\\)-axis, since they can only go backwards and forwards. This kind of walk is called a simple random walk. Random walks have applications in many fields including physics and finance.They are used to model photons escaping from the center of the sun (though photons can scatter in any direction), a molecule in a liquid, the price of a stock etc.\nWhat we have shown here is one possible path our random walker might take and where they might land up after \\(n = 30\\) minutes. We have plotted this by defining a modified Bernoulli random variable \\(X\\) that takes the values \\(-1\\) and \\(1\\) with an equal probability of \\(\\displaystyle \\frac{1}{2}\\). Then we sample \\(30\\) times from this vector with replacement. Notice here that we use an argument in sample() that we haven’t used before, prob. This defines the weights used for sampling. In this case they are the same, but they might be different. We will discuss this further in the “Ideas in Code” section. Note that the expected value of our Bernoulli random variable \\(X\\) is \\(0\\). What is its variance and SD?\nIn the code below we are defining a vector with the values taken by \\(X\\), a vector of probabilities for these values, and the distance traveled by the walker in \\(n\\) steps:\nFor example,in the path shown in the figure above, the first \\(5\\) steps taken (\\(-1\\) represents backwards and \\(+1\\) forward): \\(1, -1, -1, 1, -1, -1\\). The position at each step is the sum of all the steps thus far, so this sequence becomes \\(1, 0, -1, 0, -1, -2\\). Notice that the end position after \\(30\\) steps is \\(4\\). One might ask - what is the probability that the position is \\(4\\) after \\(30\\) steps? How would we compute this probability? This seems very difficult, so let’s look at the empirical distribution of the position after \\(n = 30\\) steps (we simulate many many such paths and look at the value of sum of the \\(+1\\) and \\(-1\\) steps at the end).\nWell, that’s a nice shape! This is an example of the fundamental result mentioned in the subtitle of this chapter. Now, before we can compute the expected value of the walker’s position after \\(n\\) steps or the associated probabilities, we need to learn about very special sets of random variables."
  },
  {
    "objectID": "3-probability/06-normal-approx/notes.html#independent-and-identically-distributed-random-variables",
    "href": "3-probability/06-normal-approx/notes.html#independent-and-identically-distributed-random-variables",
    "title": "Continuous Distributions and Normal Approximations",
    "section": "Independent and identically distributed random variables",
    "text": "Independent and identically distributed random variables\n\n\nIID random variables\n\nIf we have \\(n\\) independent random variables \\(X_1, X_2, \\ldots, X_n\\) such that they all have the same pmf \\(f(x)\\) and the same cdf \\(F(x)\\), we call the random variables \\(X_1, X_2, \\ldots, X_n\\) independent and identically distributed random variables. We usually use the abbreviation i.i.d or iid for ``independent and identically distributed’’.\n\n\n\nThis is a very important concept that we have already used to compute the expected value and variance of a binomial random variable by writing it as a sum of iid Bernoulli random variables.\nA common example is when we toss a coin \\(n\\) times and count the number of heads - each coin toss can be considered a Bernoulli random variable, and the total number of heads is a sum of \\(n\\) iid Bernoulli random variables.\nExample: Drawing tickets with replacement\nConsider the box shown below:\n\nSay I draw \\(25\\) tickets with replacement from this box, and let \\(X_k\\) be the value of the \\(k\\)th ticket. Then each of the \\(X_k\\) has the same distribution, and they are independent since we draw the tickets with replacement. Therefore \\(X_1, X_2, \\ldots, X_{25}\\) are iid random variables, and they each have a distribution defined by the following pmf: \\[\nf(x) = \\begin{cases}\n      0.2, \\; x = 0, 3, 4 \\\\\n      0.1, \\; x = 2 \\\\\n      0.3, \\; x = 1\n      \\end{cases}\n\\]"
  },
  {
    "objectID": "3-probability/06-normal-approx/notes.html#the-box-modelfppbox",
    "href": "3-probability/06-normal-approx/notes.html#the-box-modelfppbox",
    "title": "Continuous Distributions and Normal Approximations",
    "section": "The Box Model2\n",
    "text": "The Box Model2\n\nWe have seen that we can always represent a discrete random variable using tickets in a box. We define the tickets so that a ticket drawn at random from the box will have the same probability distribution as the random variable. Then it becomes very easy to define \\(n\\) random variables that are independent, and identically distributed (as in the example above), where each random variable is a draw with replacement from the same box. Representing the random variable using a box of tickets also makes it easy to set up a simulation, since we can just define a vector representing the box, and use the function sample() setting the argument replace = TRUE. In this way, we can represent a chance process in which some action that defines a random variable is repeated over and over again, as a box of tickets from which we draw tickets with replacement over and over again.\nWe could do this for any of the examples that we have seen - die rolls, coin tosses, spins of a roulette wheel etc. Once we represent the random variable using a box of tickets, then it is easy to compute the expected value of the random variable - it is just the average of the tickets in the box, and the variance of the random variable is the variance of the tickets in the box. Note that we compute the population variance, so we divide by \\(n\\) not \\(n-1\\), since the box represents the population, from which we draw a sample. Let’s revisit some examples:\nRolling a pair of dice and summing the spots\nSuppose we want to represent the rolling of a pair of dice and summing the spots using a box with appropriately marked tickets. Our box could be:\n\nLet \\(X_1\\) represent drawing a ticket at random from this box. Then \\(X_1\\) has the discrete uniform distribution. To represent the sum of spots of a pair of dice, we would draw twice with replacement from this box, calling the result of the first draw \\(X_1\\) and the second \\(X_2\\). Notice that \\(X_1\\) and \\(X_2\\) are independent, since we drew with replacement, and have the same probability distribution, so are identically distributed. That is, \\(X_1\\) and \\(X_2\\) are i.i.d. random variables, and we want to know about the sum of these. We can define \\(S = X_1+X_2\\) and use the box to simulate the distribution of \\(S\\).\nSpinning a Vegas roulette wheel and betting on red\n\nWe know that an American roulette wheel has \\(18\\) red pockets, \\(18\\) black pockets and \\(2\\) green pockets. So if we spin it, the chance of winning if we bet on red is \\(18/38\\). Suppose we bet a dollar on red, then if the ball lands in a red pocket, we get our dollar back and another dollar. If it lands on black, we lose the dollar we bet. We are usually interested in our “net gain” if we play over and over again, that is, our total winnings (or losses) after \\(n\\) spins. We can use a box of tickets to model the net gain in \\(n\\) spins by letting the box represent the possible result of one spin, and then drawing \\(n\\) times at random from this box. What would the box to represent the result of one spin of a roulette wheel look like?\n\nCheck your answer\nThe box would have \\(38\\) tickets. \\(18\\) tickets would be marked with \\(+1\\) and \\(20\\) tickets would be marked with \\(-1\\). This would represent the result of one spin. Suppose we want to see what the net gain would be from playing \\(100\\) times, we would draw \\(100\\) times with replacement from this box, and sum the draws.\n\nHow would you simulate this process (of spinning the wheel \\(n\\) times) in R?\nWe can generalize the ideas in the roulette example to any set up where we have iid discrete random variables. We need to think about three things:\n\nWhat are the values of the tickets in the box?\nHow many tickets of each value?\nHow many times do we need to draw?\n\nWe are usually interested in the sum or the average of \\(n\\) iid random variables, that is, the sum or average of \\(n\\) draws, and the probability distribution of this sum (or average)."
  },
  {
    "objectID": "3-probability/06-normal-approx/notes.html#sums-and-averages-of-random-variables",
    "href": "3-probability/06-normal-approx/notes.html#sums-and-averages-of-random-variables",
    "title": "Continuous Distributions and Normal Approximations",
    "section": "Sums and averages of random variables",
    "text": "Sums and averages of random variables\nSums\nSuppose we make \\(n\\) draws at random with replacement from the box in the example, reproduced here:\n\nSuppose we sum the drawn tickets. We denote the sum of \\(n\\) random variables by \\(S_n\\). Note that \\(S_n = X_1 + X_2 + \\ldots + X_{n}\\) is also a random variable.\nLet’s simulate this by letting \\(n=25\\) and sampling \\(25\\) tickets with replacement, summing them, and then repeating this process. Note that the smallest sum we can get is \\(S_n = 0\\) and the largest is \\(100\\). (Why?)\n\n\n[1] \"The sum of 25 draws is 56\"\n\n\nNow we will repeat this process (of drawing \\(25\\) tickets with replacement and summing the draws) \\(10\\) times (note how replicate() is used) :\n\nCodeset.seed(12345)\nbox &lt;-  c(0,0,1,1,1,2,3,3,4,4)\n\n# or you could use box &lt;- c(rep(0, 2), rep(1, 3), 2, rep(3,2), rep(4, 2))\n\nreplicate(n = 10, expr = sum(sample(x = box, size = 25, replace = TRUE)))\n\n [1] 56 50 64 51 49 41 64 44 45 51\n\n\nYou can see that the sum \\(S_n\\) is random and keeps changing with each iteration of the process (because the \\(X_k\\) are random numbers).\nSince we know the distribution of the \\(X_k\\) (which is the probability distribution of a single draw from the box), we can compute \\(E(X_k)\\) and \\(Var(X_k)\\). Note that since the \\(X_1, X_2, \\ldots, X_{n}\\) are iid, all the \\(X_k\\) have the same mean and variance. What about their sum \\(S_n\\)?\nWhat are \\(E(S_n)\\) and \\(Var(S_n)\\), when \\(n = 25\\)?\n\\(E(X_k) = 0.2\\times 0 + 0.3 \\times 1 + 0.1 \\times 2 + 0.2 \\times 3 + 0.2 \\times 4 = 1.9\\).\n(Note that you could also have just computed the average of the tickets in the box.)\n\\(Var(X_k) = \\sum_x (x-1.9)^2 \\cdot P(X=x) = 2.09\\)\n\\(E(S_{25}) = E(X_1 + X_2 + \\ldots +X_{25}) = 25 \\times E(X_1) = 25 \\times 1.9\\).\n(We just use \\(X_1\\) since all the \\(X_k\\) have the same distribution. We are using the additivity of expectation here.)\nSince the \\(X_k\\) are independent, we can use the addivity of variance to get that\n\\[\n\\begin{aligned}\nVar(S_{25}) &= Var(X_1 + X_2 + \\ldots +X_{25})\\\\\n&= Var(X_1) + Var(X_2) + \\ldots +Var(X_{25})\\\\\n&= 25 \\times 2.09\n\\end{aligned}\n\\]\nWe can see that the expectation and variance of the sum scale with \\(n\\), so that if \\(S_n\\) is the sum of \\(n\\) iid random variables \\(X_1, X_2, \\ldots, X_n\\), then:\n\\[\n\\begin{aligned}\nE(S_n) &= n \\times E(X_1) \\\\\nVar(S_n) &= n \\times Var(X_1)\\\\\n\\end{aligned}\n\\] This does not hold for \\(SD(S_n)\\), though. For the SD, we have the following ``law’’ for the standard deviation of the sum.\n\n\nSquare root law for sums of iid random variables\n\nThe standard deviation of the sum of \\(n\\) iid random variables is given by: \\[\nSD(S_n) = \\sqrt{n} \\times SD(X_1)\n\\]\n\n\n\nSince all the \\(X_k\\) have the same distribution, we can use \\(X_1\\) to compute the mean and SD of the sum. This law says that if the sample size increases as \\(n\\), the expected value scales as the number of random variables, but the standard deviation of the sum increases more slowly, scaling as \\(\\sqrt{n}\\). In other words, if you increase the number of random variables you are summing, the spread of your sum about its expected value increases, but not as fast as the expectation of the sum.\nExample: The drunkard’s walk\nBack to our inebriated random walker who is trying to walk home. Recall that at each time step, the walker moves forward or backward with equal probability. After \\(n\\) steps, their position can be written as \\[\nS_n = X_1 + X_2 + \\ldots X_n\n\\] where \\(X_k = +1\\) or \\(-1\\) with probability \\(0.5\\) each. \\(E(X_k) = 0\\) and \\(Var(X_k) = 1\\), so we get that: \\[\nE(S_n) = 0, \\; Var(S_n) = n, \\text{ and } SD(n) = \\sqrt{n}.\n\\]\nAverages\nWe denote the average of the random variables \\(X_1, X_2, \\ldots, X_n\\) by \\(\\displaystyle \\overline{X} =\\frac{S_n}{n}\\).\n\\(\\displaystyle \\overline{X}\\) is called the sample mean (where the “sample” consists of \\(X_1, X_2, \\ldots, X_n\\)).\n\\[ E(\\overline{X}) = E\\left(\\frac{S_n}{n}\\right) = \\frac{1}{n} E(S_n) = E(X_1) \\]\nThis means that the expected value of an average does not scale as \\(n\\) (as the sum did), but \\(E(\\overline{X})\\) is the same as the expected value of a single random variable. Let’s check the variance now:\n\\[\nVar(\\overline{X}) = Var\\left(\\frac{S_n}{n}\\right) = \\frac{1}{n^2} Var(S_n) =  \\frac{n}{n^2} Var(X_1)\n\\]\nTherefore \\(Var(\\overline{X}) =  \\displaystyle \\frac{1}{n} Var(X_1)\\)\nNote that, just like the sample sum \\(S_n\\), the sample mean \\(\\displaystyle \\overline{X}\\) is a random variable, and its variance scales as \\(\\displaystyle \\frac{1}{n}\\), which implies that \\(SD(\\overline{X})\\) will scale as \\(\\displaystyle \\frac{1}{\\sqrt{n}}\\). Let’s write that formally:\n\n\nSquare root law for averages of iid random variables\n\nThe standard deviation of the average of \\(n\\) iid random variables is given by: \\[\nSD(\\overline{X}) = \\frac{1}{\\sqrt{n}}SD(X_1)\n\\]\n\n\n\n\n\nStandard error\n\nSince \\(S_n\\) and \\(\\overline{X}\\) are numbers computed from the sample \\(X_1, X_2, \\ldots, X_n\\), they are called statistics. We use the term standard error to denote the standard deviation of a statistic: \\(SE(S_n)\\) and \\(SE(\\overline{X})\\) to distinguish it from the standard deviations of random variables that do not arise as statistics that are computed from \\(X_1, X_2, \\ldots, X_n\\). We will see more about these statistics later in the course.\n\n\n\nExample: Probability distributions for sums and averages\nLet’s go back to the box of colored tickets, draw from this box \\(n\\) times, and then compute the sum and average of the draws. We will simulate the distribution of the sum and the average of 25 draws to see what the distribution of the statistics looks like. Note that when \\(n=25\\), \\(E(S_n) = 25\\times 1.9 = 47.5\\) and \\(SE(S_n) = \\sqrt{n} \\times SD(X_1) = 5 \\times 1.45 = 7.25\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do we notice in these figures? First, we see the bell-shape again! Note that the black line is the expected value. We see that the center of the distribution for the sample sum grows as the sample size increases (look at the x-axis), but this does not happen for the distribution of the sample mean. You can also see that the spread of the data for the sample sum is much greater when n = 100, but this does not happen for the distribution of the sample mean. Now, the \\(y\\) axis has neither counts nor proportion, but it has density. This makes the histogram have a total area of one, similar to a probability histogram. Now we can think of this histogram, called a density histogram as an approximation of the probability histogram.\n\n\nDensity histograms\n\nThese are histograms for which the total area of all the bars add up to \\(1\\). The vertical axis is called density and the units are such that the height of each bar times its width gives the proportion of the values that fall in the corresponding interval of the histogram.\n\n\n\nWe see that we have moved from bar graphs to histograms, which is what we need when we consider random variables that are not restricted to taking particular values. These are called continuous random variables."
  },
  {
    "objectID": "3-probability/06-normal-approx/notes.html#continuous-distributions",
    "href": "3-probability/06-normal-approx/notes.html#continuous-distributions",
    "title": "Continuous Distributions and Normal Approximations",
    "section": "Continuous distributions",
    "text": "Continuous distributions\nSo far, we have talked about discrete distributions, and the probability mass functions for such distributions. Consider a random variable that takes any value in a given interval. Recall that we call such random variables continuous. In this situation, we cannot think about discrete bits of probability mass which are non-zero for certain numbers, but rather we imagine that our total probability mass of \\(1\\) is smeared over the interval, giving us a smooth curve, called a density, rather than a histogram. To define the probabilities associated with a continuous random variable, we define a probability density function (pdf) rather than a probability mass function.\nProbability density function of a distribution\n\n\nProbability density function\n\nThis is a function \\(f(x)\\) that satisfies two conditions:\n\n\n\n\nit is non-negative (\\(f(x) \\ge 0\\)) and\n\n\nthe total area under the curve \\(y = f(x)\\) is 1. That is, \\[ \\int_{-\\infty}^\\infty f(x) dx = 1 \\]\n\n\n\n\nIf \\(X\\) is a continuous random variable, we don’t talk about \\(P(X = x\\)), that is, the probability that \\(X\\) takes a particular value. Rather, we ask what is the probability that \\(X\\) lies in an interval around \\(x\\). Since there are infinitely many outcomes in any interval on the real line, no single outcome can have positive probability, so \\(P(X=x) =0\\) for any particular \\(x\\) in the interval where \\(X\\) is defined. To find the probability that \\(X\\) lies in an interval \\((a,b)\\), we integrate \\(f(x)\\) over the interval \\((a,b)\\). That is, we find the area under the curve \\(f(x)\\) over the interval \\((a, b)\\).\n\n\nThe probability that a continuous random variable lies in an interval\n\nThe probability that \\(X\\) is in the interval \\((a,b)\\) is given by \\[\nP(a &lt; X &lt; b) = P(X \\text{ is in the interval }(a,b)) =  \\int_{a}^b f(x) dx\n\\]\n\n\n\nNote that because a single point will not add any area, we have that: \\[\nP(a &lt; X &lt; b) = P(a \\le X &lt; b) = P(a &lt; X \\le b) = P(a \\le X \\le b)\n\\]\nJust as we did for discrete random variables, we define the cumulative distribution function for a continuous distribution.\nCumulative distribution function \\(F(x)\\)\n\n\n\nCumulative distribution function (cdf)\n\nThe cdf is defined the same way as for discrete random variables, except that we now have an integral instead of a sum. \\[\nF(x) = P(X \\le x) = \\int_{-\\infty}^x f(t) dt\n\\]\n\n\n\nThe difference is in how we compute \\(P(X \\le x)\\). There are no discrete bits of probability mass for \\(F(x)\\) to collect. Instead we have that \\(F(x)\\) is the area under the curve \\(y = f(x)\\) all the way up to the point \\(x\\)."
  },
  {
    "objectID": "3-probability/06-normal-approx/notes.html#special-distributions",
    "href": "3-probability/06-normal-approx/notes.html#special-distributions",
    "title": "Continuous Distributions and Normal Approximations",
    "section": "Special distributions",
    "text": "Special distributions\nJust as in the case of discrete distributions, we have many special named continuous distributions. We are going to mention two of them here. The first is the a very easy distribution to think about with a rectangulare geometry that is easy to think about: the uniform distribution. The second distribution we mention here has a shape that has already shown up in examples in these note - its probability density function has a bell-shape. This is the Normal distribution and it is the most ubiquitous distribution in statistics and its bell shaped density curve is used in many disciplines.\nLet’s consider the uniform distribution first.\nThe Uniform(0,1) distribution\nLet \\(X\\) be a random variable that takes values in the interval \\((0,1)\\) with probability density function \\(f(x) = 1\\) for \\(x\\) in \\((0,1)\\) and \\(f(x) = 0\\) outside of this interval.\nBecause \\(f(x)\\) is flat, all intervals of the same length will have the same area, so the distribution defined by \\(f\\) is called the Uniform\\((0,1)\\) distribution. If a random variable \\(X\\) has this distribution, we denote this by \\(X \\sim U(0,1)\\). The probability that \\(X\\) is in any interval \\((a, b)\\) which is a sub-interval of \\((0,1)\\) is given by the area of the rectangle formed by the interval and \\(y=f(x)\\), and so is just the width of the interval.\n\nIf you know that \\(X\\) has a \\(Unif(a, b)\\) distribution, you should try to use geometry to figure out probabilities as this is usually easier than integrating.\nExample: cdf for the Uniform (0,1) distribution\nLet \\(X \\sim U(0,1)\\). What is \\(F(0.3)\\)?\n\nCheck your answer\n\\[\nF(0.3) = P(X \\le 0.3) = \\int_{-\\infty}^{0.3} f(t) dt =  \\int_0^{0.3} 1 dt = 0.3\n\\] In general, for the \\(U(0,1)\\) distribution, \\(F(x) = x\\).\nThe Normal distribution\nThe Normal distribution, which is also called the Gaussian distribution (after the great 19th century German mathematician Carl Friedrich Gauss3) describes a continuous random variable that has a density function with a familiar bell shape4.\n\n\n\n\n\n\n\n\nIf a random variable \\(X\\) follows a normal distribution, we write\n\\[X \\sim \\textrm{N}(\\mu, \\sigma^2)\\]\nwhere \\(\\mu\\) is the mean of the distribution and \\(\\sigma\\) is its standard deviation. (The particular normal distribution shown above is the standard normal distribution, where \\(\\mu = 0\\) and \\(\\sigma = 1)\\). The curve is completely defined by these two numbers and they are called the parameters of the distribution. Every normal curve can be matched up to the standard normal by transforming the \\(x\\) to standard units which means shifting the curve so it is centered at \\(0\\) (subtracting \\(\\mu\\)) and scaling the curve so the standard deviation is \\(1\\) (dividing by \\(\\sigma\\)).\nWe can calculate the probability of any event related to \\(X\\) by finding the area under the curve corresponding to that event. That includes the probability that \\(X\\) falls within a particular interval. In the table below, we record the probabilities of three such intervals.\n\n\nInterval\nArea under the normal curve\n\n\n\nBetween -1 and 1\n0.68\n\n\nBetween -2 and 2\n0.95\n\n\nBetween -3 and 3\n0.997\n\n\n\nSo if we know a particular distribution is similar in shape to the normal distribution, we’re able to calculate the probabilities that the random variable falls within a particular interval."
  },
  {
    "objectID": "3-probability/06-normal-approx/notes.html#the-central-limit-theorem",
    "href": "3-probability/06-normal-approx/notes.html#the-central-limit-theorem",
    "title": "Continuous Distributions and Normal Approximations",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\nThe normal curve is enormously useful because many data distributions are similar to the normal curve, so we can use the areas under the normal curve to approximate the areas of the data distributions to figure out proportions. The reason so many data distributions approximately follow a normal distribution is because of one of the most fundamental results in statistics called the Central Limit Theorem. This astounding result says that sums (and averages) of independent and identically distributed random variables will follow an approximately normal distribution (after transforming them to standard units) as the sample size grows large. If we restate this in terms of our box model, this theorem says that, for large enough \\(n\\), if we draw \\(n\\) times from a box of tickets with replacement, then the probability distribution of the sum and the average of the draws (standardized) will be approximately normal, regardless of the distribution of the tickets in the box. This means that the tickets in the box can be anything, but as long as we draw enough times, the sum will have an approximately bell-shaped distribution.\nSince many useful statistics can be written as the sum or mean of iid random variables, this is a very important and useful theorem, and we will use it extensively for inference in the next unit.\nRecall our hammered homeseeker from the beginning of this chapter, and the bell-shaped distribution of their position after half an hour. Since their position can be written as a sum of all their steps (which were \\(+1\\) or \\(-1\\)), the Central Limit Theorem applies to the probability distribution of the sum, and that is why the distribution is bell-shaped. Using the normal approximation and the empirical rule, we can say that with probability about \\(32\\%\\), our tipsy friend will be farther than one standard deviation from the starting point, which was the bar. Recall that one standard deviation for \\(n=30\\) is \\(\\sqrt{n} \\approx 5.5\\) steps. So he may be close to his home (with about \\(16\\%\\) probability) or totally in the other direction (also with about \\(16\\%\\) probability)."
  },
  {
    "objectID": "3-probability/06-normal-approx/notes.html#ideas-in-code",
    "href": "3-probability/06-normal-approx/notes.html#ideas-in-code",
    "title": "Continuous Distributions and Normal Approximations",
    "section": "Ideas in code",
    "text": "Ideas in code\nUseful functions\nUniform\\((a, b)\\)\n\n\n\ndunif computes the density \\(f(x)\\) of \\(X\\) where \\(f(x) = \\displaystyle \\frac{1}{b-a}\\), for \\(a &lt; x &lt; b\\).\n\n\nArguments:\n\n\nx: the value of \\(x\\) in \\(f(x)\\)\n\n\nmin: the parameter \\(a\\), the lower end point of the interval for \\(X\\). The default value is min = 0\n\n\nmax: the parameter \\(b\\), the upper end point of the interval for \\(X\\). The default value is max = 1\n\n\n\n\n\n\npunif computes the cdf \\(F(x) = P(X \\le x)\\) of \\(X\\).\n\n\nArguments:\n\n\nq: the value of \\(x\\) in \\(F(x)\\)\n\n\nmin: the parameter \\(a\\), the lower end point of the interval for \\(X\\). The default value is min = 0\n\n\nmax: the parameter \\(b\\), the upper end point of the interval for \\(X\\). The default value is max = 1\n\n\n\n\n\n\nrunif generates random numbers from the \\(Unif(a,b)\\) distribution.\n\n\nArguments:\n\n\nn: the size of the sample we want\n\nmin: the parameter \\(a\\), the lower end point of the interval for \\(X\\). The default value is min = 0\n\n\nmax: the parameter \\(b\\), the upper end point of the interval for \\(X\\). The default value is max = 1\n\n\n\nNormal\\((\\mu, \\sigma^2)\\)\n\n\n\ndnorm computes the density \\(f(x)\\) of \\(X \\sim N(\\mu,\\sigma^2)\\)\n\n\n\nArguments:\n\n\nx: the value of \\(x\\) in \\(f(x)\\)\n\n\nmean: the parameter \\(\\mu\\), the mean of the distribution. The default value is mean = 0\n\n\nsd: the parameter \\(\\sigma\\), the sd of the distribution. The default value is sd = 1\n\n\n\n\n\n\npnorm computes the cdf \\(F(x) = P(X \\le x)\\) of \\(X\\).\n\n\nArguments:\n\n\nq: the value of \\(x\\) in \\(F(x)\\)\n\n\nmean: the parameter \\(\\mu\\), the mean of the distribution. The default value is mean = 0\n\n\nsd: the parameter \\(\\sigma\\), the sd of the distribution. The default value is sd = 1\n\n\n\n\n\n\nrnorm generates random numbers from the Normal\\((\\mu, \\sigma^2)\\) distribution.\n\n\nArguments:\n\n\nn: the size of the sample we want\n\nmean: the parameter \\(\\mu\\), the mean of the distribution. The default value is mean = 0\n\n\nsd: the parameter \\(\\sigma\\), the sd of the distribution. The default value is sd = 1\n\n\n\n\nExample\nLet’s verify the empirical rule for the standard normal random variable:\nNote that (for example) \\(P(-1 \\le X \\le 1) = F(1) - F(-1)\\):\n\n\n\n\n\n\n\n\n\npnorm(q = 1) - pnorm(q = -1)\n\n[1] 0.6826895\n\npnorm(q = 2) - pnorm(q = -2)\n\n[1] 0.9544997\n\npnorm(q = 3) - pnorm(q = -3)\n\n[1] 0.9973002\n\n\nThe argument prob in the function sample()\n\nWe have seen the function sample(), but so far, have only used it when we were sampling uniformly at random. That is, all the values are equally likely. We can sample according to a weighted probability, though, by putting in a vector of probabilities. Let’s look at the example of net gain while betting on red on a roulette spin. Recall that if we bet a dollar on red, then our net gain is \\(+1\\) with a probability of \\(\\displaystyle \\frac{18}{38}\\) and \\(-1\\) with a probability of \\(\\displaystyle \\frac{20}{38}\\).\n\ngain &lt;- c(1,-1) # define the gain for a single spin\nprob_gain &lt;- c(18/38,20/38) #define the corresponding probabilities\nexp_gain &lt;- sum(gain*prob_gain)\nexp_gain\n\n[1] -0.05263158\n\nset.seed(123)\n#simulate gain from 10 spins of the wheel\nsample(x = gain, size = 10, prob = prob_gain, replace = TRUE)\n\n [1] -1  1 -1  1  1 -1  1  1  1 -1\n\n#simulate net gain from 10 spins of the wheel which would sum these\nsum(sample(x = gain, size = 10, prob = prob_gain, replace = TRUE))\n\n[1] 0\n\n\nHere is a simulation showing the Central Limit Theorem at work, with the empirical distribution becoming gradually more bell-shaped. Net gain is the sum of \\(n\\) draws with replacement from the vector gain defined above using the prob_gain vector."
  },
  {
    "objectID": "3-probability/06-normal-approx/notes.html#summary",
    "href": "3-probability/06-normal-approx/notes.html#summary",
    "title": "Continuous Distributions and Normal Approximations",
    "section": "Summary",
    "text": "Summary\n\nWe defined the expected value or the mean of a discrete random variable and listed the properties of expectation including linearity and additivity.\nWe defined the variance and standard deviation of a random variable. Both expectation and variance (and therefore standard deviation) are constants associated to the distribution of the random variable. The variance is more convenient than the sd for computation because it doesn’t have square roots. However, the units are squared, so you have to be careful while interpreting the variance. We discussed the properties of variance and standard deviation.\nWe wrote down the expected values and variance for various special random variables."
  },
  {
    "objectID": "3-probability/06-normal-approx/notes.html#footnotes",
    "href": "3-probability/06-normal-approx/notes.html#footnotes",
    "title": "Continuous Distributions and Normal Approximations",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://en.wikipedia.org/wiki/Random_walk↩︎\nThe box model was introduced by Freedman, Pisani, and Purves in their textbook Statistics↩︎\nAnother instance where Abraham De Moivre was the first person to discover a distribution, but it was named after someone else. You can read about De Moivre at https://mathshistory.st-andrews.ac.uk/Biographies/De_Moivre/.↩︎\nFor a normally distributed random variable, \\(f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\). Read more on Wikipedia: https://en.wikipedia.org/wiki/Normal_distribution↩︎"
  },
  {
    "objectID": "3-probability/05-ev-se/notes.html",
    "href": "3-probability/05-ev-se/notes.html",
    "title": "Expected value and variance of a random variable",
    "section": "",
    "text": "We are often interested in the average value of a random variable. We might repeat the action that generates a value of a random variable over and over again, and consider the long term average. For example, we might bet on red in roulette, and think about what our average gain would be if we play hundreds of games. Maybe we roll a die four times, record a success if we see at least one six, and repeat this process and take the average - note that we did this when we computed the proportion of times we rolled at least one 6 in 4 rolls, while simulating de Méré’s paradox. The proportion is just a special case of an average, when the random variable takes only the values \\(0\\) or \\(1\\). So you can see that we can think of the average as the value we would predict for the random variable - some sort of typical or expected value."
  },
  {
    "objectID": "3-probability/05-ev-se/notes.html#expectation-of-a-random-variable",
    "href": "3-probability/05-ev-se/notes.html#expectation-of-a-random-variable",
    "title": "Expected value and variance of a random variable",
    "section": "Expectation of a random variable",
    "text": "Expectation of a random variable\n\n\nExpected value of a random variable\n\nThe expectation or expected value of a random variable \\(X\\) with pmf \\(f(x)\\) is denoted by \\(E(X)\\). It is a constant associated with the distribution, and is defined by \\[\nE(X) = \\sum_x x \\times P(X = x) = \\sum_x x \\times f(x)\n\\]\n\n\n\nYou can see that \\(E(X)\\) is a weighted average of the possible values taken by the random variable, where each possible value is weighted by its probability.\nExample: Rolls of a fair die\nIf \\(X\\) is the spots when we roll a fair six-sided die, then \\(f(x) = P(x = x) = 1/6\\) for \\(x = 1, 2, \\ldots, 6\\). In this case, \\(E(X) = \\displaystyle \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = 3.5\\).\nIn general, if we have a discrete uniform random variable on the values \\(1, 2, 3, \\ldots, n\\), if we had to predict \\(X\\), we would predict the middle of the distribution, since all the values are equally likely: (\\(f(x) =\\displaystyle 1/n\\) for all \\(x = 1, 2, 3, \\ldots, n\\)). Therefore the expected value of a discrete uniform random variable is \\(E(X) = \\displaystyle\\frac{n+1}{2},\\) or just the usual average of \\(1, 2, 3, \\ldots, n\\).\n\n\nExample: Rolls of an unfair die\nWhat happens if all the faces are not equally likely? Consider the following scenario: let \\(X\\) be the result of rolling a weird six-sided die, which has that the probability of rolling a 4 or 6 is double that of rolling any of the odd numbers (\\(P(X=4) = P(X = 6) = 2\\times P(X =1)\\) etc), and the probability of rolling a 2 is three times that of rolling any of the odd numbers (\\(P(X=2) = 3\\times P(X =1)\\) etc). What is the pmf of \\(X\\)? See if you can work it out! Remember that the probabilities have to add to 1.\n\nCheck your answer\nWriting it as a piecewise function:\n\\[\nP(X=x) = f(x) = \\begin{cases}\n\\frac{1}{10} = 0.1,\\: \\text{for}\\, x = 1, 3, 5 \\\\\n\\frac{2}{10} = 0.2, \\:, \\text{for}\\, x = 4, 6 \\\\\n\\frac{3}{10} = 0.3, \\: \\text{for}\\, x = 2\n\\end{cases}\n\\] In tabular form:\n\n\n\\(x\\)\n\\(f(x) = P(X = x)\\)\n\n\n\n\\(1\\)\n\\(0.1\\)\n\n\n\\(2\\)\n\\(0.3\\)\n\n\n\\(3\\)\n\\(0.1\\)\n\n\n\\(4\\)\n\\(0.2\\)\n\n\n\\(5\\)\n\\(0.1\\)\n\n\n\\(6\\)\n\\(0.2\\)\n\n\nNow see if you can figure out the weighted average or the expected value, using the definition. Remember, multiply each value by its probability (\\(x\\times f(x)\\)) and then add them all up. You should be adding up 6 terms.\n\nCheck your answer\n\\[\n\\begin{aligned}\nE(X) &= \\sum_x x \\times f(x) \\\\\n     &= 1 \\times 0.1 + 2 \\times 0.3 +  3 \\times 0.1 +  4 \\times 0.2 +  5 \\times 0.1 + 6 \\times 0.2\\\\\n     &= 3.5 \\\\\n\\end{aligned}\n\\]\nHere is a picture of the probability histogram with the expected value marked in red.\n\nNotice that the expected value was the same for both dice - the fair as well as unfair dice. This illustrates an important way in which the expected value of a random variable is just like the average of a list of numbers. It gives us some information about the distribution since we see the ``typical’’ value, but not that much information. Two random variables with very different distributions can have the same expected value.\nExample: Bernoulli random variables\nRecall that a Bernoulli (\\(p\\)) random variable is the special case of a binomial random variable when the parameter \\(n=1\\). This random variable takes the value \\(1\\) with probability \\(p\\) and \\(0\\) with probability \\(1-p\\). Then: \\[ E(X) = 1 \\times p + 0 \\times (1-p) = p\\] The expected value of a Bernoulli(\\(p\\)) random variable is therefore just \\(p\\). In particular, if we toss a coin and define \\(X\\) to be the number of heads, then the expected value of \\(X\\) is the probability that the coin lands heads.\n\nIn all the figures above, note the red triangle marking the expected value. If you imagine the probability histogram to be a thin lamina - like a thin sheet of metal cut in the shape of the probability histogram, you can imagine the expected value as a “balancing point” - the point where the lamina would balance. It is the center of mass for the probability distribution. The expected value for a random variable is analogous to the average for sample data. Other terms that we use for the expected value of a random variable are expectation and mean. These can be used interchangeably.\n\nExercise: Compute the expectation of the random variable whose distribution is shown below:\nLet \\(X\\) be a random variable such that\n\\[\nX = \\begin{cases}\n1\\: \\text{with prob}\\, 4/15\\\\\n2 \\: \\text{with prob}\\, 7/30 \\\\\n0 \\: \\text{with prob}\\, 1/3 \\\\\n-1 \\: \\text{with prob} \\, 1/6\n\\end{cases}\n\\]\n\nCheck your answer\n\\[\n\\begin{aligned}\nE(X) &= 1 \\times \\frac{4}{15} + 2 \\times \\frac{7}{30} +  0 \\times \\frac{1}{3} + (-1) \\times \\frac{1}{6}  \\\\\n&= \\frac{1 \\times 8 + 2 \\times 7 + 0 \\times 10 + (-1) \\times 5}{30} \\\\\n&= \\frac{8 + 14 + 0 - 5}{30} \\\\\n&= \\frac{17}{30} = 0.5666667\n\\end{aligned}\n\\]\nNow we have the definition, let’s look at what we can do with it. Most of the properties below are what you would expect when you have a sum, since recall that you can pull constants in and out of sums, for example \\(2x + 2y = 2(x+y)\\)."
  },
  {
    "objectID": "3-probability/05-ev-se/notes.html#properties-of-expectation",
    "href": "3-probability/05-ev-se/notes.html#properties-of-expectation",
    "title": "Expected value and variance of a random variable",
    "section": "Properties of expectation",
    "text": "Properties of expectation\n\n\n1. Expected value of a constant\n\nThe expected value of a constant is just the constant itself. Let \\(c\\) be any constant. The probability mass function can be considered to be \\(f(c) = 1\\). Then \\[E(c) = c \\cdot f(c) = c \\]\n\n\n\n\n\n2. Constant multiple of a random variable\n\nIf \\(c\\) is a constant, and \\(X\\) is a random variable with pmf \\(f(x)\\), then we can consider \\(cX\\) to be the random variable that takes the value \\(cx\\) when \\(X\\) takes the value \\(x\\), so \\(f(cx) = f(x)\\). Therefore, \\[ E(cX) = \\sum_x cx \\cdot f(x) = c \\sum_x x \\cdot f(x) = c\\cdot E(X) \\]\n\n\n\n\n\n3. Additivity of expectation\n\nLet \\(X\\) and \\(Y\\) be two random variables, and consider their sum \\(X + Y\\). Then, \\[ E(X + Y) = E(X) + E(Y)\\] This is true regardless of whether \\(X\\) and \\(Y\\) are independent or not (we will call random variables independent when the values of one don’t affect the probabilities of the other, like coin tosses and die rolls).\n\n\n\n\n\n4. Linearity of expectation\n\nPutting the previous two properties together, we get \\[ E(aX + bY) = E(aX) + E(bY) = a\\cdot E(X) +b \\cdot E(Y)\\]\n\n\n\n\nThe next property is very important and a little tricky, so read it carefully. When we have a function of \\(X\\), such as its square, and we want the mean of that function, we have to be careful. We would square the values of \\(X\\), but we would not square the probabilities. This is important! We never touch \\(f(x)\\).\n\n\n5. Expectation of a function of a random variable\n\nSuppose \\(Y = g(X)\\) is a function of the random variable \\(X\\). Then \\(Y\\) is also a random variable taking values \\(y = g(x)\\), and the probabilities are just the probabilities associated with \\(x\\). Then the expected value of \\(Y\\) is given by: \\[\nE(Y) = E(g(X)) = \\sum_x g(x) \\times f(x) = \\sum_x g(x) \\times P(X=x)\n\\]\n\n\n\nExample: Expectation of the square of a random variable\nLet \\(X\\) be a random variable with the following probability distribution:\n\n\n\\(x\\)\n\\(P(X = x)\\)\n\n\n\n\\(-2\\)\n\\(0.2\\)\n\n\n\\(-1\\)\n\\(0.1\\)\n\n\n\\(0\\)\n\\(0.2\\)\n\n\n\\(1\\)\n\\(0.3\\)\n\n\n\\(3\\)\n\\(0.2\\)\n\n\n\nLet’s first add a column with the product \\(x\\times P(X=x)\\):\n\n\n\\(x\\)\n\\(P(X = x)\\)\n\\(x\\times P(X=x)\\)\n\n\n\n\\(-2\\)\n\\(0.2\\)\n\\(-0.4\\)\n\n\n\\(-1\\)\n\\(0.1\\)\n\\(-0.1\\)\n\n\n\\(0\\)\n\\(0.2\\)\n\\(0\\)\n\n\n\\(1\\)\n\\(0.3\\)\n\\(0.3\\)\n\n\n\\(3\\)\n\\(0.2\\)\n\\(0.6\\)\n\n\n\nThen we sum the third column to get \\(E(X) = -0.4 -0.1 + 0 + 0.3 + 0.6 =\\) 0.4.\nLet’s do the same for the random variable \\(Y = g(X) = X^2\\). Add two columns to the original table, one with the values of \\(y = g(x)\\), and one with \\(g(x)f(x) = g(x)P(X=x)\\):\n\n\n\\(x\\)\n\\(P(X = x)\\)\n\\(y = x^2\\)\n\\(g(x)\\times P(X=x)\\)\n\n\n\n\\(-2\\)\n\\(0.2\\)\n\\(4\\)\n\\(0.8\\)\n\n\n\\(-1\\)\n\\(0.1\\)\n\\(1\\)\n\\(0.1\\)\n\n\n\\(0\\)\n\\(0.2\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(1\\)\n\\(0.3\\)\n\\(1\\)\n\\(0.3\\)\n\n\n\\(3\\)\n\\(0.2\\)\n\\(9\\)\n\\(1.8\\)\n\n\n\nSumming the last column we get \\(E(Y) = 0.8 + 0.1 + 0 + 0.3 + 1.8 =\\) 3.\n\n\n\n\n\n\nWarning\n\n\n\nDo not apply the function to \\(f(x)\\)! The probability distribution remains the same, only the values of the variable \\(X\\) change. Instead of using \\(x\\), we use \\(g(x)\\).\n\n\nNow with these tools aka properties of expectation in hand, let’s compute the expected values of a binomial random variable."
  },
  {
    "objectID": "3-probability/05-ev-se/notes.html#special-distributions",
    "href": "3-probability/05-ev-se/notes.html#special-distributions",
    "title": "Expected value and variance of a random variable",
    "section": "Special distributions",
    "text": "Special distributions\nWe use a variety of ways to describe the expected value of a random variable. We might use the phrases “mean of the random variable \\(X\\)” or “mean of the distribution” or “expected value of the random variable \\(X\\)” and they all refer to the same quantity \\(E(X)\\) which describes, in a sense, the center of mass of the probability distribution of \\(X\\). Now we will write down the mean of the named distributions we have defined. We have already computed the mean of two special distributions, the discrete uniform and the Bernoulli.\nBinomial Distribution\nLet \\(X\\sim Bin(n, p)\\). Recall that \\(X\\) counts the number of successes in \\(n\\) trials, where the probability of success on each trial is \\(p\\). We can define \\(n\\) Bernoulli (\\(p\\)) random variables \\(X_1, X_2, \\ldots, X_n\\) where \\(X_k = 1\\) with probability \\(p\\), that is, \\(X_k\\) is 1 if the \\(k\\)th trial is a success. We see that the binomial random variable \\(X\\) can be written as a sum of these \\(n\\) Bernoulli random variables:\n\\[\nX = X_1 + X_2 + \\ldots + X_n\n\\]\nThe expected value of \\(X_k\\) is \\(E(X_k) = p\\) for each \\(k\\), so using the additivity of expectation, we get\n\\[\n\\begin{aligned}\nE(X) &= E( X_1 + X_2 + \\ldots + X_n) \\\\\n&= E(X_1) + E(X_2) + \\ldots + E(X_n) \\\\\n&= p + p +  \\ldots + p\\\\\n&= np\n\\end{aligned}\n\\]\nTherefore, if \\(X\\sim Bin(n, p)\\), then \\(E(X) = np\\). This intuitively makes sense: if I toss a fair coin 100 times, I expect to see about \\(50\\) heads. This is a very neat trick to compute the expected value of a binomial random variable because you can imagine that computing the expected value using the formula \\(\\displaystyle \\sum_x x \\cdot f(x)\\) would be very messy and difficult. Using Bernoulli random variables allowed us to easily calculate the expected value of a binomial random variable.\nHypergeometric Distribution\nIt turns out that whether we sample with or without replacement does not matter, and the expected value of a hypergeometric distribution is just the sample size times the number of successes in the population to begin with, so if \\(X \\sim HG(N, G, n)\\), then: \\[\nE(X) = n \\times \\frac{G}{N}\n\\] You do not need to know how to derive the expectation for a hypergeometric random variable.\nPoisson Distribution\nThe expected value of a Poisson random variable has a rather nice derivation. It is enough to know that if \\(X \\sim Pois(\\lambda)\\), then\n\\[\nE(X) = \\lambda\n\\]\nThis makes intuitive sense, if we think of the Poisson random variable as approximating a binomial random variable that has large \\(n\\) and tiny \\(p\\), with \\(\\lambda = np\\). Think coin tosses with a very low chance of heads, we still expect to see about \\(np\\) heads on average. You are not expected to know how to derive the expected value of a Poisson random variable, but if you know about Taylor series, you can expand the following to see it.\n\nDerivation of \\(E(X) = \\lambda\\)\n\\[\n\\begin{aligned}\nE(X) &= \\sum_{k=0}^\\infty k\\times P(X = k)\\\\\n    &= \\sum_{k=0}^\\infty k \\times e^{-\\lambda}\\frac{\\lambda^k}{k!}\\\\\n    &= \\sum_{k=1}^\\infty  k \\times e^{-\\lambda}\\frac{\\lambda^k}{k!}\\\\\n    &=  e^{-\\lambda}\\lambda \\, \\times \\sum_{k=1}^\\infty \\frac{\\lambda^{k-1}}{(k-1)!} \\\\\n    &=  e^{-\\lambda}\\lambda \\, \\times \\sum_{j=0}^\\infty \\frac{\\lambda^j}{j!} \\\\\n    &= e^{-\\lambda}\\lambda \\times  e^{\\lambda} \\\\\n    &= \\lambda\n\\end{aligned}\n\\]\n\nIn the third line, the lower limit for the sum changes because the first term is \\(0\\) since \\(k = 0\\).\nIn the fourth line, we take out \\(e^{-\\lambda}\\) and \\(\\lambda\\), and cancel the \\(k\\) in \\(k!\\).\nIn the fifth line, we rewrite the lower limit of the sum and the power of \\(\\lambda\\) to be \\(j\\) which begins at \\(0\\).\nIn the fifth line, notice that sum is just the Maclaurin series of $ e^{}$.\nNow, let’s look at another constant associated with the random variable."
  },
  {
    "objectID": "3-probability/05-ev-se/notes.html#variance-and-standard-deviation-of-a-random-variable",
    "href": "3-probability/05-ev-se/notes.html#variance-and-standard-deviation-of-a-random-variable",
    "title": "Expected value and variance of a random variable",
    "section": "Variance and standard deviation of a random variable",
    "text": "Variance and standard deviation of a random variable\nWhen we looked at averages of data, we realized that computing measures of center was insufficient to give us a picture of the distribution. We needed to know how the data distribution spread out about its center, and this idea holds true for probability distributions as well. We want a number that describes how far from \\(E(X)\\) the values of \\(X\\) typically fall, similar to the standard deviation for a list of numbers.\n\n\nVariance of a random variable\n\nDefine \\(\\mu = E(X)\\), and define the function \\(g(X) = (X-\\mu)^2\\), which describes the square of the distance of each value of \\(X\\) from its mean, or the squared deviation of \\(X\\) from its mean. Then the variance of \\(X\\), written as \\(Var(X)\\) is given by: \\[\nVar(X) = E(g(X)) = E\\left[(X-\\mu)^2\\right]\n\\]\n\n\n\nThe problem with \\(Var(X)\\) is that the units are squared, so just as we did for the sample variance, we will take the square root of the variance.\n\n\nStandard deviation of a random variable\n\nThe square root of \\(Var(X)\\) is called the standard deviation of \\(X\\):\n\n\n\\[\n  SD(X) = \\sqrt{Var(X)}\n\\]\n\n\\(SD(X)\\) is a “give or take” number attached to the mean \\(E(X)\\), so we can say that a ``typical’’ value of \\(X\\) is about \\(\\mu\\), give or take one standard deviation (the value of \\(SD(X)\\)). Note that \\(SD(X)\\) is a non-negative number.\n\nExample: Rolls of a weighted (unfair) die\nRecall the example in which we roll an unfair die, and the probability mass function \\(f(x)\\) was given by: \\[\nP(X=x) = f(x) = \\begin{cases}\n0.1,\\: \\text{for}\\, x = 1, 3, 5 \\\\\n0.2, \\:, \\text{for}\\, x = 4, 6 \\\\\n0.3, \\: \\text{for}\\, x = 2\n\\end{cases}\n\\]\nWe had computed \\(E(X)=\\mu = 3.5\\). What about \\(Var(X)\\)? Let’s write out the table, and add a column for \\(g(x) = (x-3.5)^2\\).\n\n\n\\(x\\)\n\\(P(X = x)\\)\n\\(g(x) = (x - 3.5)^2\\)\n\\(g(x)\\times P(X=x)\\)\n\n\n\n\\(1\\)\n\\(0.1\\)\n\\(6.25\\)\n\\(0.625\\)\n\n\n\\(2\\)\n\\(0.3\\)\n\\(2.25\\)\n\\(0.675\\)\n\n\n\\(3\\)\n\\(0.1\\)\n\\(0.25\\)\n\\(0.025\\)\n\n\n\\(4\\)\n\\(0.2\\)\n\\(0.25\\)\n\\(0.05\\)\n\n\n\\(5\\)\n\\(0.1\\)\n\\(2.25\\)\n\\(0.225\\)\n\n\n\\(6\\)\n\\(0.2\\)\n\\(6.25\\)\n\\(1.250\\)\n\n\n\n\\(E(g(X)) = \\sum_x g(x)\\cdot P(X=x) =\\) 2.85\nTherefore the standard deviation of \\(X = SD(X)\\) is the square root of the variance, so about 1.688.\n\nExample: Rolls of a fair die\nWe have already computed that the expected value of the random variable \\(X = 3.5\\) where \\(X\\) is the result of rolling a fair die. What are \\(Var(X)\\) and \\(SD(X)\\)?\n\nCheck your answer\n\\(Var(X) = \\sum_x (x-3.5)^2 \\cdot P(X=x) =\\) 2.917. \\(SD(X) =\\) 1.708.\nWhy do you think that \\(Var(X)\\) and \\(SD(X)\\) are greater when \\(X\\) is the result of rolling the fair die than when \\(X\\) is the result of rolling the unfair die? (Hint: think about the probability distributions.)\nComputational formula for \\(Var(X)\\)\n\nTo compute variance, we usually use a nice shortcut formula:\n\\[\nVar(X) = E\\left(X^2\\right) - \\mu^2\n\\]"
  },
  {
    "objectID": "3-probability/05-ev-se/notes.html#properties-of-variance-and-standard-deviation",
    "href": "3-probability/05-ev-se/notes.html#properties-of-variance-and-standard-deviation",
    "title": "Expected value and variance of a random variable",
    "section": "Properties of variance and standard deviation",
    "text": "Properties of variance and standard deviation\n\n\n1. The variance of a constant is \\(0\\)\n\nRemember that the variance measures how spread out the values of \\(X\\) are about the mean. A constant doesn’t spread out at all, so it makes sense that the variance is \\(0\\).\n\n\n\nLet’s confirm this. Let \\(X\\) be a random variable such that \\(X = c\\) with \\(f(c) = 1\\) for some real number \\(c\\). (This means that \\(X\\) takes the value \\(c\\) with probability 1, that is, with certainty.)\n\\[\nVar(X) = E(X^2) - \\mu^2 = E(c^2) - c^2 = c^2 - c^2 = 0.\n\\] Note that \\(E(c^2) = c^2\\) as \\(c^2\\) is a constant. Thus we have that \\(SD(c) = 0\\).\n\n\n2. The variance of a constant multiple of \\(X\\): \\(Var(cX) = c^2 Var(X)\\)\n\nNote that \\(E(cX) = c\\mu\\) by the properties of expectation. Thus we have (using the short cut formula):\n\n\n\\[\n\\begin{aligned}\n    Var(cX)  &= E((cX)^2) - (c\\mu)^2 \\\\\n    &= E(c^2 X^2) - c^2 \\mu^2 \\\\\n    &= c^2E(X^2) - c^2\\mu^2\\\\\n    &= c^2\\left(E(X^2) - \\mu^2 \\right) \\\\\n    &= c^2 Var(X)\n\\end{aligned}\n\\]\n\nThus, \\(SD(cX) = \\sqrt{Var(cX)} = \\sqrt{c^2 Var(X)} = \\lvert c \\rvert SD(X).\\) (\\(SD(X) \\ge 0\\))\n\n\n3. The variance of \\(X\\) is unchanged by adding a constant\n\nIf we add a constant to \\(X\\), the spread of the distribution does not change, only its center changes. Let \\(c\\) be a real number. Then we have:\n\n\n\\[\nVar(X+c) = Var(X)\n\\]\n\nYou can verify this using the shortcut formula (we won’t ask you to derive this).\n\nCheck your answer\n\\[\n\\begin{aligned}\nVar(X +c) &= E\\left(\\left(X+c\\right)^2\\right) - \\left(E\\left(X+c\\right)\\right)^2\\\\\n          &= E\\left(X^2 + 2cX + c^2\\right) - \\left((E(X))^2 + 2cE(X) + c^2\\right) \\\\\n          &= E\\left(X^2\\right) + 2cE(X) + c^2 - \\left(E(X)\\right)^2 -  2cE(X) - c^2\\\\\n          &= E\\left(X^2\\right) -  \\left(E(X)\\right)^2 \\\\\n          &= Var(X)\n\\end{aligned}\n\\]\n\n\n4. Additivity of variance\n\nIf \\(X\\) and \\(Y\\) are independent random variables, that is the events \\(X = x\\) and \\(Y = y\\) are independent for all values of \\(x\\) and \\(y\\), we have that:\n\n\n\\[\nVar(X + Y) = Var(X) + Var(Y) \\text{ and } Var(X - Y) = Var(X) + Var(Y)\n\\]\n\nNote that in this case, \\(Var(X + Y) = Var(X) + Var(Y)\\) implies that \\(SD(X +Y) = \\sqrt{Var(X) + Var(Y)}\\) - square roots and therefore SD’s are not additive.\n\nExample: Box of tickets\nConsider the box with the following \\(30\\) tickets:\n\\(8\\) tickets marked \\(\\fbox{1}\\), \\(7\\) tickets marked \\(\\fbox{2}\\), \\(10\\) tickets marked \\(\\fbox{0}\\), and \\(5\\) tickets marked \\(\\fbox{-1}\\).\nLet \\(X\\) be the value of a single draw from this box, if we shuffle the tickets and draw one ticket at random. What is the probability distribution of \\(X\\)? What is \\(E(X)\\), rounded to 3 decimal places?\nNotice that the average of the tickets in the box is 0.567 which is the same as \\(E(X)\\)!\nWhat about the variance of \\(X\\)?\n\\[\n\\begin{aligned}\nVar(x) &= E\\left[ ( X - \\mu)^2 \\right] \\\\\n  &=  \\left( \\frac{8}{30}\\times (1-0.567)^2  + \\frac{7}{30} \\times (2-0.567)^2 \\right. \\\\\n  &+ \\left. \\frac{10}{30} \\times (0-0.567)^2 + \\frac{5}{30} \\times (-1 - 0.567)^2 \\right) \\\\\n  &\\approx 1.045\\\\\n\\end{aligned}  \n\\] The sample variance of the tickets in the box is a bit more than 1.045. This is because we use \\(n-1\\) in the denominator of sample variance and sample sd, rather than \\(n\\).\nThe standard deviation \\(SD(X) = \\sqrt{Var(X)} =\\) 1.023.\nBernoulli random variables\nLet \\(X\\) be a Bernoulli (\\(p\\)) random variable. We know that \\(E(X) = \\mu = p\\). If we compute \\(E(X^2)\\), we get that \\(E(X^2) = p\\). (Make sure you know how to compute \\(E(X^2)\\).) Therefore we have that: \\[\nVar(X) = E(X^2) - \\mu^2 = p - p^2 = p(1-p).\n\\]\nBinomial random variables\nWe use the same method as we did to compute the expectation of \\(X\\sim Bin(n,p)\\). We will write \\(X\\) as a sum of independent Bernoulli random variables: \\[ X = X_1 + X_2 + \\ldots + X_n\\] where each \\(X_k \\sim\\) Bernoulli(\\(p\\)). Since the \\(X_k\\) are results of independent trials (by the definition of the binomial distribution), we have: \\[Var(X) = Var(X_1) + Var(X_2) + \\ldots + Var(X_n) = np(1-p).\\] Therefore, \\(SD(X) = \\sqrt{np(1-p)}\\)\nPoisson random variables\nComputing the variance of a Poisson random variable is more complicated than computing its expectation, so we will just state the variance as a fact. Let \\(X \\sim Poisson(\\lambda)\\). Then \\[\nVar(X) = \\lambda.\n\\]"
  },
  {
    "objectID": "3-probability/05-ev-se/notes.html#summary",
    "href": "3-probability/05-ev-se/notes.html#summary",
    "title": "Expected value and variance of a random variable",
    "section": "Summary",
    "text": "Summary\n\nWe defined the expected value or the mean of a discrete random variable and listed the properties of expectation including linearity and additivity.\nWe defined the variance and standard deviation of a random variable. Both expectation and variance (and therefore standard deviation) are constants associated to the distribution of the random variable. The variance is more convenient than the sd for computation because it doesn’t have square roots. However, the units are squared, so you have to be careful while interpreting the variance. We discussed the properties of variance and standard deviation.\nWe wrote down the expected values and variance for various special random variables."
  },
  {
    "objectID": "3-probability/05-ev-se/ps.html",
    "href": "3-probability/05-ev-se/ps.html",
    "title": "Expected Value and Variance",
    "section": "",
    "text": "Let \\(X\\) be a Discrete Uniform random variable on \\([-4, -1, 0, 1, 2].\\) Let \\(Y = X^2\\). Find \\(\\mathbb{E}(Y)\\).\n\n\n\n\n\n\n\n\nLet \\(X\\) be a random variable such that \\(\\mathbb{E}(X) = 4\\). Let \\(Y = 2 + 3X\\). Find \\(\\mathbb{E}(Y)\\).\n\n\n\n\n\nSuppose you roll a six-sided die with a probability distribution given below. Let \\(X\\) be the number of spots rolled.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x\\)\n\\(\\textbf{1}\\)\n\\(\\textbf{2}\\)\n\\(\\textbf{3}\\)\n\\(\\textbf{4}\\)\n\\(\\textbf{5}\\)\n\\(\\textbf{6}\\)\n\n\n\n\n\\(P(X=x)\\)\n\\(\\frac{1}{10}\\)\n\\(\\frac{3}{10}\\)\n\\(\\frac{2}{10}\\)\n\\(\\frac{1}{10}\\)\n\\(\\frac{2}{10}\\)\n\\(\\frac{1}{10}\\)\n\n\n\n\nCalculate \\(\\mathbb{E}(X)\\)\n\n\n\n\n\n\nCalculate \\(Var(\\frac{1}{2}X + 300)\\).\n\n\n\n\n\nGiven the cdf of the random variable \\(X\\) below:\n\ncompute \\(\\mathbb{E}(X)\\) and \\(Var(X)\\).\n\n\n\n\n\n\n\nAn American roulette wheel has 38 slots: 18 red, 18 black and 2 green.\n\nLet \\(S\\) be the number of times in 10 spins that you land on a red space. Calculate \\(\\mathbb{E}(S)\\), \\(Var(S)\\) and \\(SD(S)\\).\n\n\n\n\n\n\n\n\nWrite R code which will allow you to simulate 10 spins of an American roulette wheel (where your possible outcome is either red or not red), and count the number of spins that result in landing on a red space.\n\n\n\n\n\n\nWrite R code which will allow you to simulate 100 of these experiments (so 100 experiments, each featuring 10 spins of the wheel, and the number of red spaces landed on is counted in each experiment). Your output should be a vector with 100 observations, where the first element is the number of red spaced landed on in experiment one, and so on.\n\n\n\n\n\n\n\n\nWrite code to calculate the mean, variance and standard deviation of the vector you created in the last step. Then, compare them to the theoretical quantities you calculated at the start of this series of problems in one to two sentences.\n\n\n\n\n\n\n\nConsider the random variable \\(X\\) whose probability mass function can be obtained from the graph in Question 5.\n\nWrite R code which will allow you to simulate 100 realizations (draws) of this random variable.\n\n\n\n\n\n\nWrite code to calculate the mean and variance of the output you created in the last step. Then, compare them to the theoretical quantities you calculated in Question 5 in one to two sentences."
  },
  {
    "objectID": "3-probability/01-chance-intro/slides.html",
    "href": "3-probability/01-chance-intro/slides.html",
    "title": "Introducing Probability",
    "section": "",
    "text": "Codeknitr::include_graphics(\"https://imgs.xkcd.com/comics/prediction.png\")\n\n\n\n\n\n\n\n\n\n\nMaybe a bit of discussion here about situations in which two things can happen, but what do they think about the chance of each eg. coin toss, even/odd number on a die roll, winning the lottery vs not winning, getting an A in stat 20 vs not etc."
  },
  {
    "objectID": "3-probability/01-chance-intro/slides.html#probability-of-two-events",
    "href": "3-probability/01-chance-intro/slides.html#probability-of-two-events",
    "title": "Introducing Probability",
    "section": "",
    "text": "Codeknitr::include_graphics(\"https://imgs.xkcd.com/comics/prediction.png\")\n\n\n\n\n\n\n\n\n\n\nMaybe a bit of discussion here about situations in which two things can happen, but what do they think about the chance of each eg. coin toss, even/odd number on a die roll, winning the lottery vs not winning, getting an A in stat 20 vs not etc."
  },
  {
    "objectID": "3-probability/01-chance-intro/slides.html#agenda",
    "href": "3-probability/01-chance-intro/slides.html#agenda",
    "title": "Introducing Probability",
    "section": "Agenda",
    "text": "Agenda\n\nAnnouncements\nReview of ideas in notes\nProbability refresher\nConcept Questions\nActivity: coin tosses\nProblem set: Introducing Probability"
  },
  {
    "objectID": "3-probability/01-chance-intro/slides.html#announcements",
    "href": "3-probability/01-chance-intro/slides.html#announcements",
    "title": "Introducing Probability",
    "section": "Announcements",
    "text": "Announcements\n. . .\n\nProblem Set 6 due next Tuesday at 9am (paper, max. 3)\n\n. . .\n\nRQ: Conditional Probability due Wednesday/Thursday at 11:59pm\n\n. . .\n\nNo lab this week."
  },
  {
    "objectID": "3-probability/01-chance-intro/slides.html#rules-of-probability",
    "href": "3-probability/01-chance-intro/slides.html#rules-of-probability",
    "title": "Introducing Probability",
    "section": "Rules of probability",
    "text": "Rules of probability\nLet \\(\\Omega\\) be the outcome space, and let \\(P(A)\\) denote the probability of the event \\(A\\). Then we have:\n. . .\n\n\\(P(A) \\ge 0\\)\n\n. . .\n\n\\(P(\\Omega) = 1\\)\n\n. . .\n\nIf \\(A\\) and \\(B\\) are mutually exclusive (\\(A \\cap B = \\{\\}\\)), then \\(P(A \\cup B) = P(A) + P(B)\\)\n\n\n\nConcepts to review: (KEEP THIS BRIEF, OR INCORPORATE INTO CQ’s) - The first two rules of probability - unions and intersections - mutually exclusive events and the addition rule - good idea to draw Venn diagrams here - Use rule 3 to write down the complement rule, and show what A^C means"
  },
  {
    "objectID": "3-probability/01-chance-intro/slides.html#the-linda-problem",
    "href": "3-probability/01-chance-intro/slides.html#the-linda-problem",
    "title": "Introducing Probability",
    "section": "The Linda Problem",
    "text": "The Linda Problem\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nThe Linda problem is from a very famous experiment conducted by Daniel Kahneman and Amos Tversky in 1983 (The version below is from the book Thinking, Fast and Slow by Kahneman, page 156):\nLinda is thirty-one years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in antinuclear demonstrations.\n\nWhich alternative is more probable?\n\n\nLinda is a bank teller.\nLinda is a bank teller and is active in the feminist movement.\n\n\nCorrect answer: (a) Depending on the response, you can discuss how even though (b) is clearly contained in (a) and therefore has lower probability, an overwhelming majority of their respondents ranked (b) as more likely. “About 85% to 90% of undergraduates at several major universities chose the second option, contrary to logic”, and talk about why this is so. Probability can be tricky and counter-intuitive. If they do well, congratulate them and say that they are among the rare people who understand that \\(P(A \\textbf{ and } B)\\) must be lower than \\(P(A)\\).\n\nKahneman, Daniel. Thinking, Fast and Slow (p. 158). Farrar, Straus and Giroux."
  },
  {
    "objectID": "3-probability/01-chance-intro/slides.html#coin-tosses",
    "href": "3-probability/01-chance-intro/slides.html#coin-tosses",
    "title": "Introducing Probability",
    "section": "Coin tosses",
    "text": "Coin tosses\n\nWe can simulate coin tosses and see if the simulations justify our intuitive understanding of what happens when we toss a fair coin. Go over code and review set.seed() and sample(). Maybe change arguments and see what happens.\n\nLet’s simulate tossing a coin ten times:\n\nCodeset.seed(12345)\n\n\ncoin &lt;- c(\"Heads\", \"Tails\")\n\ntosses &lt;- sample(coin, 10, replace = TRUE)\n\ndata.frame(tosses) %&gt;%\n  group_by(tosses) %&gt;% \n  summarise(n = n())\n\n# A tibble: 2 × 2\n  tosses     n\n  &lt;chr&gt;  &lt;int&gt;\n1 Heads      3\n2 Tails      7"
  },
  {
    "objectID": "3-probability/01-chance-intro/slides.html#coin-tosses-1",
    "href": "3-probability/01-chance-intro/slides.html#coin-tosses-1",
    "title": "Introducing Probability",
    "section": "Coin tosses",
    "text": "Coin tosses\n\nComment on the fact that it is not a 50-50 split. DON’T FORGET TO SCROLL DOWN!!\n\nWhat about if we toss the coin fifty times?\n\nCodeset.seed(12345)\n\ntosses &lt;- sample(coin, 50, replace = TRUE)\n\ndata.frame(tosses) %&gt;%\n  group_by(tosses) %&gt;% \n  summarise(n = n())\n\n# A tibble: 2 × 2\n  tosses     n\n  &lt;chr&gt;  &lt;int&gt;\n1 Heads     15\n2 Tails     35\n\n\nThis doesn’t look so good… Maybe tossing five hundred times will improve the split:\n\nCodeset.seed(12345)\n\ntosses &lt;- sample(coin, 500, replace = TRUE)\n\ndata.frame(tosses) %&gt;%\n  group_by(tosses) %&gt;% \n  summarise(n = n())\n\n# A tibble: 2 × 2\n  tosses     n\n  &lt;chr&gt;  &lt;int&gt;\n1 Heads    251\n2 Tails    249\n\n\nWe see that as the number of tosses increases, the split of heads and tails begins to look closer to 50-50."
  },
  {
    "objectID": "3-probability/01-chance-intro/slides.html#looking-at-proportions",
    "href": "3-probability/01-chance-intro/slides.html#looking-at-proportions",
    "title": "Introducing Probability",
    "section": "Looking at proportions:",
    "text": "Looking at proportions:\nHere is a plot of the proportion of tosses that land heads when we toss a coin \\(n\\) times, where \\(n\\) varies from \\(1\\) to \\(1000\\).\n\nBe sure to explain what they are looking at. We see that at the beginning, for a small number of tosses, the proportion of times that the coin lands heads is all over the place, but it eventually settles down to be around 0.5. This verifies our intuition that if we toss a fair coin, the proportion of times that the coin lands heads should be about 0.5. This idea of the probability of a particular outcome as a long run proportion of times that we see that outcome is called the frequentist theory of probability, and we will be using this theory in our class. (A different theory of probability uses a subjective notion of probability, but we won’t get into that at this time.) We will think about the probability of heads as the long-run relative frequency, or the proportion of times the coin will land heads if we toss it many, many times. This fits with our intuition that if we have a fair coin, that means that each of the two possible outcomes will occur roughly the same number of times when we toss it over and over again. This is the justification for calling the two outcomes equally likely and allowing us to define the probability of heads to be 1/2.\n\n\nCodeset.seed(12345)\ncoin1 &lt;- c(\"H\", \"T\")\ntoss_num &lt;- 1000 # total number of tosses \ntosses &lt;- sample(coin1, size = toss_num, replace = TRUE) \n# # simulating tossing a coin 1000 times\n\nprop_heads &lt;- cumsum(tosses == \"H\") / 1:toss_num\nprop_heads &lt;- data.frame(cbind(prop_heads, 1:toss_num))\nprop_heads &lt;- rename(prop_heads, toss_num = V2)\n#  computing the fraction of tosses that land heads for each n and \n#  making the resultant vector into a data frame and renaming the columns\n\nprop_heads %&gt;% \n  ggplot(aes(y = prop_heads, x = toss_num)) +\n  geom_line() + \n  annotate(\"segment\", x = 0, xend = max(toss_num) + 50, y = 0.5, yend = 0.5,\n           color = \"red\", linetype = 2, linewidth = 0.9) + \n  ylim(c(0,1)) +\n  xlab(\"Number of tosses\") +\n  ylab(\"Proportion of tosses that land heads\") +\n  ggtitle(\"As we increase the number of tosses, \\n the proportion of heads settles at about 50% \")"
  },
  {
    "objectID": "3-probability/01-chance-intro/slides.html#section",
    "href": "3-probability/01-chance-intro/slides.html#section",
    "title": "Introducing Probability",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nSuppose Ali and Bettina are playing a game, in which Ali tosses a fair coin \\(n\\) times, and Bettina wins one dollar from Ali if the proportion of heads is less than 0.4. Ali lets Bettina decide if \\(n\\) is 10 or 100.\n\nWhich \\(n\\) should Bettina choose?\n\n\nThis ties into the plot of proportions of heads as the number of coin tosses increases. Hopefully Bettina realizes she has a better chance of getting the number of heads far away from 0.5 with fewer tosses."
  },
  {
    "objectID": "3-probability/01-chance-intro/slides.html#section-1",
    "href": "3-probability/01-chance-intro/slides.html#section-1",
    "title": "Introducing Probability",
    "section": "",
    "text": "Part 1: Suppose we roll a die 4 times. The chance that we see six (the face with six spots) at least once is given by \\(\\displaystyle \\frac{1}{6} +  \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} =  \\frac{4}{6} = \\frac{2}{3}\\)\n\nTrue or false?\n\n\nPart 2: Suppose we roll a pair of dice 24 times. The chance that we see a pair of sixes at least once is given by \\(\\displaystyle 24 \\times \\frac{1}{36} =  \\frac{24}{36} = \\frac{2}{3}\\)\n\nTrue or false?\n\n\nCodecountdown::countdown(2, bottom = 0)\n\n\n\n−&plus;\n\n02:00\n\n\n\n\nBoth parts are false since we are using the addition rule on events that are not mutually exclusive. It is important that you do NOT talk about what the actual probability is since that uses the multiplication rule and we have not discussed that in the notes. This is more an exercise in when not to use the addition rule."
  },
  {
    "objectID": "3-probability/01-chance-intro/slides.html#section-2",
    "href": "3-probability/01-chance-intro/slides.html#section-2",
    "title": "Introducing Probability",
    "section": "",
    "text": "Codecountdown::countdown(1, bottom = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\nConsider the Venn diagram below, which has 20 possible outcomes in \\(\\Omega\\), depicted by the purple dots. Suppose the dots represent equally likely outcomes. What is the probability of \\(A\\) or \\(B\\) or \\(C\\)? That is, what is \\(P(A \\cup B \\cup C)\\)?\n\n\n\n\n\nAsk them what is the quickest way to do this. Maybe even give them only 15 or 30 seconds."
  },
  {
    "objectID": "assets/final-exam-slide.html",
    "href": "assets/final-exam-slide.html",
    "title": "Final Exam",
    "section": "",
    "text": "🧹 Clear your desk of everything except a pen/pencil, your cheat sheet, and your ID.\n🧢 Remove hats, hoods, and sunglasses.\n📱 All electronic devices (laptop, phone, airbuds, smart watch) should be remain in your bag for the entire class period. All items should be on the floor not on nearby seats.\nSpread out as much as possible. We may reseat you to space people out.\nWhen you get an exam, do not begin.\n\n\n\n\n✅ All answers must be marked on the answer sheet.\n👀 If a proctor sees your eyes wandering to other students’ work, you will be reseated. If it happens again, it is considered academic misconduct.\nIf you need to use the bathroom, bring phone and exam materials to the front of the class.\n⌛ When you are done with this exam, please bring your exam, answer sheet, and cheat sheet to a proctor at the front of the class.\n\n\n\nGood luck! 🍀\n\nCodecountdown::countdown(90, font_size = \"1.5em\")\n\n\n\n−&plus;\n\n90:00\n\n\n\n\nCodecountdown::countdown(90, bottom = 35, font_size = \"1.5em\")\n\n\n\n−&plus;\n\n90:00"
  },
  {
    "objectID": "assets/final-exam-slide.html#section",
    "href": "assets/final-exam-slide.html#section",
    "title": "Final Exam",
    "section": "",
    "text": "🧹 Clear your desk of everything except a pen/pencil, your cheat sheet, and your ID.\n🧢 Remove hats, hoods, and sunglasses.\n📱 All electronic devices (laptop, phone, airbuds, smart watch) should be remain in your bag for the entire class period. All items should be on the floor not on nearby seats.\nSpread out as much as possible. We may reseat you to space people out.\nWhen you get an exam, do not begin.\n\n\n\n\n✅ All answers must be marked on the answer sheet.\n👀 If a proctor sees your eyes wandering to other students’ work, you will be reseated. If it happens again, it is considered academic misconduct.\nIf you need to use the bathroom, bring phone and exam materials to the front of the class.\n⌛ When you are done with this exam, please bring your exam, answer sheet, and cheat sheet to a proctor at the front of the class.\n\n\n\nGood luck! 🍀\n\nCodecountdown::countdown(90, font_size = \"1.5em\")\n\n\n\n−&plus;\n\n90:00\n\n\n\n\nCodecountdown::countdown(90, bottom = 35, font_size = \"1.5em\")\n\n\n\n−&plus;\n\n90:00"
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/extra-practice.html",
    "href": "2-summarizing-data/07-multiple-linear-regression/extra-practice.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Load the penguins dataset into R and answer the following questions. Where code is needed, you may write it down in the space given.\n\nQuestion 1\nFit the multiple linear regression model from today’s slides, but instead having \"Chinstrap\" as the reference level in the species variable. Interpret the coefficient corresponding to the \"Adelie\" indicator variable in the context of the problem.\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWrite out the mathematical equation of the line whose coefficients are given by your model output.\n\n\n\n\n\n\nQuestion 3\nNow, consider the relationship between bill length and bill depth when broken down by species. Sketch (do not code) a plot which can be used to summarize the relationship between all three variables. Depict a shape which reflects your expectation of the phenomenon. State the aesthetic mappings and geometry involved in the plot. Finally, label your axes and give the plot a title."
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/slides.html",
    "href": "2-summarizing-data/07-multiple-linear-regression/slides.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(stat20data)"
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/slides.html#agenda",
    "href": "2-summarizing-data/07-multiple-linear-regression/slides.html#agenda",
    "title": "Multiple Linear Regression",
    "section": "Agenda",
    "text": "Agenda\n\nAnnouncements\nMultiple Linear Regression Refresher\nQuiz Review (this week’s notes)\nBreak\nLab 2.2 (extended)"
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/slides.html#announcements",
    "href": "2-summarizing-data/07-multiple-linear-regression/slides.html#announcements",
    "title": "Multiple Linear Regression",
    "section": "Announcements",
    "text": "Announcements\n\nQuiz 1 is Monday, in-class and covers all lectures from the beginning to the semester until today.\n\n. . .\n\nLab 2.2, Problem Set 4 and Problem Set 5 are due Tuesday 9am\n\nMake sure you follow Lab Submission Guidelines on Ed\n\n\n\n. . .\n\nRQ: Introducing Probability due on Monday/Tuesday at 11:59pm; Probability unit begins next week\n\n. . .\n\n\nExtra Practice for Multiple Linear Regression added to the resources tab on the course home page."
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/slides.html#question-1",
    "href": "2-summarizing-data/07-multiple-linear-regression/slides.html#question-1",
    "title": "Multiple Linear Regression",
    "section": "Question 1",
    "text": "Question 1\n\nCodem1 &lt;- lm(bill_depth_mm ~ bill_length_mm, data = penguins)\n\n\n. . .\n\nCodem2 &lt;- lm(bill_depth_mm ~ bill_length_mm + body_mass_g + species, \n         data = penguins)\n\n\n\nHow many more coefficients does the second model have than the first?\n\n\nRemind students that they need to remember whether species and body_mass_g are numerical or categorical. Students should know how many species there are (three).\nOne addl. coefficient for body mass; two addl. for species (one less than the number of species, which is three). This gives a total of three more coefficients."
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/slides.html#question-2",
    "href": "2-summarizing-data/07-multiple-linear-regression/slides.html#question-2",
    "title": "Multiple Linear Regression",
    "section": "Question 2",
    "text": "Question 2\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\n\nCodem2\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + body_mass_g + species, \n    data = penguins)\n\nCoefficients:\n     (Intercept)    bill_length_mm       body_mass_g  speciesChinstrap  \n        10.33083           0.09484           0.00117          -0.90748  \n   speciesGentoo  \n        -5.80117  \n\n\n\nWhich is the correct interpretation of the coefficient in front of bill length? Select all that apply.\n\n\nThis one assesses their ability to use a conditional interpretation of a regression coefficient (controlling for the other variables in the model…).\nThe correct interpretation controls for both other variables (body mass and species), and has the x variable and the y variable in the correct places."
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/slides.html#question-3",
    "href": "2-summarizing-data/07-multiple-linear-regression/slides.html#question-3",
    "title": "Multiple Linear Regression",
    "section": "Question 3",
    "text": "Question 3\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\n\nCodem2\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + body_mass_g + species, \n    data = penguins)\n\nCoefficients:\n     (Intercept)    bill_length_mm       body_mass_g  speciesChinstrap  \n        10.33083           0.09484           0.00117          -0.90748  \n   speciesGentoo  \n        -5.80117  \n\n\n\nWhich is the correct interpretation of the coefficient in front of Gentoo?\n\n\nThis one assesses their ability to use a conditional interpretation of a regression coefficient (controlling for the other variables in the model…)\nThe correct interpretation needs to involve a comparison to the reference level, Adelie and should not involve anything about bill length or body mass, (other than to say they should be fixed)."
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/slides.html#question-4",
    "href": "2-summarizing-data/07-multiple-linear-regression/slides.html#question-4",
    "title": "Multiple Linear Regression",
    "section": "Question 4",
    "text": "Question 4\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\n\nCodem2\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + body_mass_g + species, \n    data = penguins)\n\nCoefficients:\n     (Intercept)    bill_length_mm       body_mass_g  speciesChinstrap  \n        10.33083           0.09484           0.00117          -0.90748  \n   speciesGentoo  \n        -5.80117  \n\n\n\nHow would this linear model best be visualized?\n\n\nThree numerical variables means we will have planes in a 3D space. The indicator coefficients will shift the planes up and down. There are three species, so three parallel planes is what we are looking for."
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/slides.html#question-5",
    "href": "2-summarizing-data/07-multiple-linear-regression/slides.html#question-5",
    "title": "Multiple Linear Regression",
    "section": "Question 5",
    "text": "Question 5\nConsider the following linear regression output where the variable school is categorical and the variable hours_studied is numerical.\n\n\nCoefficients\nEstimate\n\n\n\n(Intercept)\n2.5\n\n\nhours_studied\n.2\n\n\nschoolCal\n1\n\n\nschoolStanford\n-1"
  },
  {
    "objectID": "2-summarizing-data/07-multiple-linear-regression/slides.html#question-5-cont.",
    "href": "2-summarizing-data/07-multiple-linear-regression/slides.html#question-5-cont.",
    "title": "Multiple Linear Regression",
    "section": "Question 5 (cont.)",
    "text": "Question 5 (cont.)\n\nSay I wanted to create a data frame from the original edu dataframe which contains the minimum, median, and IQR for hours_studied among each school. In order to do this, I make use of group_by() followed by summarize(). I save this data frame into an object called GPA_summary.\n\n. . .\n\nWhat are the dimensions of GPA_summary?\n\n. . .\n\nCodecountdown::countdown(minutes = 1)\n\n\n\n−&plus;\n\n01:00\n\n\n\n. . .\n\nThe correct answer in the poll should be 3x4. The three rows are for the three levels in the school category (there is an additional level beyond \"Cal\" and \"Stanford\", regardless if it is not stated). One column is for the school name, the other three are for each of the statistics calculated; the three rows are for the three levels in the school category."
  },
  {
    "objectID": "2-summarizing-data/labs/03-flights/lab.html",
    "href": "2-summarizing-data/labs/03-flights/lab.html",
    "title": "Lab 2: Flights",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "2-summarizing-data/labs/03-flights/lab.html#part-i-understanding-the-context-of-the-data",
    "href": "2-summarizing-data/labs/03-flights/lab.html#part-i-understanding-the-context-of-the-data",
    "title": "Lab 2: Flights",
    "section": "Part I: Understanding the Context of the Data",
    "text": "Part I: Understanding the Context of the Data\nFor the questions on the handout, consult the image of the data frame found in the slides linked above.\n\nPart 1: Understanding the Context of the Data"
  },
  {
    "objectID": "2-summarizing-data/labs/03-flights/lab.html#part-ii-computing-on-the-data",
    "href": "2-summarizing-data/labs/03-flights/lab.html#part-ii-computing-on-the-data",
    "title": "Lab 2: Flights",
    "section": "Part II: Computing on the Data",
    "text": "Part II: Computing on the Data\nThe data for this lab can be found in the flights data frame in the stat20data package. Run ?flights at the console to learn more about the columns. Where applicable, answer each question with one pipeline, which may include dplyr code, ggplot2 code or both.\nQuestion 1\nHow many flights in the dataset left in the springtime and were destined for Portland, Oregon?\nQuestion 2\nArrange the data set to find out: which flight holds the record for longest departure delay (in hrs) and what was its destination? What was the destination and delay time (in hrs) for the flight that was least delayed, i.e. that left the most ahead of schedule? This can be done in two separate pipelines.\nQuestion 3\nI attempted to find some summary statistics on the arrival delay of flights, grouped by their airline carrier. You can find the mininmum and maximum of numerical variables using the min() and max() function, respectively. This is the code that I ran.\n\nflights |&gt;\n    group_by(carrier) |&gt;\n    summarise(min_arr_delay = min(arr_delay)) |&gt;\n    summarise(max_arr_delay = max(arr_delay)) |&gt;\n    summarise(prop_long_flight = mean(distance &gt; 1000))\n\nHowever, I received an error and the code would not run. Explain the origin of this error, then modify/correct the code and run it to display the output I was hoping to achieve.\nQuestion 4\nUsing the airport nearest your hometown, which day of the week and which airline seems best for flying there from San Francisco? If you’re from near SFO or OAK or from abroad, use Chicago as your hometown. Be clear on how you’re defining best. Feel free to mutate a column(s) to your dataset which might be your preferred way to determine best.\nThere is no explicit weekday column in this data set, but there is sufficient information to piece it together. The following line of code can be added to your pipeline to create that new column. Note also that it uses functions which are contained in the lubridate package.\n\nmutate(day_of_week =\n            wday(ymd(paste(year, month, day, set = \"-\")),\n            label = T))\n\nQuestion 5\nThe plot below shows the relationship between the number of flights going out of SFO and the average departure delay. It illustrates the hypothesis that more flights on a given day would lead to a more congested airport which would lead to greater delays on average. Each point represents single day in 2020; there are 366 of them on the plot. Form a single dplyr and ggplot2 chain that will create this plot, starting with the original data set. Hint: What does each point on the plot represent? Is it the same as the unit of observation of the initial dataset?\n\n\n\n\nQuestion 6\nCreate a histogram showing the distribution of departure delays for all flights. You must follow the steps below:\n\nSet the limits of the x-axis to focus on where most of the data lie.\nOutline the bars of your histogram with one color.\nAdd a text annotation somewhere on the plot that explains the meaning of a negative departure delay.\nFinally, title your plot with a claim about the shape and modality of the distribution.\nQuestion 7\nCreate a plot to examine the relationship between average speed and distance (you will have to make the average speed column first) in one pipeline, labeling your x and y axes. Save this plot into an object and print it to the screen.\nQuestion 8\nRewrite the code for the plot you made in Question 7, coloring the points by destination, and use this to provide a possible explanation for the relationship between the two variables (particularly, the noticeable gap) in the plot title.\nQuestion 9\n\nFirst, calculate the correlation coefficient between average speed and distance.\n\nThen, mutate two new variables onto the data frame:\n\n(natural) log of average speed\n(natural) log of distance\n\n\nVisualize the new variables and title your plot with a claim based on what you see.\nFinally, calculate the correlation coefficient between log of average speed and log of distance.\n\nThe process of applying functions to existing variables (such as logarithm or squaring) in a dataset is often called transforming one’s data.\nQuestion 10\nExplain which pair of variables have a relationship which is better described via a linear model:\n\ndistance and average speed\nlog distance and log average speed\n\nExplain in one or two sentences, using your results from Question 7 and Question 9 as support.\nQuestion 11\nFit a linear model to the variables you chose from Question 10. Then determine which flight had the fastest average speed given its distance traveled.\nQuestion 12\nFit a multiple linear regression model that explains arrival delay using departure delay and the distance of the flight and print out the coefficients (the intercept and two slopes).\nQuestion 13\nOn average, which carrier’s flights had the smallest arrival delay given their departure delay and distance?\nQuestion 14\nCan we compare the coefficients for departure delay and distance to understand which has the stronger relationship? Why or why not? State your answer in 2-3 sentences."
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/slides.html",
    "href": "2-summarizing-data/labs/01-arbuthnot/slides.html",
    "title": "Lab: Arbuthnot",
    "section": "",
    "text": "Put concepts into practice with real data\nFollow best practices in reproducible analyses (.qmd)\nIn class we’ll work on the most challenging problems\nYou likely will need to work outside of class to finish the assignment\nVisit office hours for extra help or ask on named thread on Ed"
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/slides.html#the-goal-of-labs",
    "href": "2-summarizing-data/labs/01-arbuthnot/slides.html#the-goal-of-labs",
    "title": "Lab: Arbuthnot",
    "section": "",
    "text": "Put concepts into practice with real data\nFollow best practices in reproducible analyses (.qmd)\nIn class we’ll work on the most challenging problems\nYou likely will need to work outside of class to finish the assignment\nVisit office hours for extra help or ask on named thread on Ed"
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/slides.html#section",
    "href": "2-summarizing-data/labs/01-arbuthnot/slides.html#section",
    "title": "Lab: Arbuthnot",
    "section": "",
    "text": "What is the chance that a child born tomorrow is a girl?\n\n\nMost students will volunteer 50%. Follow up with:\n\nWhy do you think it’s 50%? How did you come to that number? - Did you read it somewhere? - Did you hear it from someone?\n\n\nIf you wanted to confirm it was 50% what would you do? - [likely answer: google it]\n\n\nWhat source for that number would you likely come across if you kept digging? - ultimately, maybe some scientific article.\n\n\nWhat would their evidence for that claim of 50%? - likely vast amounts of demographic / census data\n\n\n. . .\n\n50%?\n\n. . .\nWhat evidence do we ultimately rely upon?\n. . .\n\nVast amounts of data\n\n\nNow consider the case of a scientist who tackled this question long before google was around."
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/slides.html#dr.-john-arbuthnot",
    "href": "2-summarizing-data/labs/01-arbuthnot/slides.html#dr.-john-arbuthnot",
    "title": "Lab: Arbuthnot",
    "section": "Dr. John Arbuthnot",
    "text": "Dr. John Arbuthnot\n\n\n\n1667 - 1735\nScottish physician, mathematician, satirist\nInterested in the question of what the proportion of girls to boys was at birth\n\n\n\n\nA painted portrait of John Arbuthnot\n\n\n\n\n\nRead a bit about Arbuthnot on wikipedia to learn some of his backstory."
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/slides.html#arbuthnots-london",
    "href": "2-summarizing-data/labs/01-arbuthnot/slides.html#arbuthnots-london",
    "title": "Lab: Arbuthnot",
    "section": "Arbuthnot’s London",
    "text": "Arbuthnot’s London\n\n\n\n1710, St. Paul’s Cathedral completed\nVery few paved streets\nDefinitely no google\n\n\n\n\nA painting of London in 1710.\n\n\n\n\n\nArbuthnot was living in London while thinking about this problem. It was a period of rapid growth and modernization for the city but it still had no google.\nMain point to get across here: what we take for granted in terms of how we reason from data was nearly absent from life in 18th century UK. Most people would reason from direct experience, anecdote, appeals to tradition or religion, etc.\nIf you were Dr. Arbuthnot and you tell the person on the street: “My wife will soon be giving birth. What are the chances it’s a girl?”, what sort of answer do you think he might get? What type of information might that person be relying upon? - their own experience (the ratio of girls born in their family) - anecdotes that they’ve heard from others\nYou may want to note that even our notion of “chance” and probability was not wide spread at the time.\nArbuthnot’s work is notable because he takes the big step of realizing that an individual can learn a lot by pooling the experiences / anecdotes of others in a systematic way."
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/slides.html#section-1",
    "href": "2-summarizing-data/labs/01-arbuthnot/slides.html#section-1",
    "title": "Lab: Arbuthnot",
    "section": "",
    "text": "Where could Arbuthnot find vast amounts on systematically collected data regarding births?\n\n. . .\n\nThe church."
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/slides.html#section-2",
    "href": "2-summarizing-data/labs/01-arbuthnot/slides.html#section-2",
    "title": "Lab: Arbuthnot",
    "section": "",
    "text": "A photograph of a page of a book of handwritten christening records taken a church in England in the 18th century\n\n\nThe church is the only organization that is collecting systematic demographic data in this era."
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/slides.html#section-3",
    "href": "2-summarizing-data/labs/01-arbuthnot/slides.html#section-3",
    "title": "Lab: Arbuthnot",
    "section": "",
    "text": "A zoomed in version of the christening records, where you can identify that each record was a single christening.\n\n\nMost children, shortly after they’re born, are taken to the nearby parish church and “christened” - given a name in the church. The parish churches record the name and date of each of these christenings.\nArbuthnot went from parish church to parish church in London, pored over these records and tallied the number of boys and girls. He then combined these counts across all of the parishes and created a data set that we can read into R today."
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/slides.html#what-is-a-christening-record",
    "href": "2-summarizing-data/labs/01-arbuthnot/slides.html#what-is-a-christening-record",
    "title": "Lab: Arbuthnot",
    "section": "What is a christening record?",
    "text": "What is a christening record?\n. . .\nA Christening is a ceremony/rite in the Church of England where:\n\nThe parents bring their new born child to a priest at the church.\nAs part of the ceremony, the parents give a first name to the child before the child is baptized (inducted into the church).\nThe name of the child and their parents are recorded in a ledger.\n\n. . .\nJohn Arbuthnot tabulated the total count of names in each year that were traditionally female and male names."
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/slides.html#your-lab-assignment",
    "href": "2-summarizing-data/labs/01-arbuthnot/slides.html#your-lab-assignment",
    "title": "Lab: Arbuthnot",
    "section": "Your Lab Assignment",
    "text": "Your Lab Assignment\n\n\nLab 1.1: Understanding the Context of the Data\n\nLearn context\nFormulate questions\nSet expectations\nSubmitted as pdf of worksheet\n\n\n\nLab 1.2: Computing with the Data\n\nDive into the data\nSubmitted as a pdf from a qmd."
  },
  {
    "objectID": "2-summarizing-data/labs/01-arbuthnot/learning-objectives.html",
    "href": "2-summarizing-data/labs/01-arbuthnot/learning-objectives.html",
    "title": "Stat 20",
    "section": "",
    "text": "Note: this is leftover verbiage from a previous iteration of this lab. It can be adapted form the learning objectives.\n\nThe Stucture of Lab Assignments\nQuestions in Part I deal with the context in which Arbuthnot collected his data. These questions should should be answered before you have looked at the data itself. In general, in Part I you will identify the question of interest, consider the manner in which it arose, and set expectations for the shape and structure of the data.\nPart II is where you get your hands on the data and consider where it aligns with and diverges from your expections from Part I. Part III features extensions of the ideas in Part I and Part II, often to a new data set.\nYour work should feature writing that is clear and in full sentences. Your document should be formatted cleanly, with appropriate use of headers, body text, and lists. Your code should be clear and simple, with no extraneous code.\nCertain questions on the labs in this class call for speculation or for your opinion. There may not be a single correct answer, but some are more reasonable and thoughtful than others. You’re encouraged to talk these questions through with your peers and course staff during lab sessions, evening study session, and office hours.\n\n\n\nConcepts\n\nProposing a research question and articulating the evidence (data and other) that could bear on that question.\nIdentification of the unit of observation and the names and types of variables.\nCreating a statistical graphic that can answer a research question and interpreting the observed structure in the data.\nAssessing the degree to which a statistical graphic supports a claim / answers a question.\n\n\n\nSkills from the R Workshop\n\nRStudio Terminology\n\nConsole\nEnvironment\nEditor\nFile Directory\n\n\n\nR/RStudio Concepts\n\nPrinting to the console vs saving to the environment\nR scripts as final draft of code, console as the sandbox \n\n\n\nR Functions\n\n+, -, *, /, ^\n&lt;-\n?\nlibrary()\nc()\nclass()\nsum()\nmean()\ndata()\ntibble()\nselect()\narrange()\nmutate()\n\n\n\n\n\nSkills from Thursday lab\n\nloading arbuthnot\nloading tidyverse\npractice with select(), mutate(), and arrange()\nmaking a line plot using ggplot"
  },
  {
    "objectID": "2-summarizing-data/labs/02-class-survey/glimpse-class-survey.html",
    "href": "2-summarizing-data/labs/02-class-survey/glimpse-class-survey.html",
    "title": "Class Survey Data Frame",
    "section": "",
    "text": "Rows: 619\nColumns: 31\n$ year                   &lt;chr&gt; \"I'm in my second year.\", \"This is my first sem…\n$ major                  &lt;chr&gt; \"Economics\", \"Intended Economics major\", \"Econo…\n$ coding_exp_words       &lt;chr&gt; \"None.\", \"None.\", \"Some. I've had to write code…\n$ coding_exp_scale       &lt;dbl&gt; 1, 1, 7, 6, 2, 5, 4, 8, 1, 5, 1, 6, 5, 7, 4, 1,…\n$ calculus               &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\"…\n$ favorite_thing_cal     &lt;chr&gt; \"My favorite thing about Cal is that everyone i…\n$ body_piercings         &lt;dbl&gt; 2, 2, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 2, 0,…\n$ pets_plus_siblings     &lt;chr&gt; \"I have 1 pet and 4 siblings.\", \"1 sibling. 0 p…\n$ diet                   &lt;chr&gt; \"I eat both plants and animals.\", \"I eat both p…\n$ title                  &lt;chr&gt; \"Entrepreneur\", \"Social Scientist\", \"Entreprene…\n$ study_place            &lt;chr&gt; \"My favorite place to study on campus is Chou H…\n$ olympic_sport          &lt;chr&gt; \"Snowboarding\", \"I am not excited to watch the …\n$ season                 &lt;chr&gt; \"Summer\", \"Summer\", \"Winter\", \"Fall\", \"Spring\",…\n$ beach_or_mtns          &lt;chr&gt; \"At the beach\", \"At the beach\", \"In the mountai…\n$ coffee_or_tea          &lt;chr&gt; \"Tea\", \"Tea\", \"Tea\", \"Coffee\", \"I don't drink e…\n$ best_boba_shop         &lt;chr&gt; \"The best boba tea shop in Berkeley is Taiwanes…\n$ tech                   &lt;dbl&gt; 1, 3, 7, 7, 7, 7, 5, 4, 4, 3, 7, 4, 4, 8, 5, 6,…\n$ climate_change         &lt;dbl&gt; 10, 6, 6, 6, 7, 8, 7, 8, 7, 7, 8, 4, 6, 6, 10, …\n$ crypto                 &lt;dbl&gt; 10, 8, 3, 4, 6, 3, 2, 3, 8, 3, 7, 4, 2, 6, 7, 6…\n$ new_COVID_variant      &lt;dbl&gt; 0.25000, 0.10000, 0.00000, 0.20000, 0.90000, 0.…\n$ fire_alarm             &lt;dbl&gt; 1.00, 0.50, 1.00, 0.05, 0.60, 0.50, 0.20, 1.00,…\n$ are_hotdogs_sandwiches &lt;chr&gt; \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\", …\n$ horse_duck             &lt;chr&gt; \"Duck-sized horse\", \"Duck-sized horse\", \"Duck-s…\n$ dog_pants              &lt;chr&gt; \"Right image\", \"Right image\", \"Right image\", \"R…\n$ coding_langauges       &lt;chr&gt; \"I haven't coded before.\", \"I haven't coded bef…\n$ artist                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ humanist               &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ entrepreneur           &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, T…\n$ social_scientist       &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ natural_scientist      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE,…\n$ comp_scientist         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, …"
  },
  {
    "objectID": "2-summarizing-data/labs/02-class-survey/lab.html",
    "href": "2-summarizing-data/labs/02-class-survey/lab.html",
    "title": "Lab 1: Class Survey",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "2-summarizing-data/labs/02-class-survey/lab.html#part-i-understanding-the-context-of-the-data",
    "href": "2-summarizing-data/labs/02-class-survey/lab.html#part-i-understanding-the-context-of-the-data",
    "title": "Lab 1: Class Survey",
    "section": "Part I: Understanding the Context of the Data",
    "text": "Part I: Understanding the Context of the Data\nPlease record your answers on the worksheet below and upload it to Gradescope.\n\nPart 1: Understanding the Context of the Data\n\nTo answer this worksheet, consult the printout of the original survey found here."
  },
  {
    "objectID": "2-summarizing-data/labs/02-class-survey/lab.html#part-ii-computing-on-the-data",
    "href": "2-summarizing-data/labs/02-class-survey/lab.html#part-ii-computing-on-the-data",
    "title": "Lab 1: Class Survey",
    "section": "Part II: Computing on the Data",
    "text": "Part II: Computing on the Data\nThe real data from your surveys is available in the stat20data package as class_survey. To complete the following questions, you will need to find the columns in the class_survey data frame associated with each variable mentioned.\nQuestion 1\n\nHow much coding experience do students have?\n\nUse the “scale” version for this question.\npart a\nConstruct an appropriate plot for this variable using ggplot2.\npart b\nThen calculate one appropriate measure of a spread and one appropriate measure of center for this variable.\npart c\nUse these summaries to answer the survey question in a sentence or two.\nQuestion 2\nFor this question only, you may use the following, modified dataset, which can be obtained by running the following code in R.\n\nfiltered_survey &lt;- filter(class_survey, \n                          new_COVID_variant &gt;= 0,\n                          new_COVID_variant &lt; 1)\n\n\nWhat are students’ perceptions of the chance that there is a new COVID variant that disrupts instruction in Fall 2022?\n\npart a\nConstruct an appropriate plot for this variable using ggplot2.\npart b\nThen calculate one appropriate measure of a spread and one appropriate measure of center for this variable.\npart c\nUse these summaries to answer the survey question in a sentence or two.\nQuestion 3\n\nIs there an association between students’ favorite season and terrain preference (beach or mountains)?\n\npart a\nConstruct the plot that you proposed in Part I using ggplot2. Ensure that the seasons are plotted in the order: spring, summer, fall, winter.\npart b\nThen, state a measure of a typical observation of the terrain preference variable for each season group (what terrain does a typical student whose favorite season is fall prefer, and so on).\npart c\nUse these summaries to answer the survey question in a sentence or two.\nQuestion 4\n\nIs there an association between students most identifying as an entrepreneur and their optimism for cryptocurrency?\n\nYou may treat the entrepreneur variable categorically.\npart a\nConstruct an appropriate plot including both of these variables using ggplot2.\npart b\nThen calculate one appropriate measure of a spread and one appropriate measure of center for the cryptocurrency variable, grouping by the levels of the entrepreneur variable.\npart c\nUse these summaries to answer the survey question in a sentence or two.\nQuestion 5\nSix variables appear in the survey data frame that were derived from the original title question: artist, humanist, natural_scientist, social_scientist, comp_scientist and entrepreneur. The artist variable is TRUE for those students who most identified as an artist and FALSE otherwise. The other five variables are defined similarly. You may treat these variables categorically.\npart a\nPropose your own question involving any two variables in the dataset–one of which comes from the title variable group mentioned above.\npart b\nConstruct a plot which would aid in answering this question using ggplot2.\npart c\nGrouping by the levels of the title group variable, calculate or state one appropriate measure of center for your second variable.\npart d\nUse these summaries to answer your question in part a in a sentence or two."
  },
  {
    "objectID": "2-summarizing-data/labs/02-class-survey/lab-context.html",
    "href": "2-summarizing-data/labs/02-class-survey/lab-context.html",
    "title": "Stat 20",
    "section": "",
    "text": "What is the unit of observation in the survey of students in Stat 20?\n\n\n\n\n\nWhere in the Taxonomy of Data are the variables that correspond to each of the following survey questions (i.e., what is their type)?\n\nQuestion 8: What is your favorite thing about Cal?\nQuestion 16: Where would you rather be?\nQuestion 23: What is the chance that a fire alarm goes off in one of the dorms this week?\n\n\n\n\n\n\nWhat values would you expect major to take? Do you anticipate any challenges when analyzing this data (and why)?\n\n\n\n\n\nAre there any other variables in the dataset which would present the same challenge you mentioned in Question 3? List them here.\n\n\n\n\nQuestions 5 - 7 can be answered using data visualization. To lay the groundwork for answering these questions using the glimpse of the survey data set that you have, plan out your plots and set your expectations for what the data might look like. Please:\n\nsketch (with paper and pencil) a plot that captures the distribution or relationship between the variables,\nlabel the axis and add axis tick marks with plausible values,\ndepict a shape that reflects your expectation of the phenomena. There is no right or wrong at this point, because in theory, you have not seen the full dataset yet! You do not need to retroactively change your answers once you do see it.\ntitle your plot with a claim based on the sketch you have made.\n\n\nDo students prefer to spend time at the beach or in the mountains?\n\n\n\nIs there an association between students’ favorite season and terrain preference (beach or mountains)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow many students who preferred the mountains selected Winter as their favorite season?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsider the question:\n\n\nHow much coding experience do students have?\n\nImagine you were working with the “words” version of this variable as shown in the glimpse and wanted to make a one-variable bar chart using this data. Do you anticipate any possible challenges with your visualization properly representing the scale in words, and why?"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "Announcements\nConcept Questions and Activity\nProblem Set 3\nBreak\nCoding Refresher\nLab 1.2: Computing on the Data"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#agenda",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#agenda",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "Announcements\nConcept Questions and Activity\nProblem Set 3\nBreak\nCoding Refresher\nLab 1.2: Computing on the Data"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#announcements",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#announcements",
    "title": "Summarizing Numerical Data",
    "section": "Announcements",
    "text": "Announcements\n\n\nRQ: Data Pipelines released Friday afternoon and due Monday at 11:59pm\n\n\n. . .\n\nProblem Sets 2 (Tue/Wed) and 3 (today) due Tuesday at 9am\n\n\n. . .\n\nLab 1: Class Survey (both parts) due Tuesday at 9am\n\n\n. . .\n\nGroup Tutoring Sessions started yesterday in Evans Hall!\n\n. . .\n\nWe slightly changed Problem 7, Lab 1.1"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#describing-shape",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#describing-shape",
    "title": "Summarizing Numerical Data",
    "section": "Describing Shape",
    "text": "Describing Shape\n\nWhich of these variables do you expect to be uniformly distributed?\n\nbill length of Gentoo penguins\nsalaries of a random sample of people from California\nhouse sale prices in San Francisco\nbirthdays of classmates (day of the month)\n\nPlease vote at pollev.com.\n\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#mean-median-mode-which-is-best",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#mean-median-mode-which-is-best",
    "title": "Summarizing Numerical Data",
    "section": "Mean, median, mode: which is best?",
    "text": "Mean, median, mode: which is best?\n. . .\n\nIt depends on your desiderata: the nature of your data and what you seek to capture in your summary.\n\n. . .\n\nGet out a piece of paper. You’ll be watching a 3 minute video that discusses characteristics of a typical human. Note which numerical summaries are used and what for."
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#general-advice",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#general-advice",
    "title": "Summarizing Numerical Data",
    "section": "General Advice",
    "text": "General Advice\n. . .\n\n\nMeans are often a good default for symmetric data.\n\n. . .\n\n\nMeans are sensitive to very large and small values, so can be deceptive on skewed data. &gt; Use a median\n\n. . .\n\n\nModes are often the only option for categorical data.\n\n. . .\nBut there are other notions of typical… what about a maximum?"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#section-5",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#section-5",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "Why are measures of spread so important? Consider the following question.\n\n. . .\n\nThere are two new food delivery services that open in Berkeley: Oski Eats and Cal Cravings. A friend of yours that took Stat 20 collected data on each and noted that Oski Eats has a mean delivery time of 29 minutes and Cal Cravings a mean delivery time of 27 minutes. Which would would you rather order from?\n\n\nDiscuss this question with your classmates! (no poll question).\n\n. . .\n\nCodecountdown::countdown(minutes = 1, top = 0)\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#one-possible-reality",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#one-possible-reality",
    "title": "Summarizing Numerical Data",
    "section": "One possible reality",
    "text": "One possible reality\n\nCodeset.seed(513)\noski &lt;- rnorm(23, 0, 2)\noski &lt;- oski + (29 - mean(oski))\ncal &lt;- rnorm(23, 0, 8)\ncal &lt;- cal +  (27 - mean(cal))\ndf &lt;- tibble(service = rep(c(\"oski\", \"cal\"), each = 23),\n             time = c(oski, cal))\np1 &lt;- ggplot(df, aes(x = time)) +\n  geom_histogram(col = \"white\") + \n  facet_wrap(vars(service), nrow = 2) +\n  theme_gray(base_size = 20)\np1"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/slides.html#section-6",
    "href": "2-summarizing-data/02-summarizing-numerical-data/slides.html#section-6",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "Codep1 + \n  geom_vline(xintercept = 27, color = \"blue\", lty = 1, lwd = 2) +\n  geom_vline(xintercept = 29, color = \"goldenrod\", lty = 1, lwd = 2)\n\n\n\n\n\n\n\n\nWould you still prefer to order from Cal?"
  },
  {
    "objectID": "2-summarizing-data/02-summarizing-numerical-data/ps.html",
    "href": "2-summarizing-data/02-summarizing-numerical-data/ps.html",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "Below is a smaller version of the data from a future lab called flights_mini. It contains all flights out of Oakland (OAK) from December 2020. This data frame is used to create the plot that follows. distance refers to the distance a given plane travels on its flight, measured in miles. carrier refers to the carrier code for a specific airline.\n\n\n\n\n\n\n\n\n\nWhich of the following interpretations of the plot above are true? (select all that apply)\n\n\nThe carrier with the most heavily skewed distance distribution is HA.\nThe median distance of the flights operated by DL, G4, and OO are roughly equivalent.\nThe minimum distance traveled in this data set is roughly 200.\nThere is no clear association between the carrier and the distance of their flights.\nThe carrier with the greatest variability in distance, as measured by the IQR, is AS.\n\nConsider the small data set from the notes.\n\\[ 6 \\quad 7 \\quad 7 \\quad  7 \\quad 8 \\quad  8 \\quad 9 \\quad 9 \\quad 10 \\quad 11  \\quad 11\\]\n\nThe data set above was measured in meters, but what would have happened if it had been measured in decimeters (10 decimeters to a meter)? Provide reasoning for would happen to the measures of center - mean, median, mode - if it had instead been measured in decimeters. Repeat the exercise for three measures of spread: range, standard deviation, and IQR. Which measures remain the same after a multiplicative change in units?\n\n\n\nSketch your best sense of the distribution of the following variable(s). For each, please:\n\n\nUse a form of statistical graphic that emphasizes the important elements of the distribution.\nLabel the axes and provide plausible values for the tick marks.\nDescribe in words the shape of the distribution.\nState which measure of center and spread would be most appropriate and approximate their values.\n\nMake a note of any assumptions you’re making in interpreting these variable names.\nNumber of body piercings among Stat 20 students\n\nScores on an easy quiz among Stat 20 students\n\nThe mpg dataset is available as a part of the tidyverse library. It contains information on fuel consumption for 38 models of car between 1999 and 2008. Datasets can have help files, too! You do not need to include code for loading in libraries or accessing help files in your answers to the below questions.\n\nWrite dplyr code to calculate the median and IQR city miles per gallon for the vehicles in the dataset and copy it below. The result of your code should be one data structure.\n\n\n\nWrite dplyr code to calculate the mean and standard deviation city miles per gallon for the vehicles in the dataset for each class of car and copy it below. The result of your code should be one data structure."
  },
  {
    "objectID": "2-summarizing-data/05-communicating-with-graphics/notes.html",
    "href": "2-summarizing-data/05-communicating-with-graphics/notes.html",
    "title": "Communicating with Graphics",
    "section": "",
    "text": "At this point in the course, you have a bevy of different types of statistical graphics under your belt: scatterplots, histograms, dot plots, violin plots, box plots, density curves, and bar plots of several kinds. You also have a broad framework to explain how these graphics are composed: the Grammar of Graphics. But to what purpose? Why plot data? For whom?\nEvery time you build a plot, you do so with one of two audiences in mind.\nThe process of building understanding from a data set is one that should be driven by curiosity, skepticism, and thoughtfulness. As a data scientist, you’ll find yourself in conversation with your data: asking questions of it, probing it for structure, and seeing how it responds. This thoughtful conversation is called exploratory data analysis (or EDA).\nDuring EDA, the aim is to uncover the shape and structure of your data and to uncover unexpected features. It’s an informal iterative process where you are your own audience. In this setting, you should construct graphics that work best for you.\nAt some point, you’ll find yourself confident in the claim that can be supported by your data and the focus changes to communicating that claim as effectively as possible with a graphic. Here, your audience shifts from yourself to someone else: other scientists, customers, co-workers in a different part of your company, or casual readers. You must consider the context in which they’ll be viewing your graphic: what they know, what they expect, what they want.\nIn these notes we discuss six ways to hone the message of your data visualization. They are:\nWe will use two running examples throughout these notes: a line plot of the number of christenings in 17th century London1 and a scatter plot showing the bill sizes of penguins near Palmer Station, Antarctica2.\nCodelibrary(tidyverse)\nlibrary(stat20data)\nlibrary(patchwork)\n\narbuthnot &lt;- mutate(arbuthnot, total = boys + girls)\n\np1 &lt;- ggplot(arbuthnot, aes(x = year,\n                            y = total)) +\n    geom_line()\n\np2 &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                           y = bill_depth_mm)) +\n    geom_point()\n\np1 + p2"
  },
  {
    "objectID": "2-summarizing-data/05-communicating-with-graphics/notes.html#mapping-vs-setting",
    "href": "2-summarizing-data/05-communicating-with-graphics/notes.html#mapping-vs-setting",
    "title": "Communicating with Graphics",
    "section": "1. Mapping vs Setting",
    "text": "1. Mapping vs Setting\nOnce you have your first draft of a plot complete and you’re thinking about how to fine tune it for your audience, your eye will turn to the aesthetic attributes. Is that color right? What about the size of the points?\nConsider the first draft of the penguins plot above. It might feel a bit drab to have a large mass of points all in black, the same color as the labels and surrounding text. Let’s make the points blue instead to make them stand out a bit more.\n\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = \"blue\")) +\n    geom_point()\n\n\nClick here when you’re ready to see the result.\n\n\n\n\n\n\n\n\nThis is . . . unexpected. Why did it color the points red? Is this a bug?\nWhat we’ve stumbled into is a subtle but essential distinction in the grammar of graphics: mapping vs setting. When you put an aesthetic attribute (x, color, size, etc.) into the aes() function, you’re mapping that attribute in the plot to whatever data lives in the corresponding column in the data frame. Mapping was this process:\n\n\n\n\nThat’s not what we set out to do here. We just wanted to tweak the look of our aesthetic attributes in a way that doesn’t have anything to do with the data in our data frame. This process is called setting the attribute.\nTo set the color to blue3, we need to make just a small change to the syntax. Let’s move the color = \"blue\" argument outside of the aes() function and into the geom_() function.\n\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm)) +\n    geom_point(color = \"blue\")\n\n\n\n\n\n\n\nAh, that looks much better!\nColor isn’t the only aesthetic attribute that you can set. Let’s increase slightly the size of our points by setting the size to three times the size of the default.\n\nCodeggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm)) +\n    geom_point(color = \"blue\", size = 3)\n\n\n\n\n\n\n\nIt’s not clear that that improves the readability of the plot - there is more overlap between the points now - but the setting worked. How would it have looked if instead we had mapped the size? When you map, you need a map to a column in your data frame, so let’s map size to species.\n\nCodeggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     size = species)) +\n    geom_point(color = \"blue\")\n\n\n\n\n\n\n\nWe’ve made a mess of our plot now, but it is clear what happened. R looked inside the species column, found a categorical variable with three levels and selected a distinct size for each of those levels.\nThis is another area in which the grammar of graphic guides clear thinking when constructing a graphic. The aesthetic attributes of a plot can be determined either by variability found in a data set or by fixed values that we set. The former is present in all data visualization but it’s the latter that comes into play when fine-tuning your plot for an audience."
  },
  {
    "objectID": "2-summarizing-data/05-communicating-with-graphics/notes.html#adding-labels-for-clarity",
    "href": "2-summarizing-data/05-communicating-with-graphics/notes.html#adding-labels-for-clarity",
    "title": "Communicating with Graphics",
    "section": "2. Adding Labels for Clarity",
    "text": "2. Adding Labels for Clarity\nYou may have noticed that ggplot2 pulls the labels for the x-axis, the y-axis, and the legend directly from the names of the variables in the data frame. This results in labels like bill_length_mm, which is no problem when you’re making plots for yourself - you know what this variable means. But will an outside audience?\nYou can change the labels of your plot by adding a labs() layer.\n\nCodeggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm)) +\n    geom_point(color = \"blue\") +\n    labs(x = \"Bill Length (mm)\",\n         y = \"Bill Depth (mm)\")\n\n\n\n\n\n\n\nAxis and legend labels should be concise and often include the units of measurement. If you find them getting too wordy, know that you can clarify or expand on what is being plotted either in a figure caption or in the accompanying text.\nSpeaking of captions, those a can be added too, as well as a title.\n\nCodeggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm)) +\n    geom_point(color = \"blue\") +\n    labs(x = \"Bill Length (mm)\",\n         y = \"Bill Depth (mm)\",\n         title = \"Penguins show little association between bill length\\n and bill depth\",\n         caption = \"Source: Data collected by Dr. Kristen Gorman at Palmer Station, Antarctica\")\n\n\n\n\n\n\n\nThe title of a plot is valuable real estate for communicating the primary story of your plot. It should highlight the most important structure in the data. In the plot above, there appears to be little correspondence between bill length and bill depth. Of course, that changes when we map species to color. Let’s make that plot and title it accordingly.\n\nCodeggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = species)) +\n    geom_point() +\n    labs(x = \"Bill Length (mm)\",\n         y = \"Bill Depth (mm)\",\n         title = \"Bill length and bill depth positively correlated and\\n strongly dependent on species\",\n         caption = \"Source: Data collected by Dr. Kristen Gorman at Palmer Station, Antarctica\")\n\n\n\n\n\n\n\nThe practice of using the plot title to convey the main message of the plot is used to powerful effect by the visualization experts at the British publication, The Financial Times4. They have developed a wealth of visualizations to help readers understand what is happening with public health throughout the pandemic. The sobering graphic below uses the title to guide the viewer to the most important visual structure in the plot: the yawning vertical gap between dosage rates between high and low income countries."
  },
  {
    "objectID": "2-summarizing-data/05-communicating-with-graphics/notes.html#the-importance-of-scale",
    "href": "2-summarizing-data/05-communicating-with-graphics/notes.html#the-importance-of-scale",
    "title": "Communicating with Graphics",
    "section": "3. The Importance of Scale",
    "text": "3. The Importance of Scale\nWhen a person views your plot, their first impression will be determined by a coarse interpretation of the boldest visual statement. When using a line plot, that is usually the general trend seen when reading left to right.\nWhat is the first word that comes to mind to describe the trend in each of the four plots below?\n\nCodearbuthnot &lt;- mutate(arbuthnot, p_girls = girls / total)\n\np1 &lt;- ggplot(arbuthnot, aes(x = year,\n                            y = p_girls)) +\n    geom_line() +\n    xlim(1629, 1635) +\n    labs(x = \"\", y = \"\")\n\np2 &lt;- ggplot(arbuthnot, aes(x = year,\n                            y = p_girls)) +\n    geom_line() +\n    xlim(1703, 1707) +\n    labs(x = \"\", y = \"\")\n\np3 &lt;- ggplot(arbuthnot, aes(x = year,\n                            y = p_girls)) +\n    geom_line() +\n    labs(x = \"\", y = \"\")\n\np4 &lt;- ggplot(arbuthnot, aes(x = year,\n                            y = p_girls)) +\n    geom_line() +\n    ylim(0, 1) +\n    labs(x = \"\", y = \"\")\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\nClockwise from the upper left, you likely said something like “increasing”, “decreasing”, “variable”, and “stable”. Now take a second look. What exactly is being plotted here?\nThe labels along the axes are a hint to what you’re looking at here. These are, in fact, four plots from the exact same data: Arbuthnot christening records, with the proportion of girls christened on the x-axis. What differs is the limits of the x- and y-axes.\nMost software will automatically set the limits of your plot based on the range of the data. In the Arbuthnot data, the years range from 1629 to 1710 and the proportion of girls christened ranges from .463 to .497. The leads to the default graphic found in the lower left panel. Each of the other three plots have modified the limits of the x- or y-axis to capture different parts the data scaled in different ways. In ggplot2 this is done by adding an xlim() or ylim() layer.\nThis is the power of scaling. From one data set, you can convey four different (and incompatible!) messages by changing the scale. So which one is correct? It depends on the context and the question that drove the collection of the data. John Arbuthnot sought to understand the whether the chance of being born a girl is 1/2. That question is answered most clearly by the following plot (with the title driving home that main message).\n\nCodeggplot(arbuthnot, aes(x = year,\n                      y = p_girls)) +\n    geom_line() +\n    ylim(0, 1) +\n    labs(title = \"Proportion girls christened slightly but persistently\\n below 50%\",\n         x = \"Year\",\n         y = \"Proportion\")\n\n\n\n\n\n\n\nThe importance of scale extends beyond scatter and line plots. Barcharts are often the subject of careful scaling to convey a particular message. What do you think the goal was of the creator of the plot titled “Should Britain Leave EU?”5"
  },
  {
    "objectID": "2-summarizing-data/05-communicating-with-graphics/notes.html#overplotting",
    "href": "2-summarizing-data/05-communicating-with-graphics/notes.html#overplotting",
    "title": "Communicating with Graphics",
    "section": "4. Overplotting",
    "text": "4. Overplotting\nIntroductory statistics students filled out a survey that asked them their opinion on several topics including:\n\n\n\n\n\n\n\n\nThe result was a data frame with 707 rows (one for every respondent) and 2 columns of discrete numerical data. A natural way to visualize this data is by creating a scatter plot.\n\nCodeggplot(class_survey, aes(x = tech,\n                         y = crypto)) +\n    geom_point() +\n    labs(x = \"Technology is destructive to relationships\",\n         y = \"cryptocurrency will play a\\n dominant role in finance\",\n         title = \"No association between opinions on technology and \\n cryptocurrency\")\n\n\n\n\n\n\n\nThe eye is immediately drawn to the eerie geometric regularity of this data. Isn’t real data messier than this? What’s going on?\nA hint is in the sample size. The number of observations in the data set was over 600 and yet the number of points shown here is just a bit under 100. Where did those other observations go?\nIt turns out they are in this plot, they’re just piled on on top of the other. Since there are only 10 possible values for each question, many students ended up selecting the same values for both, leading their points to be drawn on top of one another.\nThis phenomenon is called overplotting and it is very common in large data sets. There are several strategies for dealing with it, but here we cover two of them.\nOne approach to fixing the problem of points piled on top of one another is to unpile them by adding just a little bit of random noise to their x- and y-coordinate. This technique is called jittering and can be done in ggplot2 by replacing geom_point() with geom_jitter().\n\nCodeset.seed(20)\nggplot(class_survey, aes(x = tech,\n                         y = crypto)) +\n    geom_jitter() +\n    labs(x = \"Technology is destructive to relationships\",\n         y = \"cryptocurrency will play a\\n dominant role in finance\",\n         title = \"No association between opinions on technology and \\n cryptocurrency\")\n\n\n\n\n\n\n\nAhh . . . there are those previously hidden students. Interestingly, the title on the first plot still holds true: even when we’re looking at all of the students, there doesn’t appear to be much of a pattern. There is certainly not the case in all overplotted data sets! Often overplotting will obscure a pattern that jumps out after the overplotting has been attended to.\nThe second technique is to make the points transparent by changing an aestheric attribute called the alpha value. Let’s combine tranparency with jittering to understand the effect.\n\nCodeset.seed(20)\nggplot(class_survey, aes(x = tech,\n                         y = crypto)) +\n    geom_jitter(alpha = .3) +\n    labs(x = \"Technology is destructive to relationships\",\n         y = \"cryptocurrency will play a\\n dominant role in finance\",\n         title = \"No association between opinions on technology and \\n cryptocurrency\")\n\n\n\n\n\n\n\nAlpha is a number between 0 and 1, where 1 is fully opaque and 0 is fully see-through. Here, alpha = .3, which changes all observations from black to gray. Where the points overlap, their alpha values add to create a dark blob.\nThere’s still no sign of a strong association between these variables, but at least, by taking overplotting into consideration, we’ve made that determination after incorporating all of the data."
  },
  {
    "objectID": "2-summarizing-data/05-communicating-with-graphics/notes.html#choosing-a-theme",
    "href": "2-summarizing-data/05-communicating-with-graphics/notes.html#choosing-a-theme",
    "title": "Communicating with Graphics",
    "section": "5. Choosing a Theme",
    "text": "5. Choosing a Theme\nWhat piece of software did I use to produce the following plot?\n\nCodelibrary(ggthemes)\n\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = species)) +\n    geom_point() +\n    labs(x = \"Bill Length (mm)\",\n         y = \"Bill Depth (mm)\") +\n    theme_excel()\n\n\n\n\n\n\n\nIf you said “Excel”, you are correct! Well… it is Excel in spirit. What makes this plot look like it was made in Excel are a series of small visual choices that were made: the background is a dark gray, there are black horizontal guide lines, and the plot and the legend is surrounded by a black box. Small decisions like these that effect the overall look and feel of the plot are called the theme.\nLet’s look at a few more. Do they look familiar?\n\nCodep1 &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = species)) +\n    geom_point() +\n    labs(x = \"Bill Length (mm)\",\n         y = \"Bill Depth (mm)\") +\n    theme_wsj()\np2 &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = species)) +\n    geom_point() +\n    labs(x = \"Bill Length (mm)\",\n         y = \"Bill Depth (mm)\") +\n    theme_bw()\np3 &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = species)) +\n    geom_point() +\n    labs(x = \"Bill Length (mm)\",\n         y = \"Bill Depth (mm)\") +\n    theme_economist()\n\np1 / p3 / p2\n\n\n\n\n\n\n\nThey are, from top to bottom, a theme based on The Wall Street Journal, The Economist, and one of the themes built into ggplot2 packaged called bw for “black and white” (there are no grays). The ggplot2 library has several themes to choose from and yet more live in other packages like ggthemes. To use a theme, all you need to do is add a layer called theme_NAME (e.g. for the black and white theme, use theme_bw).\nThemeing your plots is an easy way to change the look of your plot. Tinker with a few different themes and considering adding them to your labs6. But, as with all design decisions around graphics, be sure to think about your audience. You might find the Excel aesthetics ugly and dated, but will your audience? If you’re presenting your plot to a community that works with Excel plots day in and day out, that’s probably a sound choice. If you are preparing a plot for submission to a scientific journal, a more minimalist theme is more appropriate."
  },
  {
    "objectID": "2-summarizing-data/05-communicating-with-graphics/notes.html#annotations",
    "href": "2-summarizing-data/05-communicating-with-graphics/notes.html#annotations",
    "title": "Communicating with Graphics",
    "section": "6. Annotations",
    "text": "6. Annotations\nIn the same way that a title highlights the main message of a plot, you can rely upon visual cues to draw attention to certain components or provide helpful context.\nThe christening records collected by John Arbuthnot, although they seem like a very simple data set, actually capture a wealth of historical information. We can add this information to our plot by adding annotations.\n\nCodeggplot(arbuthnot, aes(x = year,\n                      y = total)) +\n    geom_line(color = \"steelblue\") +\n    labs(x = \"Year\", y = \"Total Christenings\") +\n    theme_bw() +\n    annotate(\"text\", \n             label = \"English Civil War begins\",\n             x = 1642, y = 13000) +\n    annotate(\"segment\", \n             x = 1642, xend = 1642,\n             y = 12600, yend = 10900)\n\n\n\n\n\n\n\nWere you curious about what caused that dip in the number of christenings in 17th century London? It happens to correspond to the duration of the English Civil War, when the monarchy was overthrown by a dictator named Oliver Cromwell. This very important context can be conveyed by adding a text label and a line segment through two new annotate() layers.\nWithin ggplot2, annotations are a flexible way to add the context or comparisons that help guide readers in interpreting your data. You can add text, shapes, lines, points. To learn more, consult the documentation7.\nSo if the drop after 1642 corresponds to the English Civil War, what about the spike down around 1666? What about 1703? If you’re curious, explore Wikipedia to find out and add those events as annotations to this plot."
  },
  {
    "objectID": "2-summarizing-data/05-communicating-with-graphics/notes.html#summary",
    "href": "2-summarizing-data/05-communicating-with-graphics/notes.html#summary",
    "title": "Communicating with Graphics",
    "section": "Summary",
    "text": "Summary\nThere are two main uses for data visualization. The first is as part of exploratory data analysis, when you are constructing plots for yourself to better understand the structure of the data. When you’re ready to communicate with an outside audience using graphics, more thought is needed: you must think about the difference between mapping and setting, the use of labels for clarity, the importance of scale, overplotting, themes, and annotations.\nThis is far from a complete list of what all can be done to improve your plots, but it is sufficient to produce polished graphics that effectively communicate your message."
  },
  {
    "objectID": "2-summarizing-data/05-communicating-with-graphics/notes.html#footnotes",
    "href": "2-summarizing-data/05-communicating-with-graphics/notes.html#footnotes",
    "title": "Communicating with Graphics",
    "section": "Footnotes",
    "text": "Footnotes\n\nFor data documentation, see the stat20data R package.↩︎\nFor data documentation, see the palmerpenguins R package.↩︎\nTo see the vast (and somewhat strange) palette of color names that R knows, type colors() at the console.↩︎\nVisualization drawn from the excellent collection of graphics at the Financial Times Covid Tracker https://ig.ft.com/coronavirus-vaccine-tracker/.↩︎\nGraphics from the keynote of John Burn-Murdoch at rstudio::conf() 2021.↩︎\nExplore the themes available within ggplot2 by reading the documentation https://ggplot2.tidyverse.org/reference/ggtheme.html. For the additional themes held in the ggthemes package, read this: https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/.↩︎\nDocumentation for annotation layers in ggplot2: https://ggplot2.tidyverse.org/reference/annotate.html.↩︎"
  },
  {
    "objectID": "2-summarizing-data/05-communicating-with-graphics/ps.html",
    "href": "2-summarizing-data/05-communicating-with-graphics/ps.html",
    "title": "Communicating with Graphics",
    "section": "",
    "text": "Question 1\nUse the gapminder data set in the gapminder library to recreate a version of the Hans Rosling’s famous data visualization (a single plot instead of a movie). You can revisit the slides from Grammar of Graphics to reference it.\nSome guidelines:\n\nPlot the data for just a single year. You can see at a glance which years are available by running count(gapminder, year).\nPut gdpPercap on the x-axis and lifeExp on the y-axis.\nTo distinguish the continents, you can use either shape or color. Which ever one you do not use, set its value to something other than the default.\nAlter the labels so that they’re more descriptive.\nChange to a theme of your choosing.\nAdd an annotation that draws attention to a particular feature of the data.\n\nTurn to the notes for today for guidance on how to add each of these elements.\n\n\nQuestion 2\nRecreate one of the plots that you created for Lab 2.2, but incorporate at least 3 of the 6 elements of Communicating with Graphics to polish your plot into one that tells a particular story."
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html",
    "title": "Summarizing Numerical Associations",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set 5\nBreak\nLab 2.2 (extended): Computing on The Data"
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html#agenda",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html#agenda",
    "title": "Summarizing Numerical Associations",
    "section": "",
    "text": "Announcements\nConcept Questions\nProblem Set 5\nBreak\nLab 2.2 (extended): Computing on The Data"
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html#announcements",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html#announcements",
    "title": "Summarizing Numerical Associations",
    "section": "Announcements",
    "text": "Announcements\n\nRQ: Multiple Linear Regression due Wed/Thu night at 11:59pm\n\n. . .\n\nLab 2.2 extended and due Tuesday, February 13 at 9am\n\n. . .\n\nProblem Set 5 due Tuesday, February 13 at 9am, along with Problem Set 4\n\n. . .\n\nPlease read updated Lab Submission Guidelines on Ed! (titling and labeling plots)\n\n. . .\n\nQuiz 1 next Monday in-class. Covers all lectures from the beginning of the year until Thursday/Friday!"
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html#estimate-the-correlation",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html#estimate-the-correlation",
    "title": "Summarizing Numerical Associations",
    "section": "Estimate the correlation",
    "text": "Estimate the correlation\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nWhat is the (Pearson) correlation coefficient between these two variables?\n\n\nCodelibrary(tidyverse)\nn_samples &lt;- 100\nset.seed(1)\nx &lt;- runif(n = n_samples, min = -1, max = 1) \ny &lt;- -x + rnorm(n = n_samples, mean = 0, sd = .1)\n\ndata.frame(x = x, \n           y = y) %&gt;% \n  ggplot(aes(x = x, y = y)) +  \n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis plot has a strong negative linear association, so the correlation coefficient will be around -.8 or -.9.\nThe goal of this exercise is threefold: - Start calibrating the ability to associate the correlation with the structure in a scatter plot. - Make it clear that this is still just an informal eye-balling procedure (therefore multiple answers can be reasonable, but some are un-reasonable). - Set some of them up for a challenge on the next one; some will conflate correlation with association."
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html#section",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html#section",
    "title": "Summarizing Numerical Associations",
    "section": "",
    "text": "Code# simulate data -----------------------------------------------------\nset.seed(9274)\n\nx1 &lt;- seq(0, 6, by = 0.05)\n\ny_u &lt;- (x1-3)^2 - 4 + rnorm(length(x1), mean = 0, sd = 1)\ny_lin_pos_strong &lt;- 3*x1 + 10 + rnorm(length(x1), mean = 0, sd = 2)\ny_lin_pos_weak &lt;- 3*x1 + 10 + rnorm(length(x1), mean = 0, sd = 20)\n\nx2 &lt;- seq(-8, -2, by = 0.05)\n\ny_n &lt;- -1 * (x2 + 5)^2 + 1 + rnorm(length(x2), mean = 0, sd = 2)\ny_lin_neg_strong &lt;- -5 * x2 + 3 + rnorm(length(x2), mean = 0, sd = 2)\ny_none &lt;- rnorm(length(x2), mean = 0, sd = 1)\n\ndf &lt;- data.frame(x = c(rep(x1, 3), rep(x2, 3)),\n                 y = c(y_u, y_lin_pos_strong, y_lin_pos_weak,\n                       y_n, y_lin_neg_strong, y_none),\n                 plot_num = rep(LETTERS[1:6], each = length(x1)))\n\nlibrary(tidyverse)\npa &lt;- df %&gt;%\n    filter(plot_num == \"A\") %&gt;%\n    ggplot(aes(x = x, \n               y = y)) +\n    geom_point() +\n    facet_wrap(vars(plot_num), scales = \"free\") +\n    theme_bw(base_size = 14) +\n    theme(axis.text.x=element_blank(),\n          axis.ticks.x=element_blank(),\n          axis.text.y=element_blank(),\n          axis.ticks.y=element_blank()) +\n    labs(x = \"\",\n         y = \"\")\n\npb &lt;- df %&gt;%\n    filter(plot_num == \"B\") %&gt;%\n    ggplot(aes(x = x, \n               y = y)) +\n    geom_point() +\n    facet_wrap(vars(plot_num), scales = \"free\") +\n    theme_bw(base_size = 14) +\n    theme(axis.text.x=element_blank(),\n          axis.ticks.x=element_blank(),\n          axis.text.y=element_blank(),\n          axis.ticks.y=element_blank()) +\n    labs(x = \"\",\n         y = \"\")\n\npc &lt;- df %&gt;%\n    filter(plot_num == \"C\") %&gt;%\n    ggplot(aes(x = x, \n               y = y)) +\n    geom_point() +\n    facet_wrap(vars(plot_num), scales = \"free\") +\n    theme_bw(base_size = 14) +\n    theme(axis.text.x=element_blank(),\n          axis.ticks.x=element_blank(),\n          axis.text.y=element_blank(),\n          axis.ticks.y=element_blank()) +\n    labs(x = \"\",\n         y = \"\")\n\npd &lt;- df %&gt;%\n    filter(plot_num == \"D\") %&gt;%\n    ggplot(aes(x = x, \n               y = y)) +\n    geom_point() +\n    facet_wrap(vars(plot_num), scales = \"free\") +\n    theme_bw(base_size = 14) +\n    theme(axis.text.x=element_blank(),\n          axis.ticks.x=element_blank(),\n          axis.text.y=element_blank(),\n          axis.ticks.y=element_blank()) +\n    labs(x = \"\",\n         y = \"\")\n\npe &lt;- df %&gt;%\n    filter(plot_num == \"E\") %&gt;%\n    ggplot(aes(x = x, \n               y = y)) +\n    geom_point() +\n    facet_wrap(vars(plot_num), scales = \"free\") +\n    theme_bw(base_size = 14) +\n    theme(axis.text.x=element_blank(),\n          axis.ticks.x=element_blank(),\n          axis.text.y=element_blank(),\n          axis.ticks.y=element_blank()) +\n    labs(x = \"\",\n         y = \"\")\n\npf &lt;- df %&gt;%\n    filter(plot_num == \"F\") %&gt;%\n    ggplot(aes(x = x, \n               y = y)) +\n    geom_point() +\n    facet_wrap(vars(plot_num), scales = \"free\") +\n    theme_bw(base_size = 14) +\n    ylim(-9, 9) +\n    theme(axis.text.x=element_blank(),\n          axis.ticks.x=element_blank(),\n          axis.text.y=element_blank(),\n          axis.ticks.y=element_blank()) +\n    labs(x = \"\",\n         y = \"\")\n\nlibrary(patchwork)\n(pa + pb + pc) / (pd + pe + pf)\n\n\n\n\n\n\n\n\nWhich four plots exhibit the strongest association?\n\n\nCodecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nStudent will likely identify A, B, E, and F. F only appears to be highly associated because of the scaling of the y-axis. This is a good opportunity to review definition of association. The following slides demonstrate the effects of that scaling."
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html#correlation-and-scale",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html#correlation-and-scale",
    "title": "Summarizing Numerical Associations",
    "section": "Correlation and Scale",
    "text": "Correlation and Scale\n\n\n\nCodedf_f"
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html#pearson-correlation-and-scale",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html#pearson-correlation-and-scale",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\n\n\nCodedf_f\n\n        x            y\n1   -8.00 -1.307007369\n2   -7.95 -0.466886973\n3   -7.90 -1.317638373\n4   -7.85  1.008059015\n5   -7.80  1.153714017\n6   -7.75  0.136000314\n7   -7.70  1.081268517\n8   -7.65  0.027234440\n9   -7.60 -0.755489564\n10  -7.55  0.643581028\n11  -7.50 -2.221816425\n12  -7.45 -0.352711462\n13  -7.40  0.788325702\n14  -7.35  0.348209977\n15  -7.30  0.888132692\n16  -7.25  1.494640228\n17  -7.20  0.539665177\n18  -7.15  1.534245783\n19  -7.10 -0.636288267\n20  -7.05  1.400750746\n21  -7.00  0.857266821\n22  -6.95  0.689240640\n23  -6.90 -0.578689332\n24  -6.85 -0.247484190\n25  -6.80 -1.616711187\n26  -6.75 -0.211921880\n27  -6.70 -1.389650022\n28  -6.65 -0.667030427\n29  -6.60  0.265923179\n30  -6.55  0.368512913\n31  -6.50  0.159370185\n32  -6.45 -0.803029578\n33  -6.40 -1.181853575\n34  -6.35  0.799141786\n35  -6.30 -0.119741600\n36  -6.25  1.055099362\n37  -6.20  0.260610303\n38  -6.15  1.215058556\n39  -6.10 -2.093776227\n40  -6.05 -0.340475843\n41  -6.00  1.098181964\n42  -5.95 -1.022920292\n43  -5.90  0.997308309\n44  -5.85  0.442282185\n45  -5.80  2.024841893\n46  -5.75 -0.206236931\n47  -5.70 -0.366921396\n48  -5.65 -0.540982671\n49  -5.60  0.326545855\n50  -5.55  1.185875818\n51  -5.50  0.163270582\n52  -5.45 -0.252746010\n53  -5.40 -0.538783647\n54  -5.35  0.239830058\n55  -5.30  0.227238979\n56  -5.25  0.146973095\n57  -5.20  1.206565164\n58  -5.15  0.274237725\n59  -5.10  0.291223416\n60  -5.05 -0.048435420\n61  -5.00 -0.160503335\n62  -4.95  0.982662625\n63  -4.90 -0.009812706\n64  -4.85 -1.236449694\n65  -4.80 -0.220086594\n66  -4.75  0.916450245\n67  -4.70  0.312158589\n68  -4.65 -0.271222289\n69  -4.60 -0.362834150\n70  -4.55  0.690127669\n71  -4.50  0.954579372\n72  -4.45 -1.002334741\n73  -4.40 -0.827510811\n74  -4.35 -0.481050964\n75  -4.30  0.213284850\n76  -4.25  0.694018655\n77  -4.20  0.318705891\n78  -4.15  1.359291786\n79  -4.10  0.299499214\n80  -4.05 -0.136853535\n81  -4.00 -0.907524278\n82  -3.95  0.580039124\n83  -3.90 -0.451761432\n84  -3.85 -0.637651429\n85  -3.80 -0.377287558\n86  -3.75 -0.108975033\n87  -3.70  0.266174878\n88  -3.65  1.950537958\n89  -3.60  2.481797153\n90  -3.55  0.024020307\n91  -3.50 -0.271974465\n92  -3.45 -0.324420253\n93  -3.40  0.914990966\n94  -3.35 -0.177070403\n95  -3.30  1.668125591\n96  -3.25 -0.196626395\n97  -3.20  0.909735944\n98  -3.15  0.379802619\n99  -3.10 -0.717674077\n100 -3.05 -0.137978296\n101 -3.00 -0.032596143\n102 -2.95 -1.151078285\n103 -2.90 -0.035147120\n104 -2.85  1.289653594\n105 -2.80  1.301664628\n106 -2.75 -1.314453018\n107 -2.70  0.279983464\n108 -2.65 -0.134496134\n109 -2.60 -0.993968666\n110 -2.55 -1.125493956\n111 -2.50 -0.585075814\n112 -2.45 -0.360440229\n113 -2.40 -0.128946258\n114 -2.35 -1.763949074\n115 -2.30  0.249921212\n116 -2.25  1.356315758\n117 -2.20 -0.221241869\n118 -2.15 -0.255945298\n119 -2.10  0.764032808\n120 -2.05 -0.429023148\n121 -2.00 -0.506416626"
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html#pearson-correlation-and-scale-1",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html#pearson-correlation-and-scale-1",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\n\n\nCodedf_f %&gt;%\n    ggplot(aes(x = x,\n               y = y)) +\n    geom_point() +\n    ylim(-9, 9) +\n    theme_bw()"
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html#pearson-correlation-and-scale-2",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html#pearson-correlation-and-scale-2",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\n\n\nCodedf_f %&gt;%\n    ggplot(aes(x = x,\n               y = y)) +\n    geom_point() +\n    ylim(-9, 9) +\n    theme_bw()"
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html#pearson-correlation-and-scale-3",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html#pearson-correlation-and-scale-3",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\n\n\nCodedf_f %&gt;%\n    ggplot(aes(x = x,\n               y = y)) +\n    geom_point() +\n    theme_bw()"
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html#pearson-correlation-and-scale-4",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html#pearson-correlation-and-scale-4",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\n\n\nCodedf_f %&gt;%\n    ggplot(aes(x = x,\n               y = y)) +\n    geom_point() +\n    theme_bw()"
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html#pearson-correlation-and-scale-5",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html#pearson-correlation-and-scale-5",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\nCodedf_f %&gt;%\n    summarize(pearson = cor(x, y))"
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html#pearson-correlation-and-scale-6",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html#pearson-correlation-and-scale-6",
    "title": "Summarizing Numerical Associations",
    "section": "Pearson Correlation and Scale",
    "text": "Pearson Correlation and Scale\n\nCodedf_f %&gt;%\n    summarize(pearson = cor(x, y))\n\n      pearson\n1 -0.03106456"
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html#describing-associations",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html#describing-associations",
    "title": "Summarizing Numerical Associations",
    "section": "Describing Associations",
    "text": "Describing Associations\nWhen considering the structure in a scatter plot, pay attention to:\n\nThe strength of the association\n\nHow much does the conditional distribution of y change when you move along the x?\n\n\n\n. . .\n\nThe shape of the association\n\nLinear? Quadratic? Cubic or beyond?\n\n\n\n. . .\n\nThe direction of the (linear) association\n\nPositive or negative?"
  },
  {
    "objectID": "2-summarizing-data/06-summarizing-associations/slides.html#section-1",
    "href": "2-summarizing-data/06-summarizing-associations/slides.html#section-1",
    "title": "Summarizing Numerical Associations",
    "section": "",
    "text": "Codecountdown::countdown(1, top = 0)\n\n\n\n−&plus;\n\n01:00\n\n\n\n\nCodelibrary(ggrepel)\np_nolabs +   \n  geom_text_repel(aes(label = State), size = 3)\n\n\n\n\n\n\n\n. . .\n\nWhich state has the highest graduation rate given its poverty rate?\n\n\nThis assesses whether or not students have an informal sense of what a residual is."
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/notes.html",
    "href": "2-summarizing-data/01-summarizing-categorical-data/notes.html",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "In spring of 2022, the New York Times ran the following story1.\n“Consumer Prices” refers to the Consumer Price Index2, a weighted average of the prices of thousands of everyday consumer goods: sports equipment, soft drinks, sneakers, internet service, etc. An increase in that index is thought to correspond to rising inflation.\nLook carefully at the line plot. Which of the following four claims does it support?\nIn truth, this plot could be consistent with all these claims. They are, in turn, a summary, a prediction, a generalization, and a causal claim. This newspaper headline falls squarely in the first category, a summary, which seeks only to describe the data set that is on hand.\nAlthough 8.3% seems like a simple enough number, it is actually summarizing a vast data set of thousands of prices. The process of describing a data set invariably involves summarizing it, either with numerical summaries like 8.3% or with graphical summaries like the line plot show above.\nIn this unit, we will learn to critique and construct descriptive claims made with data. Although they sound elementary, descriptive claims are the most common form of claim made using data. They have the power to move, if not mountains, at least markets."
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/notes.html#welcome-to-unit-i-summarizing-data",
    "href": "2-summarizing-data/01-summarizing-categorical-data/notes.html#welcome-to-unit-i-summarizing-data",
    "title": "Summarizing Categorical Data",
    "section": "Welcome to Unit I: Summarizing Data",
    "text": "Welcome to Unit I: Summarizing Data"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/notes.html#summarizing-categorical-data",
    "href": "2-summarizing-data/01-summarizing-categorical-data/notes.html#summarizing-categorical-data",
    "title": "Summarizing Categorical Data",
    "section": "Summarizing Categorical Data",
    "text": "Summarizing Categorical Data\nThe tools for calculating numerical summaries and graphical summaries can be cleanly divided between tools developed for categorical data and tools for numerical data. We’ll discuss each in turn, starting with categorical data.\nWhen Dr. Gorman collected data near Palmer Station, Antarctica, she recorded a total of eight variables on 333 penguins, presented here in a data frame.\n\n\n# A tibble: 333 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ 323 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThe first two of these you’ll recognize as nominal categorical variables. The species of each penguin can take one of three levels: Adelie, Chinstrap, or Gentoo; and the island on which they were found can also take three levels: Biscoe, Dream, or Torgersen.\nContingency tables\nIf I asked you to look at this data frame and describe what these two variables show, that’s a surprisingly difficult task! The raw data frame has simply too much information to process at a glance. To make it easier, we need to consolidate all of that information into a few numerical summaries. Since these variables don’t take numbers as values, we can’t take an average or a median. What we can do, though, is simply count up the number of penguins that appeared in every combination of levels and lay them out in a table like this:\n\n\n\n\n\n\nBiscoe\nDream\nTorgersen\n\n\n\nAdelie\n44\n55\n47\n\n\nChinstrap\n0\n68\n0\n\n\nGentoo\n119\n0\n0\n\n\n\n\n\n\nAfter a few moments of looking at this table, a few features emerge:\n\nThe Chinstrap penguins were only found on Dream island.\nAdelie penguins are the most common.\nThe most prevalent penguin type in this data set was a Gentoo from Biscoe Island.\n\nThis method of displaying data is called a contingency table.\n\nContingency table\n\nA table that shows the counts or frequencies of observations that occur in every combination of levels of two categorical variables. Used to display the relationship between variables.\n\n\nWe can also opt to present these counts in graphical form by constructing a bar chart. There are two common methods for laying out counts across two variables in a bar chart.\n\n\n\n\n\n\n\n\nStacked bar chart\n\n\n\n\n\nDodged bar chart\n\n\n\n\nThe stacked bar chart puts one of the variables along the x-axis (the species) and fills up the y-axis according to the counts in each level of the other variable (the island). The side-by-side bar chart is similar, but unstacks the “Adelie” bar to put the three islands besides one another.\nSo with two similar to charts to choose from, which one should you pick? The stacked bar chart succeeds in highlighting the total number of Adelie penguins: 146 (also the sum of the top row of the contingency table). The side-by-side version makes that total harder to see, but it is easier to see at a glance the relative sizes of which islands each of those Adelie penguins came from.\n\n\n\n\n\nFigure 3: A schematic showing the link between a data frame, a contingency table of counts, and a stacked bar chart for a subset of 16 of the 333 penguins.\n\n\nFrom Counts to Proportions\nWhen you count the number of observations in each level of a categorical variable, you’re encoding the overall magnitude of each. But often what is more important is the relative magnitude of each. We can emphasize this by converting counts into proportions.\n\n\n\n\n\n\n\n\n\nBiscoe\nDream\nTorgersen\n\n\n\nAdelie\n44\n55\n47\n\n\nChinstrap\n0\n68\n0\n\n\nGentoo\n119\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiscoe\nDream\nTorgersen\n\n\n\n\nAdelie\n0.132\n0.165\n0.142\n0.439\n\n\nChinstrap\n0.000\n0.204\n0.000\n0.204\n\n\nGentoo\n0.357\n0.000\n0.000\n0.357\n\n\n\n0.489\n0.369\n0.142\n1.000\n\n\n\n\n\n\n\n\n\n\nThe second table contains two different types of proportions that can be found in the middle and margins of the table, respectively.\n\nJoint Proportion\n\nThe proportion of observations of multiple variables that appear in a combination of levels of those variables. E.g. 44 / 333 = .132, the proportion of all 333 penguins that were Adelie and from Biscoe.\n\nMarginal Proportion\n\nThe proportion of observations in one variable that appear in a single level of that variable. E.g. (44 + 119) / 333 = .489, the proportion of all penguins that were from Biscoe.\n\n\nFrom these proportions we can derive a third type of proportion.\n\nConditional Proportion\n\nThe proportion of observations in one level of one variable that appear in a level of a second variable. E.g. .132 / .489 = .269, the proportion of penguins from Biscoe that were Adelie.\n\n\nThese distinctions are important because often a subtle change in language can result in a dramatically different number. Revisit the observations made about the original table of counts:\n\nThe Chinstrap penguins were only found on Dream Island.\nAdelie penguins were the most common.\nThe most prevalent penguin type in this data set was a Gentoo from Biscoe Island.\n\nThe first observation refers to a conditional proportion: of the 68 total Chinstrap penguins, 68 of them were on Dream (a proportion of 1). The second refers to a marginal proportion: .439 of the penguins were Adelie versus .204 Chinstrap and .357 Gentoo. The third is a joint proportion: .357, or 124 out of all 333 penguins were Gentoos living on Biscoe.\nWith these distinctions in hand, we can construct a useful variant of the bar chart: the normalized stacked bar chart.\n\n\n\n\n\n\n\n\nInstead of plotting raw counts, a normalized bar chart plots proportions. But which proportions?\nIf you look back and forth between this bar chart and the table of proportions, you’ll eventually decide that this bar chart is showing conditional proportions. The height of the blue bar in the lower left corner indicates the proportion of all Adelie penguins that were from Torgersen: 47 / 146 = .322. Since species is in the denominator of this proportion, we say that we’ve “conditioned on” species.\nWe can take the same data and get a very different story if instead we condition on island.\n\n\n\n\n\n\n\n\nNow, the height of the blue bar in the lower left of the plot shows the proportion of penguins from Biscoe that were Gentoo: 119 / 163 = .730.\nOne last question before we move on: of these two charts, which one more clearly shows the species distribution on Dream Island? Dream shows up in both charts, but only one gives you a good answer to this question.\nThe answer to this question comes from the second chart. Here the species breakdown on Dream is clearly shown in the middle bar: a bit more than half are Chinstrap, the rest are Adelie. Trying to pry this information out of the first chart is actually impossible! When we conditioned on species, we lost the relative numbers of Chinstraps and Adelies - where they found in equal numbers or were there 100 Chinstraps for every Adelie? That information can’t be back-engineered from the first chart, and without it, we can’t reconstruct the distribution as seen in the middle bar of the second chart.\nThe lesson from this is powerful: if you wish to make a claim rooted in a conditional proportion and are using a normalized bar chart, it is essentially to think carefully about which conditional proportion your chart is displaying.\nIdentifying an association\nIs there an association between species and which island they are found on? With a clearer sense of proportions under our belt, we can define this more precisely.\n\nAssociation\n\nThere is an association between two categorical variables if the conditional proportions vary as you move from one one level of the conditioning variable to the next.\n\n\nThis is best illustrated by showing the previous plot of the real data right next to an example of what a penguins data set would look like that exhibits no association between species and island.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn plot on the right, as you gaze from left to right, looking across first Biscoe, then Dream, then Torgersen Islands, you see that the distribution of species is unchanged. There are always the most Gentoos, then Chintraps, then Adelies, regardless of the island. This indicates that there is no association between the variables in this data set.\nThis stands in stark contrast to the plot of the real data on the left. On Biscoe Island, Gentoo penguins dominate, with a smaller proportion of Adelies. Chinstraps are nowhere to be found. On Dream Island, its the Gentoos that are absent and we find roughly similar proportions of Chinstraps and Adelies. On Torgersen we observed only Adelies. This data set exhibits a strong association between species and island."
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/notes.html#the-ideas-in-code",
    "href": "2-summarizing-data/01-summarizing-categorical-data/notes.html#the-ideas-in-code",
    "title": "Summarizing Categorical Data",
    "section": "The Ideas in Code",
    "text": "The Ideas in Code\nHelp and Arguments\nEvery function in R has a built-in help file that tells you about how it works. It can be accessed using ?.\n\n?mean\n\nThis will pop up the help file in a tab next to your Files tab in the lower right hand corner of RStudio. In addition to describing what the function does, the help file lists out the function’s arguments. Arguments are the separate pieces of input that you can supply to a function and they can be named or unnamed.\nIn Taxonomy of Data, we created a vector called my_fav_numbers and then calculated its mean using the mean() function.\n\nmy_fav_numbers &lt;- c(9, 11, 19, 28)\nmean(my_fav_numbers)\n\n[1] 16.75\n\n\nIn the second command that we entered above, we used a single unnamed argument, my_fav_numbers. We could have alternatively written this with a named argument:\n\nmean(x = my_fav_numbers)\n\n[1] 16.75\n\n\nAs the help file suggests, x is the R object (here a vector of numbers) that you want to take the mean of. You can always pass objects to a function as named arguments, or if you want to be more concise, you can pass it unnamed and R will rely on the order to figure things out.\n\n\n\n\n\n\n\nFigure 4: Help file for mean().\n\n\nTo test how this actually works, let’s add a second unnamed argument to our function. From reading the help file, you learn that you can supply it a trim argument to trim off some percent of the highest and lowest values before computing the mean. The default value of this argument is 0, but we can change this. Let’s trim off 25% from the lower end and 25% from the upper end.\n\nmean(my_fav_numbers, .25)\n\n[1] 15\n\n\nIt worked! We trim off the 9 and the 28, then take \\((11 + 19) / 2 = 15\\). We can also write the command using named arguments. The code will be a bit more verbose but the answer will be the same.\n\nmean(x = my_fav_numbers, trim = .25)\n\n[1] 15\n\n\nWhat happens if we use unnamed arguments but change the order? Let’s find out.\n\nmean(.25, my_fav_numbers)\n\nError in mean.default(0.25, my_fav_numbers): 'trim' must be numeric of length one\n\n\nSince there are no names, R looks at the second argument and expects it to be the a proportion between 0 and .5 that it will use to trim. You have passed it a vector of three integers instead, so it’s justified in complaining.\nLibraries\n\n\nThe tidyverse library, which contains many useful packages for data science\n\nIn Taxonomy of Data, we learned about a few different functions that can be used on vectors, such as mean(). We also learned about a function called data.frame(), which allowed us to bring vectors together as part of a new structure called a data frame, with each of the vectors used as columns.\nWhile the base functionality provided by R is powerful, developers often seek to find more efficient ways to complete tasks. In doing so, they push the power of R forward. R has a vast ecosystem of libraries that add new functions. Any installed library can be loaded with the library() function. Here, we will load the tidyverse library, one of the core external libraries that we will be using this semester.\n\nlibrary(tidyverse)\n\nTo use these functions within this library (or any other library), you will need to run the above line of code each time you start an RStudio session!\nThe tidyverse library has many different packages which contain smaller pieces of functionality. Today, our focus is to summarize categorical data, and one avenue we have explored already is a visual summary; a bar chart. Let’s explore how the plots shown earlier in this set of notes were made. We will use ggplot2, tidyverse’s resident visualization package.\nFirst, we should load the penguins data into our environment. The penguins data is actually located in a special library made just for this course called stat20data. This library hosts the datasets which will be the subjects of your labs. Therefore, we need to load this library first.\n\nlibrary(stat20data)\n\nThen, we can load in the penguins data by using the aptly named data function.\n\ndata(penguins)\n\nOnce you do this, you will see the penguins dataset, in all of its glory, appear in the environment pane at the top right of your RStudio session (you may need to click in the area once or twice).\n\n\n\n\nYou can click the blue dropdown arrow to see each variable in the dataset, or for a more traditional view, you can click the white spreadsheet icon to the right:\n\n\n\n\nNow, we’re ready to write some ggplot2 code and make our first visualization of the year! We will create the exact stacked, normalized bar chart you saw earlier in the notes.\nA first visualization with ggplot2\n\nThe main function within the ggplot2 package is, well, ggplot().\n\nggplot(data = penguins)\n\n\n\n\n\n\n\nThe ggplot() function takes as its first argument a data frame (the data argument). By itself, there’s not much to look at; it just creates a blank canvas.\nLet’s now indicate how we want to map our variables to each area of the plot. For now, let’s just focus on the species of penguin; we’ll handle the island later.\nWe had species of penguin on the horizontal (x) axis. This piece of information (which is known as an aesthetic attribute of the plot), goes into a second argument called mapping and within a function called aes().\n\nggplot(data = penguins, \n       mapping = aes(x = species))\n\n\n\n\n\n\n\nNow we’re getting somewhere. We can see the x axis has been set up with labels for the species of penguins.\nAll that is left is to actually put the observations on the plot. We do this by declaring what geometry we want and adding this information to our existing code. We can add this information via a new layer, which can be added on by using the + syntax and taking a newline.\n\nggplot(data = penguins, \n       mapping = aes(x = species)) +\n  geom_bar()\n\n\n\n\n\n\n\nThe layer we added was a geometry layer corresponding to the bar chart, called geom_bar(). In tomorrow’s notes, you will similarly named geometry layers relating to visualizations for numerical data.\nNote that the bars are not colored like before, but are just gray. This is because we are missing the island variable! Island was represented by coloring in (filling) the bars for us according to the conditional proportions of penguins within a specific species belonging to a specific island. We can include this by adding to our aes() function in the original layer.\n\nggplot(data = penguins, \n       mapping = aes(x = species, \n                     fill = island)) +\n  geom_bar()\n\n\n\n\n\n\n\nAlmost there! Note that what we have right now is just a stacked bar chart. There’s one more fix that we need to apply. Note that geom_bar() can be thought of as a function as well. One of its arguments, position, will help us solve this issue3. position defaults to a stacked bar chart, but we can fix this by supplying it with \"fill\"!\n\nggplot(data = penguins, \n       mapping = aes(x = species, \n                     fill = island)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nA side-by-side (dodged) by chart can be produced by replacing \"fill\" with \"dodge\". To recover the original, stacked bar chart, use \"stack\"!\nThere we have it! Looking forward:\n\nIn our very next lecture, you’ll learn how to use ggplot2 to make visualizations of numerical variables.\nWe will also learn more about the machinery behind ggplot2 through a dedicated lecture in a couple of weeks."
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/notes.html#summary",
    "href": "2-summarizing-data/01-summarizing-categorical-data/notes.html#summary",
    "title": "Summarizing Categorical Data",
    "section": "Summary",
    "text": "Summary\nA wise statistician once said, “In statistics, most of what we do is add things up. Sometimes we divide. The challenging part is deciding what to add and divide.” This captures the deceptive simplicity of summarizing categorical data. Categorical summaries involve counts of categories or proportions. Those proportions can be joint proportions, marginal proportions, or conditional proportions. Those counts and proportions are commonly displayed in contingency tables or in bar charts. Subtle choices of which proportion to present results in the telling of dramatically different stories."
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/notes.html#footnotes",
    "href": "2-summarizing-data/01-summarizing-categorical-data/notes.html#footnotes",
    "title": "Summarizing Categorical Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nSmialek, Jeanna (2022, May 11). Consumer Prices are Still Climbing Rapidly. The New York Times. https://www.nytimes.com/2022/05/11/business/economy/april-2022-cpi.html↩︎\nTo learn more, check out the Wikipedia page on the CPI in the US and the exhaustive description of how the data is collected at the US Bureau of Labor Statistics↩︎\nThis argument is generally not important for the other plots we create in this course, which is why in the initial piece of code featuring geom_bar(), it was not specified at all.↩︎"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/reading-questions.html",
    "href": "2-summarizing-data/01-summarizing-categorical-data/reading-questions.html",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(infer)\ndata(gss)\n\nggplot(gss, aes(x = partyid, \n                fill = college)) +\n    geom_bar(position = \"dodge\")\n\n\n\n\n\n\nCodeggplot(gss, aes(x = partyid, \n                fill = college)) +\n    geom_bar(position = \"fill\") +\n    labs(y = \"proportion\")"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/reading-questions.html#question-2.1",
    "href": "2-summarizing-data/01-summarizing-categorical-data/reading-questions.html#question-2.1",
    "title": "Summarizing Categorical Data",
    "section": "Question 2.1",
    "text": "Question 2.1\nWhat was the most frequently cited party that respondents identified with?\n( ) dem (X) ind ( ) rep ( ) other ( ) no degree ( ) degree"
  },
  {
    "objectID": "2-summarizing-data/01-summarizing-categorical-data/reading-questions.html#question-2.2",
    "href": "2-summarizing-data/01-summarizing-categorical-data/reading-questions.html#question-2.2",
    "title": "Summarizing Categorical Data",
    "section": "Question 2.2",
    "text": "Question 2.2\nTrue or False: most of the survey respondents had a college degree.\n( ) True (X) False"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/notes.html",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/notes.html",
    "title": "A Grammar of Graphics",
    "section": "",
    "text": "In the last set of notes, we saw that there is a side to the discipline of Statistics that looks like engineering. Summary statistics - medians, standard deviations, etc. - are carefully crafted tools that capture different characteristics of a data set for use in very specific situations. There is another practice in statistics that looks more like a science; that is, a field that seeks to take many different phenomena and explain them using a systematic overarching theory. That practice is data visualization.\nAt this point in the course, you’ve seen several several examples of data visualizations.\nThe diversity of shapes and structures used in these plots suggest that each one is a thing-unto-itself, specially devised to provide one particular style of visualization. But what elements do they share?\nFocus on the nature of the data being used. Exactly half of the plots above are illustrating the distribution of a single variable; the other half illustrate the relation between two variables. Can you tell which is which?\nConsider the manner in which variability in the data is being conveyed used different visual cues. How many of the plots above utilize an x-axis? A y-axis? Color?\nFinally, how are the observations finding their way onto the plot? Three of the plots above share the same data variable, utilize the same visual cues, and differ only in the shape used to encode the observations.\nBy asking these questions, we begin to find recurring structures in a wide range of plot types. These recurring structures have been compiled into a widely-used framework called the Grammar of Graphics."
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/notes.html#the-grammar-of-graphics",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/notes.html#the-grammar-of-graphics",
    "title": "A Grammar of Graphics",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nIn 1999, a statistician named Leland Wilkinson published the first edition of what has been the most influential work in data visualization, The Grammar of Graphics1. The title is fitting. In the same way that a grammar defines the regular structures and composition of a language, his book outlines a framework to structure statistical graphics.\n\nNearly every current software tool used to build plots has been informed by this book2. Its influence can be found in Tableau, Plotly, and the Python libraries bokeh, altair, seaborn, and plotnine. The most complete implementation of the grammar is found in an R package called ggplot2 by Hadley Wickham3.\nIn Wickham’s adaptation of the grammar of graphics, a plot can be decomposed into three primary elements:\n\nthe data,\nthe aesthetic mapping of the variables in the data to visual cues, and\nthe geometry used to encode the observations on the plot.\n\nLet’s go through each of these components one-by-one to understand the role that they play in a plot like this, which we’ll refer to as the “penguin plot”.\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = species)) +\n    geom_point()\n\n\n\n\n\n\n\nThe above plot is an example new type of plot which involves at least two numerical variables: a scatter plot. The points may also be colored by a third variable, generally a categorical one.\nData\nWhat variables are needed to construct the penguin plot above?\nWe see bill_length_mm and bill_depth_mm; those are labeled clearly on the x and y axes. We must also know the species of each of these penguins in order to know which color to label each point. In other words, there are three columns of a data frame that we need to have on hand.\n\npenguins |&gt;\n  select(bill_length_mm,\n       bill_depth_mm, \n       species)\n\n# A tibble: 333 × 3\n   bill_length_mm bill_depth_mm species\n            &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;  \n 1           39.1          18.7 Adelie \n 2           39.5          17.4 Adelie \n 3           40.3          18   Adelie \n 4           36.7          19.3 Adelie \n 5           39.3          20.6 Adelie \n 6           38.9          17.8 Adelie \n 7           39.2          19.6 Adelie \n 8           41.1          17.6 Adelie \n 9           38.6          21.2 Adelie \n10           34.6          21.1 Adelie \n# ℹ 323 more rows\n\n\nThrough working on your assignments, you may have also seen the following plot, which is a variant of the “penguin plot”:\n\n\n\n\n\n\n\n\nNotice that the Adelie penguins are missing. Based on your last set of notes, they must have been filtered out! What this means is that to make this plot, we first had to modify the dataset and then use the modified version in our plot. We can do these steps in one piece of code by using the pipe.\n\npenguins |&gt;\n  filter(species != \"Adelie\") |&gt;\n  ggplot(mapping = aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = species)) +\n    geom_point()\n\nConnecting pipeline pieces together is still done with |&gt;. Once we reach ggplot(), we switch to +, as this is our method of adding layers to our plot.\nIn this course we’ve talked plenty about the structure of a data frame, so this part of the grammar of graphics is straight-forward. Be sure that every variable that you wish to include in your plot is present in the same data frame. Make sure also that if you want to focus on specific pieces of your data, that you zero in on these before making your visualization.\n\nMore fundamentally, be sure the data you’re using is well-suited to the message you aim to convey with your plot. Many plots go wrong right here at the outset, so be sure you’re on firm footing.\nAesthetics\nThe most impactful decision that you’ll make when constructing a plot using the grammar of graphics is deciding how to encode variables in a data frame into visual variation in your plot.\nThe penguin plot relies upon three forms of visual variation. The first is the location along the x-axis. Penguins with longer bills are placed on the right side of the plot and those with shorter bills are placed on the left. Variation in bill depth is captured by variation in the location along the y-axis, which is the second form. The third form is color: each of the three species is designated by one of three colors.\n\n\n\n\nWe can summarize this encoding, or “aesthetic mapping”, as:\n\n\nbill_length_mm is mapped to the x-axis\n\nbill_depth_mm is mapped to the y-axis\n\nspecies is mapped to color\n\nThese are three of many different techniques for visually encoding variability. Here is a list of the aesthetic attributes that are most commonly used:\n\n\nx: location along the x-axis\n\ny: location along the y-axis\n\ncolor: hue of the mark that represents the observation\n\nalpha: the level of transparency of the color\n\nsize: the size of the mark representing the observation\n\nshape: the shape of the mark representing the observation\n\nfill: the color of the inside of the representation of an observation\nGeometries\nWith the data set in place and the aesthetic mappings selected, the final choice in making our plot is to decide how to graphically express the observations themselves. For the penguin plot above, each observation in represented by a point, so it is said to use a “point” geometry. That is just one of many options. Other options are listed below.\n\npoint\nbar\nline\nhistogram\ndensity\nviolin\ndotplot\nboxplot\n\nWhen we speak about whether a plot is a scatter plot, a bar chart, a histogram, etc, we are discussing the geometry of a plot. The impact of this choice can be seen in the following two plots.\n\nCodelibrary(patchwork)\n\np1 &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                           y = species)) +\n    geom_violin()\np2 &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                           y = species)) +\n    geom_boxplot()\np1 + p2\n\n\n\n\n\n\n\nBoth plots share the same data (penguins) and the same aesthetic mappings(bill_length to the x-axis and species to the y-axis). Where they differ is the geometry: the plot on the left uses the violin while the one on the right uses the boxplot.\n\nGrammar of Graphics Summary\nThe Grammar of Graphics is a framework to express a great variety of statistical graphics in terms of their shared elements. In this framework, the core features of the plot are the data, the aesthetic mapping between aesthetic attributions and variables in the data frame, and the geometry that is used to express the observation. There are a wide range of geometries and aesthetic attributes that can be drawn from and recombined in powerful ways. What we have done so far is cover just the fundamentals of the framework, so if you are unsatisfied with the resulting plots, that’s good. Now, we will polish up these plots to make thoughtful graphics that focus on effectively conveying a single message."
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/notes.html#communicating-with-graphics",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/notes.html#communicating-with-graphics",
    "title": "A Grammar of Graphics",
    "section": "Communicating with Graphics",
    "text": "Communicating with Graphics\nAt this point in the course, you have a bevy of different types of statistical graphics under your belt: scatterplots, histograms, dot plots, violin plots, box plots, density curves, and bar plots of several kinds. You also have a broad framework to explain how these graphics are composed: the Grammar of Graphics. But to what purpose? Why plot data? For whom?\nEvery time you build a plot, you do so with one of two audiences in mind.\n\nYourself.\nSomeone else.\n\nThe process of building understanding from a data set is one that should be driven by curiosity, skepticism, and thoughtfulness. As a data scientist, you’ll find yourself in conversation with your data: asking questions of it, probing it for structure, and seeing how it responds. This thoughtful conversation is called exploratory data analysis (or EDA).\nDuring EDA, the aim is to uncover the shape and structure of your data and to uncover unexpected features. It’s an informal iterative process where you are your own audience. In this setting, you should construct graphics that work best for you.\nAt some point, you’ll find yourself confident in the claim that can be supported by your data and the focus changes to communicating that claim as effectively as possible with a graphic. Here, your audience shifts from yourself to someone else: other scientists, customers, co-workers in a different part of your company, or casual readers. You must consider the context in which they’ll be viewing your graphic: what they know, what they expect, what they want.\nIn these notes we discuss six ways to hone the message of your data visualization. They are:\n\nMapping versus setting\nLabels for clarity\nThe importance of scale \n\nChoosing a theme\nAnnotations\n\nWe will use two running examples throughout these notes: a line plot of the number of christenings in 17th century London which were collected by a man named John Arbuthnot, 4 and a scatter plot showing the bill sizes of penguins near Palmer Station, Antarctica5.\nLine plots also include two numerical variables and are useful for modeling trends that occur over time. The time variable goes on the x-axis; the trend variable goes on the y-axis. In the line plot shown here, we model the number of children that were christened in 16th and 17th century England.\n\nCodelibrary(tidyverse)\nlibrary(stat20data)\nlibrary(patchwork)\n\narbuthnot &lt;- mutate(arbuthnot, total = boys + girls)\n\np1 &lt;- ggplot(arbuthnot, aes(x = year,\n                            y = total)) +\n    geom_line()\n\np2 &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                           y = bill_depth_mm)) +\n    geom_point()\n\np1 + p2\n\n\n\n\n\n\n\nMapping vs Setting\nOnce you have your first draft of a plot complete and you’re thinking about how to fine tune it for your audience, your eye will turn to the aesthetic attributes. Is that color right? What about the size of the points?\nConsider the first draft of the penguins plot above. It might feel a bit drab to have a large mass of points all in black, the same color as the labels and surrounding text. Let’s make the points blue instead to make them stand out a bit more.\n\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = \"blue\")) +\n    geom_point()\n\n\nClick here when you’re ready to see the result.\n\n\n\n\n\n\n\n\nThis is . . . unexpected. Why did it color the points red? Is this a bug?\nWhat we’ve stumbled into is a subtle but essential distinction in the grammar of graphics: mapping vs setting. When you put an aesthetic attribute (x, color, size, etc.) into the aes() function, you’re mapping that attribute in the plot to whatever data lives in the corresponding column in the data frame. Mapping was this process:\n\n\n\n\nThat’s not what we set out to do here. We just wanted to tweak the look of our aesthetic attributes in a way that doesn’t have anything to do with the data in our data frame. This process is called setting the attribute.\nTo set the color to blue6, we need to make just a small change to the syntax. Let’s move the color = \"blue\" argument outside of the aes() function and into the geom_() function.\n\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm)) +\n    geom_point(color = \"blue\")\n\n\n\n\n\n\n\nAh, that looks much better!\nColor isn’t the only aesthetic attribute that you can set. Let’s increase slightly the size of our points by setting the size to three times the size of the default.\n\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm)) +\n    geom_point(color = \"blue\", size = 3)\n\n\n\n\n\n\n\nIt’s not clear that that improves the readability of the plot - there is more overlap between the points now - but the setting worked. One thing we might do to get more visibility on some of the points that were clumped is to use the alpha mapping!\n\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     size = species)) +\n    geom_point(color = \"blue\", \n               size = 3,\n               alpha = .5)\n\n\n\n\n\n\n\nHow would it have looked if instead we had mapped the size? When you map, you need a map to a column in your data frame, so let’s map size to species.\n\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     size = species)) +\n    geom_point(color = \"blue\")\n\n\n\n\n\n\n\nWe’ve made a mess of our plot now, but it is clear what happened. R looked inside the species column, found a categorical variable with three levels and selected a distinct size for each of those levels.\nAll in all, this is another area in which the grammar of graphic guides clear thinking when constructing a graphic. The aesthetic attributes of a plot can be determined either by variability found in a data set or by fixed values that we set. The former is present in all data visualization but it’s the latter that comes into play when fine-tuning your plot for an audience.\nAdding Labels for Clarity\nYou may have noticed that ggplot2 pulls the labels for the x-axis, the y-axis, and the legend directly from the names of the variables in the data frame. This results in labels like bill_length_mm, which is no problem when you’re making plots for yourself - you know what this variable means. But will an outside audience?\nYou can change the labels of your plot by adding a labs() layer.\n\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm)) +\n    geom_point(color = \"blue\") +\n    labs(x = \"Bill Length (mm)\",\n         y = \"Bill Depth (mm)\")\n\n\n\n\n\n\n\nAxis and legend labels should be concise and often include the units of measurement. If you find them getting too wordy, know that you can clarify or expand on what is being plotted either in a figure caption or in the accompanying text.\nSpeaking of captions, those a can be added too, as well as a title.\n\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm)) +\n    geom_point(color = \"blue\") +\n    labs(x = \"Bill Length (mm)\",\n         y = \"Bill Depth (mm)\",\n         title = \"Penguins show little association between bill length\\n and bill depth\",\n         caption = \"Source: Data collected by Dr. Kristen Gorman at Palmer Station, Antarctica\")\n\n\n\n\n\n\n\nThe title of a plot is valuable real estate for communicating the primary story of your plot. It should highlight the most important structure in the data. In the plot above, there appears to be little correspondence between bill length and bill depth. Of course, that changes when we map species to color. Let’s make that plot and title it accordingly.\n\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = species)) +\n    geom_point() +\n    labs(x = \"Bill Length (mm)\",\n         y = \"Bill Depth (mm)\",\n         title = \"Bill length and bill depth positively correlated and\\n strongly dependent on species\",\n         caption = \"Source: Data collected by Dr. Kristen Gorman at Palmer Station, Antarctica\")\n\n\n\n\n\n\n\nThe practice of using the plot title to convey the main message of the plot is used to powerful effect by the visualization experts at the British publication, The Financial Times7. They have developed a wealth of visualizations to help readers understand what is happening with public health throughout the pandemic. The sobering graphic below uses the title to guide the viewer to the most important visual structure in the plot: the yawning vertical gap between dosage rates between high and low income countries.\n\n\n\n\nThe Importance of Scale\nWhen a person views your plot, their first impression will be determined by a coarse interpretation of the boldest visual statement. When using a line plot, that is usually the general trend seen when reading left to right.\nWhat is the first word that comes to mind to describe the trend in each of the four plots below?\n\n\n\n\n\n\n\n\nClockwise from the upper left, you likely said something like “increasing”, “decreasing”, “variable”, and “stable”. Now take a second look. What exactly is being plotted here?\nThe labels along the axes are a hint to what you’re looking at here. These are, in fact, four plots from the exact same data: the Arbuthnot christening records, with the proportion of girls christened on the x-axis. What differs is the limits of the x- and y-axes.\nMost software will automatically set the limits of your plot based on the range of the data. In the Arbuthnot data, the years range from 1629 to 1710 and the proportion of girls christened ranges from .463 to .497. The leads to the default graphic found in the lower left panel. Each of the other three plots have modified the limits of the x- or y-axis to capture different parts the data scaled in different ways. In ggplot2 this is done by adding an xlim() or ylim() layer.\nThis is the power of scaling. From one data set, you can convey four different (and incompatible!) messages by changing the scale. So which one is correct? It depends on the context and the question that drove the collection of the data. John Arbuthnot’s mission in collecting his data was to understand the whether the chance of being born a girl is 1/2. That question is answered most clearly by the following plot (with the title driving home that main message).\n\nggplot(arbuthnot, aes(x = year,\n                      y = p_girls)) +\n    geom_line() +\n    ylim(0, 1) +\n    labs(title = \"Proportion girls christened slightly but persistently\\n below 50%\",\n         x = \"Year\",\n         y = \"Proportion\")\n\n\n\n\n\n\n\nThe importance of scale extends beyond scatter and line plots. Barcharts are often the subject of careful scaling to convey a particular message. What do you think the goal was of the creator of the plot titled “Should Britain Leave EU?”8\n\n\n\n\n\nChoosing a Theme\nWhat piece of software did I use to produce the following plot?\n\n\n\n\n\n\n\n\nIf you said “Excel”, you are correct! Well… it is Excel in spirit. What makes this plot look like it was made in Excel are a series of small visual choices that were made: the background is a dark gray, there are black horizontal guide lines, and the plot and the legend is surrounded by a black box. Small decisions like these that effect the overall look and feel of the plot are called the theme. The theme used here belongs to the ggthemes library. Here’s the code used.\n\nlibrary(ggthemes)\n\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = species)) +\n    geom_point() +\n    labs(x = \"Bill Length (mm)\",\n         y = \"Bill Depth (mm)\") +\n    theme_excel()\n\nLet’s look at a few more. Do they look familiar?\n\nCodep1 &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = species)) +\n    geom_point() +\n    labs(x = \"Bill Length (mm)\",\n         y = \"Bill Depth (mm)\") +\n    theme_wsj()\np2 &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = species)) +\n    geom_point() +\n    labs(x = \"Bill Length (mm)\",\n         y = \"Bill Depth (mm)\") +\n    theme_bw()\np3 &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = species)) +\n    geom_point() +\n    labs(x = \"Bill Length (mm)\",\n         y = \"Bill Depth (mm)\") +\n    theme_economist()\n\np1 / p3 / p2\n\n\n\n\n\n\n\nThey are, from top to bottom, a theme based on The Wall Street Journal, The Economist, and one of the themes built into ggplot2 packaged called bw for “black and white” (there are no grays). The ggplot2 library has several themes to choose from and yet more live in other packages like ggthemes. To use a theme, all you need to do is add a layer called theme_NAME (e.g. for the black and white theme, use theme_bw()).\nThemeing your plots is an easy way to change the look of your plot. Tinker with a few different themes and considering adding them to your labs9. But, as with all design decisions around graphics, be sure to think about your audience. You might find the Excel aesthetics ugly and dated, but will your audience? If you’re presenting your plot to a community that works with Excel plots day in and day out, that’s probably a sound choice. If you are preparing a plot for submission to a scientific journal, a more minimalist theme is more appropriate.\nAnnotations\nIn the same way that a title highlights the main message of a plot, you can rely upon visual cues to draw attention to certain components or provide helpful context.\nAlthough the Arbuthnot records seem very simple, they actually capture a wealth of historical information. We can add this information to our plot by adding annotations.\n\nggplot(arbuthnot, aes(x = year,\n                      y = total)) +\n    geom_line(color = \"steelblue\") +\n    labs(x = \"Year\", y = \"Total Christenings\") +\n    theme_bw() +\n    annotate(\"text\", \n             label = \"English Civil War begins\",\n             x = 1642, y = 13000) +\n    annotate(\"segment\", \n             x = 1642, xend = 1642,\n             y = 12600, yend = 10900)\n\n\n\n\n\n\n\nWere you curious about what caused that dip in the number of christenings in 17th century London? It happens to correspond to the duration of the English Civil War, when the monarchy was overthrown by a dictator named Oliver Cromwell. This very important context can be conveyed by adding a text label and a line segment through two new annotate() layers.\nWithin ggplot2, annotations are a flexible way to add the context or comparisons that help guide readers in interpreting your data. You can add text, shapes, lines, points. To learn more, consult the documentation10.\nSo if the drop after 1642 corresponds to the English Civil War, what about the spike down around 1666? What about 1703? If you’re curious, explore Wikipedia to find out and add those events as annotations to this plot.\n\nCommunication Summary\nThere are two main uses for data visualization. The first is as part of exploratory data analysis, when you are constructing plots for yourself to better understand the structure of the data. When you’re ready to communicate with an outside audience using graphics, more thought is needed: you must think about the difference between mapping and setting, the use of labels for clarity, the importance of scale, overplotting, themes, and annotations.\nThis is far from a complete list of what all can be done to improve your plots, but it is sufficient to produce polished graphics that effectively communicate your message."
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/notes.html#footnotes",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/notes.html#footnotes",
    "title": "A Grammar of Graphics",
    "section": "Footnotes",
    "text": "Footnotes\n\nWilkinson, Leland. The grammar of graphics. Springer Science & Business Media, 2005.↩︎\nFor more context around The Grammar of Graphics and the development of modern visualization tools, read the brief Three Waves of Data Visualization by Elijah Meeks, Senior Data Visualization Engineer at Netflix: https://www.tableau.com/about/blog/2019/2/three-waves-data-visualization-brief-history-and-predictions-future-100830.↩︎\nThe ggplot2 package is described in the manuscript, A layered grammar of graphics, by Hadley Wickham in the Journal of Computational and Graphical Statistics in 2010.↩︎\nFor data documentation, see the stat20data R package.↩︎\nFor data documentation, see the palmerpenguins R package.↩︎\nTo see the vast (and somewhat strange) palette of color names that R knows, type colors() at the console.↩︎\nVisualization drawn from the excellent collection of graphics at the Financial Times Covid Tracker https://ig.ft.com/coronavirus-vaccine-tracker/.↩︎\nGraphics from the keynote of John Burn-Murdoch at rstudio::conf() 2021.↩︎\nExplore the themes available within ggplot2 by reading the documentation https://ggplot2.tidyverse.org/reference/ggtheme.html. For the additional themes held in the ggthemes package, read this: https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/.↩︎\nDocumentation for annotation layers in ggplot2: https://ggplot2.tidyverse.org/reference/annotate.html.↩︎"
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/ps.html",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/ps.html",
    "title": "Communicating with Graphics",
    "section": "",
    "text": "Complete the following two questions in a Quarto Document."
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/ps.html#question-1",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/ps.html#question-1",
    "title": "Communicating with Graphics",
    "section": "Question 1",
    "text": "Question 1\nUse the gapminder data set in the gapminder library to recreate a version of Hans Rosling’s famous data visualization shown in the A Grammar of Graphics slides (a single plot instead of a movie; in other words, for just a single year).\n\nYou can see at a glance which years are available by running count(gapminder, year) in an R chunk.\nYou may use GDP per capita rather than fertility rate.\n\nYou will need to take the following steps.\n\nStep A\nDetermine the correct geometry for the plot and make an initial ggplot with the two variables on the x and y axis.\n\n\nStep B\nDistinguish the continents by either shape or color. Which ever one you do not use, set its value to something other than the default. Hint: use the help_file for geom_point() to find options you can set to!\n\n\nStep C\nAlter the x and y axis labels so that they’re more descriptive than just the variable given names in gapminder.\n\n\nStep D\nApply a theme of your choosing.\n\n\nStep E\nAdd an annotation that draws attention to a particular feature of the data (of your choosing).\n\n\nStep F\nTitle your plot with a claim based on your data.\n\n\nStep G\nApply a theme of your choosing."
  },
  {
    "objectID": "2-summarizing-data/03-a-grammar-of-graphics/ps.html#question-2",
    "href": "2-summarizing-data/03-a-grammar-of-graphics/ps.html#question-2",
    "title": "Communicating with Graphics",
    "section": "Question 2",
    "text": "Question 2\nRecreate one of the plots that you created for Lab 1.2, but incorporate at least 3 of the 6 elements of Communicating with Graphics to polish your plot into one that tells a particular story."
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html",
    "href": "2-summarizing-data/04-conditioning/slides.html",
    "title": "Data Pipelines",
    "section": "",
    "text": "Concept Questions\nProblem Set 4\nBreak\nLab 2.1: Flights"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#agenda",
    "href": "2-summarizing-data/04-conditioning/slides.html#agenda",
    "title": "Data Pipelines",
    "section": "",
    "text": "Concept Questions\nProblem Set 4\nBreak\nLab 2.1: Flights"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#announcements",
    "href": "2-summarizing-data/04-conditioning/slides.html#announcements",
    "title": "Data Pipelines",
    "section": "Announcements",
    "text": "Announcements\n\n\nRQ: A Grammar of Graphics due Wednesday at 11:59pm\n\n. . .\n\n\nProblem Set 4 (paper, max. 3) due next Tuesday at 9am\n\n. . .\n\n\nLab 2.1 (paper, max. 2) due next Tuesday at 9am\n\n. . .\n\n\nQuiz 1 next Monday at 11:59pm (direct logistical and content questions to the syllabus and megathread on Ed)."
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#question-1",
    "href": "2-summarizing-data/04-conditioning/slides.html#question-1",
    "title": "Data Pipelines",
    "section": "Question 1",
    "text": "Question 1\n\nCodec(\"fruit\", \"fruit\", \"vegetable\") == \"fruit\"\n\n\n\nWhat will this line of code return?\nRespond at pollev.com.\n\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#evaluating-equivalence-cont.",
    "href": "2-summarizing-data/04-conditioning/slides.html#evaluating-equivalence-cont.",
    "title": "Data Pipelines",
    "section": "Evaluating equivalence, cont.",
    "text": "Evaluating equivalence, cont.\nIn R, this evaluation happens element-wise when operating on vectors.\n\nCodec(\"fruit\", \"fruit\", \"vegetable\") == \"fruit\"\n\n\n\n\n[1]  TRUE  TRUE FALSE\n\n\n. . .\n\nCodec(\"fruit\", \"fruit\", \"vegetable\") != \"fruit\"\n\n\n. . .\n\n\n[1] FALSE FALSE  TRUE\n\n\n. . .\n\nCodec(\"fruit\", \"vegetable\", \"boba\") %in% c(\"fruit\", \"vegetable\")\n\n\n. . .\n\n\n[1]  TRUE  TRUE FALSE"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#question-2",
    "href": "2-summarizing-data/04-conditioning/slides.html#question-2",
    "title": "Data Pipelines",
    "section": "Question 2",
    "text": "Question 2\nWhich observations will be included in the following data frame?\n\nCodeclass_survey |&gt;\n  filter(coding_exp_scale &lt; 3,\n        olympic_sport %in% c(\"Ice skating\", \"Speed skating\"),\n        entrepreneur == TRUE)\n\n\n\nPlease respond at pollev.com.\n\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#question-3",
    "href": "2-summarizing-data/04-conditioning/slides.html#question-3",
    "title": "Data Pipelines",
    "section": "Question 3",
    "text": "Question 3\n\nWhich data frame will have fewer rows?\n\n. . .\n\nCode# this one\nfilter(class_survey, year == \"This is my first semester!\")\n\n# or this one\nclass_survey |&gt;\n  mutate(first_sem = (year == \"This is my first semester!\")) |&gt;\n  filter(first_sem)\n\n\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#nesting",
    "href": "2-summarizing-data/04-conditioning/slides.html#nesting",
    "title": "Data Pipelines",
    "section": "Nesting",
    "text": "Nesting\n\nCodefilter(class_survey, \n       coding_exp_scale &lt; 3,\n       olympic_sport %in% c(\"Ice skating\", \"Speed skating\"),\n       entrepreneur == TRUE)"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#nesting-1",
    "href": "2-summarizing-data/04-conditioning/slides.html#nesting-1",
    "title": "Data Pipelines",
    "section": "Nesting",
    "text": "Nesting\n\nCodeselect(filter(class_survey, \n       coding_exp_scale &lt; 3,\n       olympic_sport %in% c(\"Ice skating\", \"Speed skating\"),\n       entrepreneur == TRUE),\n       coding_exp_xcale,\n       olympic_sport,\n       entrepreneur,\n       new_COVID_variant)"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#nesting-2",
    "href": "2-summarizing-data/04-conditioning/slides.html#nesting-2",
    "title": "Data Pipelines",
    "section": "Nesting",
    "text": "Nesting\n\nCodesummarize(select(filter(class_survey, \n       coding_exp_scale &lt; 3,\n       olympic_sport %in% c(\"Ice skating\", \"Speed skating\"),\n       entrepreneur == TRUE),\n       coding_exp_scale,\n       olympic_sport,\n       entrepreneur,\n       new_COVID_variant),\n       covid_avg = mean(new_COVID_variant))"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#nesting-3",
    "href": "2-summarizing-data/04-conditioning/slides.html#nesting-3",
    "title": "Data Pipelines",
    "section": "Nesting",
    "text": "Nesting\n\nCodesummarize(select(filter(class_survey, \n       coding_exp_scale &lt; 3,\n       olympic_sport %in% c(\"Ice skating\", \"Speed skating\"),\n       entrepreneur == TRUE),\n       coding_exp_scale,\n       olympic_sport,\n       entrepreneur,\n       new_COVID_variant),\n       covid_avg = mean(new_COVID_variant))\n\n# A tibble: 1 × 1\n  covid_avg\n      &lt;dbl&gt;\n1      0.52"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#nesting-4",
    "href": "2-summarizing-data/04-conditioning/slides.html#nesting-4",
    "title": "Data Pipelines",
    "section": "Nesting",
    "text": "Nesting\n\nCodesummarize(select(filter(class_survey, \n       coding_exp_scale &lt; 3,\n       olympic_sport %in% c(\"Ice skating\", \"Speed skating\"),\n       entrepreneur == TRUE),\n       coding_exp_scale,\n       olympic_sport,\n       entrepreneur,\n       new_COVID_variant),\n       covid_avg = mean(new_COVID_variant))\n\n\n. . .\n\n\nCons\n\nMust be read from inside out 👎\nHard to keep track of arguments 👎\n\n\nPros\n\nAll in one line of code 👍\nOnly refer to one data frame 👍"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#step-by-step",
    "href": "2-summarizing-data/04-conditioning/slides.html#step-by-step",
    "title": "Data Pipelines",
    "section": "Step-by-step",
    "text": "Step-by-step\n. . .\n\nCodedf1 &lt;- filter(class_survey, \n              coding_exp_scale &lt; 3,\n              olympic_sport %in% c(\"Ice skating\", \"Speed skating\"),\n              entrepreneur == TRUE)\ndf2 &lt;- select(df1, \n              coding_exp_scale,\n              olympic_sport,\n              entrepreneur,\n              new_COVID_variant)\nsummarize(df2,\n          covid_avg = mean(new_COVID_variant))\n\n\n. . .\n\n\nCons\n\nHave to repeat data frame names 👎\nCreates unnecessary objects 👎\n\n\nPros\n\nStores intermediate objects 👍\nCan be read top to bottom 👍"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#using-the-pipe-operator",
    "href": "2-summarizing-data/04-conditioning/slides.html#using-the-pipe-operator",
    "title": "Data Pipelines",
    "section": "Using the pipe operator",
    "text": "Using the pipe operator\n. . .\n\nCodeclass_survey |&gt;\n  filter(coding_exp_scale &lt; 3,\n         olympic_sport %in% c(\"Ice skating\", \"Speed skating\"),\n         entrepreneur == TRUE) |&gt;\n  select(coding_exp_scale,\n         olympic_sport,\n         entrepreneur,\n         new_COVID_variant) |&gt;\n  summarize(covid_avg = mean(new_COVID_variant))\n\n\n\n\nCons\n\n🤷\n\n\nPros\n\nCan be read like an english paragraph 👍\nOnly type the data once 👍\nNo leftovers objects 👍"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#understanding-your-pipeline",
    "href": "2-summarizing-data/04-conditioning/slides.html#understanding-your-pipeline",
    "title": "Data Pipelines",
    "section": "Understanding your pipeline",
    "text": "Understanding your pipeline\n. . .\nIt’s good practice to understand the output of each line of code by breaking the pipe.\n. . .\n\n\n\nCodeclass_survey |&gt;\n  select(new_COVID_variant) |&gt;\n  filter(year == \"It's my first year.\")\n\nError in `filter()`:\nℹ In argument: `year == \"It's my first year.\"`.\nCaused by error in `year == \"It's my first year.\"`:\n! comparison (==) is possible only for atomic and list types\n\n\n\n\nCodeclass_survey |&gt;\n  select(new_COVID_variant)\n\n# A tibble: 619 × 1\n   new_COVID_variant\n               &lt;dbl&gt;\n 1           0.25   \n 2           0.1    \n 3           0      \n 4           0.2    \n 5           0.9    \n 6           0.2    \n 7           0.4    \n 8           0.00005\n 9           0.2    \n10           0.3    \n# ℹ 609 more rows"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#concept-question-2-redux",
    "href": "2-summarizing-data/04-conditioning/slides.html#concept-question-2-redux",
    "title": "Data Pipelines",
    "section": "Concept Question 2 Redux",
    "text": "Concept Question 2 Redux\n\nCodeclass_survey |&gt; # A #&lt;&lt;\n  filter(coding_exp_scale &lt; 3,\n         olympic_sport %in% c(\"Ice skating\", \n                         \"Speed skating\"),\n         entrepreneur == TRUE) |&gt; # B #&lt;&lt;\n  select(coding_exp_scale,\n         olympic_sport,\n         entrepreneur,\n         new_COVID_variant) |&gt; # C #&lt;&lt;\n  summarize(covid_avg = mean(new_COVID_variant)) # D #&lt;&lt;\n\n\n\nWhat are the dimensions (rows x columns) of the data frames output at each stage of this pipe?\n\n. . .\n\n\n\n\n−&plus;\n\n01:00"
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#concept-question-4",
    "href": "2-summarizing-data/04-conditioning/slides.html#concept-question-4",
    "title": "Data Pipelines",
    "section": "Concept Question 4",
    "text": "Concept Question 4\n\nCodesummarize(class_survey, mean(year == \"I'm in my first year.\"))\n\n\n\nWhat is will this line of code return?\nRespond at pollev.com."
  },
  {
    "objectID": "2-summarizing-data/04-conditioning/slides.html#boolean-algebra",
    "href": "2-summarizing-data/04-conditioning/slides.html#boolean-algebra",
    "title": "Data Pipelines",
    "section": "Boolean Algebra",
    "text": "Boolean Algebra\nLogical vectors have a dual representation as TRUE FALSE and 1, 0, so you can do math on logicals accordingly.\n\nCodeTRUE + TRUE\n\n[1] 2\n\n\n. . .\n\nCodeTRUE * TRUE\n\n[1] 1\n\n\n. . .\n\nTaking the mean of a logical vector is equivalent to find the proportion of rows that are TRUE (i.e. the proportion of rows that meet the condition)."
  },
  {
    "objectID": "1-questions-and-data/assignments/lab-getting-started/lab.html",
    "href": "1-questions-and-data/assignments/lab-getting-started/lab.html",
    "title": "Lab 0: Getting Started with R, RStudio and Quarto",
    "section": "",
    "text": "Complete the following questions in a Quarto document (.qmd). Render this document to HTML, print the corresponding HTML to PDF, and then submit the final PDF to Gradescope by Monday, June 24 at 12pm!"
  },
  {
    "objectID": "1-questions-and-data/assignments/lab-getting-started/lab.html#question-1",
    "href": "1-questions-and-data/assignments/lab-getting-started/lab.html#question-1",
    "title": "Lab 0: Getting Started with R, RStudio and Quarto",
    "section": "Question 1",
    "text": "Question 1\n\nWhere are you from or where would you like to visit?\n\n\npart a\nTell us the county and state that you are from. If you are from outside the United States, provide the name of a county and state that you would like to visit. This text needs to show up as a header!\n\n\npart b\nCopy 1-3 paragraphs about this place from its Wikipedia page, and cite the page using a hyperlink.\n\n\npart c\nWrite an enumerated list of your top three favorite things about this place."
  },
  {
    "objectID": "1-questions-and-data/assignments/lab-getting-started/lab.html#question-2",
    "href": "1-questions-and-data/assignments/lab-getting-started/lab.html#question-2",
    "title": "Lab 0: Getting Started with R, RStudio and Quarto",
    "section": "Question 2",
    "text": "Question 2\n\npart a\nIn an R chunk, perform any calculation that results in the number 20. Make sure the number 20 prints out to the screen.\n\n\npart b\nCreate the data frame of Problem Set 1, Question 3 using R code and save it into the object my_classmates."
  },
  {
    "objectID": "1-questions-and-data/assignments/lab-getting-started/lab.html#question-3",
    "href": "1-questions-and-data/assignments/lab-getting-started/lab.html#question-3",
    "title": "Lab 0: Getting Started with R, RStudio and Quarto",
    "section": "Question 3",
    "text": "Question 3\nSurvey respondents are often asked questions containing answer choices belonging to what is called Likert Scale.\n\npart a\nVisit the Wikipedia page for the Likert Scale and create a vector containing the options within a typical, five-level Likert Scale.\n\n\npart b\nThen, use the factor() function to sort the levels of the vector you made in part a from decreasing to increasing."
  }
]